{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import tarfile\n",
    "import sys\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 9999999)\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189b7d6",
   "metadata": {},
   "source": [
    "# Use EN samples from covost2 data \n",
    "\n",
    "(all data except for the pairs that has DE, just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ea70e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "\n",
    "original_data = pd.DataFrame(columns=['SRC'])\n",
    "\n",
    "data_dir = \"data/covost2/EN-translations\"\n",
    "\n",
    "deduplicated_each_dataset = 0\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".tar.gz\") and ('de' not in filename):\n",
    "        unzipped_file_name = filename.replace(\".tar.gz\", \"\")\n",
    "        \n",
    "        # Extract the file if not yet done so\n",
    "        if not os.path.exists(os.path.join(data_dir, unzipped_file_name)):\n",
    "            tar = tarfile.open(os.path.join(data_dir, filename))\n",
    "            tar.extractall(data_dir)\n",
    "            tar.close()\n",
    "            \n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_df['SRC'] = pd.read_csv(os.path.join(data_dir, unzipped_file_name), sep='\\t')['translation']\n",
    "\n",
    "        deduplicated_each_dataset = deduplicated_each_dataset + tmp_df.drop_duplicates(subset='SRC').shape[0]\n",
    "        \n",
    "        original_data = pd.concat([original_data, tmp_df], axis=0, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "print(f\"all: {original_data.shape}\")\n",
    "print(f\"deduplicated_each_dataset: {deduplicated_each_dataset}\")\n",
    "print(f\"deduplicated all: {original_data.drop_duplicates(subset='SRC').shape}\")\n",
    "\n",
    "original_data = original_data.drop_duplicates(subset='SRC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c20da",
   "metadata": {},
   "source": [
    "Filter out the errornously long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84136d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = original_data['SRC'].apply(lambda x: len(x))\n",
    "\n",
    "length_stats = sentence_lengths.describe(percentiles=[.25, .5, .75, .99])\n",
    "\n",
    "original_data = original_data[sentence_lengths < length_stats['99%']]\n",
    "\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de2fee",
   "metadata": {},
   "source": [
    "Remove empty sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d554c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_data[original_data['SRC'] != \"\"]\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5d214",
   "metadata": {},
   "source": [
    "Remove the begining and end quotes for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda843b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Remove the begining and end quotes \n",
    "    \"\"\"\n",
    "    if (sentence.startswith('\\\"') and sentence.endswith('\\\"')) or \\\n",
    "        (sentence.startswith('“') and sentence.endswith('”')):\n",
    "        return sentence[1:-1]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "original_data['SRC'] = original_data['SRC'].apply(lambda x: prepare_sentence(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d2b93",
   "metadata": {},
   "source": [
    "Reindex after filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data['SRC'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3de20",
   "metadata": {},
   "source": [
    "# Use EN samples from winoMT data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "\n",
    "original_data = pd.read_csv('data/winoMT_src.csv', index_col=0)\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fee04",
   "metadata": {},
   "source": [
    "# Use EN samples from Must-SHE data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "\n",
    "original_data = pd.read_csv(\n",
    "    f\"data/MuST-SHE_v1.2/MuST-SHE-v1.2-data/tsv/MONOLINGUAL.fr_v1.2.tsv\",\n",
    "    sep='\\t', index_col=0\n",
    ")[['SRC', 'CATEGORY']]\n",
    "print(original_data.shape)\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625dcc97",
   "metadata": {},
   "source": [
    "# Use EN samples from WMT21 DA test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da031451",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "data_name = 'WMT21_DA'\n",
    "\n",
    "with open(f\"data/wmt-qe-2021-data/en-de-test21/test21.src\") as f:\n",
    "    en_sentences = f.readlines()\n",
    "    en_sentences = [line.rstrip() for line in en_sentences]\n",
    "    \n",
    "original_data = pd.DataFrame(data={'SRC': en_sentences})\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e0e03",
   "metadata": {},
   "source": [
    "# Use EN samples from WMT22 MQM test data\n",
    "\n",
    "Same as WMT22 word-level test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "data_name = 'WMT22_MQM'\n",
    "\n",
    "with open(f\"data/wmt-qe-2022-data/test_data-gold_labels/task1_mqm/en-de/test.2022.src\") as f:\n",
    "    en_sentences = f.readlines()\n",
    "    en_sentences = [line.rstrip() for line in en_sentences]\n",
    "    \n",
    "original_data = pd.DataFrame(data={'SRC': en_sentences})\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965aa003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f1281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b989c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42a941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7edbe491",
   "metadata": {},
   "source": [
    "# Create input data where we mask a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Filter out the words that has all punctuations in it\n",
    "    \n",
    "    all_puncts = string.punctuation + '—'\n",
    "    contains_all_puncts = True\n",
    "    for char in word:\n",
    "        if char not in all_puncts:\n",
    "            contains_all_puncts = False\n",
    "            break\n",
    "    if contains_all_puncts:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def is_content_tag(nltk_pos):\n",
    "    content_tags_prefix = ['NN', 'V', 'JJ', 'RB', 'PRP']  # Noun, verb, adj, adv, pronoun\n",
    "    for prefix in content_tags_prefix:\n",
    "        if nltk_pos.startswith(prefix):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "def mask_sentence(sentence, tokenized_sentence, masked_word_tokenized_index):\n",
    "    \"\"\"\n",
    "        sentence: the original sentence without preprocessing\n",
    "        masked_word: the word to be masked (in lowercase)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the location of the word in the sentence\n",
    "    lowercased_sentence = sentence.lower()\n",
    "    tokenized_sentence = tokenizer.tokenize(\n",
    "        lowercased_sentence, escape=False, aggressive_dash_splits=False\n",
    "    )\n",
    "    masked_word_location = None\n",
    "    prev_index = 0\n",
    "    for word in tokenized_sentence:\n",
    "        word_location = lowercased_sentence.find(word, prev_index)\n",
    "        if word == masked_word:\n",
    "            masked_word_location = word_location\n",
    "        else:\n",
    "            prev_index = word_location + len(word)\n",
    "    \n",
    "    assert masked_word_location is not None\n",
    "    \n",
    "    return sentence[:masked_word_location] + '[MASK]' + sentence[masked_word_location+len(masked_word):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "\n",
    "tokenizer = MosesTokenizer(lang=src_lang)\n",
    "detokenizer = MosesDetokenizer(lang=src_lang)\n",
    "\n",
    "\n",
    "masked_data = pd.DataFrame(columns=['SRC_original_idx', 'SRC', 'SRC_masked', 'original_word', 'original_word_tag'])\n",
    "\n",
    "for src_idx, src_row in original_data.iterrows():\n",
    "    tokenized_src = tokenizer.tokenize(src_row['SRC'], escape=False, aggressive_dash_splits=False)\n",
    "    pos_tags = nltk.pos_tag(tokenized_src)\n",
    "    original_words = []\n",
    "    original_words_tags = []\n",
    "    masked_sentences = []\n",
    "    for i, word_tag in enumerate(pos_tags):\n",
    "        word, pos_tag = word_tag\n",
    "        # Only mask the valid content word\n",
    "        if is_valid_word(word) and is_content_tag(pos_tag):\n",
    "            masked_tokenized = copy.deepcopy(tokenized_src)\n",
    "            masked_tokenized[i] = '[MASK]'\n",
    "            masked_sentences.append(\n",
    "                detokenizer.detokenize(masked_tokenized)\n",
    "            )\n",
    "            original_words.append(word)\n",
    "            original_words_tags.append(pos_tag)\n",
    "    single_sentence_df = pd.DataFrame()\n",
    "    single_sentence_df['SRC_masked'] = masked_sentences\n",
    "    single_sentence_df['SRC'] = src_row['SRC']\n",
    "    single_sentence_df['SRC_original_idx'] = src_idx\n",
    "    single_sentence_df['original_word'] = original_words\n",
    "    single_sentence_df['original_word_tag'] = original_words_tags\n",
    "    \n",
    "    masked_data = pd.concat([masked_data, single_sentence_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data.shape\n",
    "masked_data = masked_data.dropna()\n",
    "masked_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc66bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_data.to_csv('data/masked_content_covost2_for_en2de_no_sentence_group.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_data.to_csv('data/masked_content_winoMT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_data.to_csv(f'data/masked_content_{data_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab31ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
