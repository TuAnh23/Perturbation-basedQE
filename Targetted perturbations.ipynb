{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import tarfile\n",
    "import sys\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189b7d6",
   "metadata": {},
   "source": [
    "# Use EN samples from covost2 data \n",
    "\n",
    "(all data except for the pairs that has DE, just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ea70e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "\n",
    "original_data = pd.DataFrame(columns=['SRC'])\n",
    "\n",
    "data_dir = \"data/covost2/EN-translations\"\n",
    "\n",
    "deduplicated_each_dataset = 0\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".tar.gz\") and ('de' not in filename):\n",
    "        unzipped_file_name = filename.replace(\".tar.gz\", \"\")\n",
    "        \n",
    "        # Extract the file if not yet done so\n",
    "        if not os.path.exists(os.path.join(data_dir, unzipped_file_name)):\n",
    "            tar = tarfile.open(os.path.join(data_dir, filename))\n",
    "            tar.extractall(data_dir)\n",
    "            tar.close()\n",
    "            \n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_df['SRC'] = pd.read_csv(os.path.join(data_dir, unzipped_file_name), sep='\\t')['translation']\n",
    "\n",
    "        deduplicated_each_dataset = deduplicated_each_dataset + tmp_df.drop_duplicates(subset='SRC').shape[0]\n",
    "        \n",
    "        original_data = pd.concat([original_data, tmp_df], axis=0, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "print(f\"all: {original_data.shape}\")\n",
    "print(f\"deduplicated_each_dataset: {deduplicated_each_dataset}\")\n",
    "print(f\"deduplicated all: {original_data.drop_duplicates(subset='SRC').shape}\")\n",
    "\n",
    "original_data = original_data.drop_duplicates(subset='SRC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c20da",
   "metadata": {},
   "source": [
    "Filter out the errornously long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84136d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = original_data['SRC'].apply(lambda x: len(x))\n",
    "\n",
    "length_stats = sentence_lengths.describe(percentiles=[.25, .5, .75, .99])\n",
    "\n",
    "original_data = original_data[sentence_lengths < length_stats['99%']]\n",
    "\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de2fee",
   "metadata": {},
   "source": [
    "Remove empty sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d554c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_data[original_data['SRC'] != \"\"]\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5d214",
   "metadata": {},
   "source": [
    "Remove the begining and end quotes for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda843b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Remove the begining and end quotes \n",
    "    \"\"\"\n",
    "    if (sentence.startswith('\\\"') and sentence.endswith('\\\"')) or \\\n",
    "        (sentence.startswith('“') and sentence.endswith('”')):\n",
    "        return sentence[1:-1]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "original_data['SRC'] = original_data['SRC'].apply(lambda x: prepare_sentence(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d2b93",
   "metadata": {},
   "source": [
    "Reindex after filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data['SRC'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3de20",
   "metadata": {},
   "source": [
    "# Use EN samples from winoMT data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "\n",
    "original_data = pd.read_csv('data/winoMT_src.csv', index_col=0)\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fee04",
   "metadata": {},
   "source": [
    "# Use EN samples from Must-SHE data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "\n",
    "original_data = pd.read_csv(\n",
    "    f\"data/MuST-SHE_v1.2/MuST-SHE-v1.2-data/tsv/MONOLINGUAL.fr_v1.2.tsv\",\n",
    "    sep='\\t', index_col=0\n",
    ")[['SRC', 'CATEGORY']]\n",
    "print(original_data.shape)\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1de402",
   "metadata": {},
   "source": [
    "# Use EN samples from WMT21 DA test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da031451",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "data_name = 'WMT21_DA'\n",
    "\n",
    "with open(f\"data/wmt-qe-2021-data/en-de-test21/test21.src\") as f:\n",
    "    en_sentences = f.readlines()\n",
    "    en_sentences = [line.rstrip() for line in en_sentences]\n",
    "    \n",
    "original_data = pd.DataFrame(data={'SRC': en_sentences})\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0839d69",
   "metadata": {},
   "source": [
    "# Use EN samples from WMT22 MQM test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "data_name = 'WMT22_MQM'\n",
    "\n",
    "with open(f\"data/wmt-qe-2022-data/test_data-gold_labels/task1_mqm/en-de/test.2022.src\") as f:\n",
    "    en_sentences = f.readlines()\n",
    "    en_sentences = [line.rstrip() for line in en_sentences]\n",
    "    \n",
    "original_data = pd.DataFrame(data={'SRC': en_sentences})\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098dbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6411610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be327673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7085a9",
   "metadata": {},
   "source": [
    "### Perform stemming on the data\n",
    "\n",
    "This would help reduce the vocab size, easier to later on choose the word to perturb\n",
    "\n",
    "ABORT: it reduce the vocab from 88066 to 71362, so not that much, so doesnt worth it. Also stemming makes the word invalid, so cannot use POS afterward to filter it out.\n",
    "\n",
    "Lemmatization would require defining POS --> not preferable, since we would want chinese and china to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_sentence(stemmer, sentence):\n",
    "#     \"\"\" \n",
    "#     Return the stemmed sentence and \n",
    "#     a dictionary mapping the stem to the original word in the sentence\n",
    "#     \"\"\"\n",
    "#     tokenizer = MosesTokenizer(lang=src_lang)\n",
    "#     tokenized_sentence = tokenizer.tokenize(sentence,\n",
    "#                                             escape=False,\n",
    "#                                             aggressive_dash_splits=False)\n",
    "#     stem_word_dict = {}\n",
    "#     stemmed_tokenized_sentence = []\n",
    "    \n",
    "#     for word in tokenized_sentence:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stemmed_tokenized_sentence.append(stem)\n",
    "#         stem_word_dict[stem] = word\n",
    "        \n",
    "#     return ' '.join(stemmed_tokenized_sentence), stem_word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73131ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# original_data['StemSRC'], original_data['StemDict'] = \\\n",
    "#     zip(*original_data.apply(lambda x: stem_sentence(stemmer, x['SRC']), axis=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd28fa",
   "metadata": {},
   "source": [
    "### Invesitigate in the frequencies of words across sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f77e3f",
   "metadata": {},
   "source": [
    "Count the number of occurance in sentence of each word. Here we **only use the sentences where the words only occurs 1 time**, which is convenient to analyse on the influence of the word on the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import find, csr_matrix\n",
    "\n",
    "corpus = original_data['SRC'].values\n",
    "tokenizer = MosesTokenizer(lang=src_lang)\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: tokenizer.tokenize(\n",
    "        x,\n",
    "        escape=False,\n",
    "        aggressive_dash_splits=False\n",
    "    ),\n",
    "    lowercase=True\n",
    ")\n",
    "count_fit = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Only consider the single occurance of a word in a sentence\n",
    "# count_fit[count_fit > 1] = 0\n",
    "\n",
    "count_fit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba29f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import time \n",
    "\n",
    "\n",
    "word_df = pd.DataFrame()\n",
    "word_df['word'] = vectorizer.get_feature_names_out()\n",
    "word_df['freq'] = np.asarray(count_fit.sum(axis=0)).flatten()\n",
    "\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_pos_tag(word, spacy_model):\n",
    "    doc = spacy_model(word)\n",
    "    return [t.pos_ for t in doc][0]\n",
    "\n",
    "def nltk_pos_tag(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "def get_entity_name(word, spacy_model):\n",
    "    \"\"\"\n",
    "    Function returning the NER output from spacy on a word\n",
    "    Return None if the word does not have any entity name\n",
    "    Labels and there descriptions:\n",
    "    ```\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    labels = nlp.get_pipe('ner').labels\n",
    "    for label in labels:\n",
    "        print(f'{label}: {spacy.explain(label)}')\n",
    "    ```\n",
    "        CARDINAL: Numerals that do not fall under another type\n",
    "        DATE: Absolute or relative dates or periods\n",
    "        EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "        FAC: Buildings, airports, highways, bridges, etc.\n",
    "        GPE: Countries, cities, states\n",
    "        LANGUAGE: Any named language\n",
    "        LAW: Named documents made into laws.\n",
    "        LOC: Non-GPE locations, mountain ranges, bodies of water\n",
    "        MONEY: Monetary values, including unit\n",
    "        NORP: Nationalities or religious or political groups\n",
    "        ORDINAL: \"first\", \"second\", etc.\n",
    "        ORG: Companies, agencies, institutions, etc.\n",
    "        PERCENT: Percentage, including \"%\"\n",
    "        PERSON: People, including fictional\n",
    "        PRODUCT: Objects, vehicles, foods, etc. (not services)\n",
    "        QUANTITY: Measurements, as of weight or distance\n",
    "        TIME: Times smaller than a day\n",
    "        WORK_OF_ART: Titles of books, songs, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = spacy_model(word)\n",
    "    for w in doc.ents:\n",
    "        return w.label_\n",
    "\n",
    "start = time.time()\n",
    "word_df['POS'] = word_df['word'].apply(lambda x: nltk_pos_tag(x))\n",
    "print(f\"POS tagging execution time: {time.time() - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc84c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8851bcd7",
   "metadata": {},
   "source": [
    "Have a look at the most frequent content words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 9999999)\n",
    "\n",
    "def is_content_tag(nltk_pos):\n",
    "    content_tags_prefix = ['NN', 'V', 'JJ', 'RB', 'PRP']  # Noun, verb, adj, adv (RB, but removed), pronoun\n",
    "    for prefix in content_tags_prefix:\n",
    "        if nltk_pos.startswith(prefix):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_stopword(word):\n",
    "    # Manually define some stopwords (words that dont contain much content, or errornous)\n",
    "    stopwords = ['is', 'are', 'was', 'were', 'am', 'be', \n",
    "                 'not', 'let',\n",
    "                 'have', 'has', 'had', \n",
    "                 'de', 'la', 'du', 're', 'sur', 'des', 'le', 'll', \n",
    "                 'oh', 'lot', 'les', 'ah', 'en', 've',\n",
    "                 'didn', 'bois']\n",
    "    return word in stopwords\n",
    "\n",
    "\n",
    "content_word_bool = word_df['POS'].apply(lambda x: is_content_tag(x)) \\\n",
    "    & (~word_df['word'].apply(lambda x: is_stopword(x)))\n",
    "\n",
    "word_df[content_word_bool].sort_values(\n",
    "    by='freq', ascending=False\n",
    ").head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e66a8",
   "metadata": {},
   "source": [
    "### Create input data where we mask a set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85a7e5",
   "metadata": {},
   "source": [
    "#### Set of regional words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c959e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_regional_tag(spacy_ner):\n",
    "#     regional_tags = ['GPE', 'LANGUAGE', 'NORP']\n",
    "#     return spacy_ner in regional_tags\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# word_df['NER'] = word_df['word'].apply(lambda x: get_entity_name(x, spacy_model))\n",
    "# print(f\"NER execution time: {time.time() - start} seconds\")\n",
    "\n",
    "\n",
    "# regional_word_bool = word_df['NER'].apply(lambda x: is_regional_tag(x)) \\\n",
    "#     & (~word_df['word'].apply(lambda x: is_stopword(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f987d",
   "metadata": {},
   "source": [
    "#### Set of all content words that is frequent in the inference data\n",
    "\n",
    "We select the words that appears in over 50 sentences.\n",
    "\n",
    "When we dont need to group sentences with the same masked word, we keep the word freq over sentences lower (10) just to filter out the weird rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_SENTENCES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df[content_word_bool][word_df['freq'] > NR_OF_SENTENCES].sort_values('freq', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b76d13",
   "metadata": {},
   "source": [
    "Filter out the strange words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Filter out the words that has all punctuations in it\n",
    "    \n",
    "    all_puncts = string.punctuation + '—'\n",
    "    contains_all_puncts = True\n",
    "    for char in word:\n",
    "        if char not in all_puncts:\n",
    "            contains_all_puncts = False\n",
    "            break\n",
    "    if contains_all_puncts:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    # Filter out the words with strange characters in it\n",
    "    # Strange characters are punctuations, except ' . -\n",
    "    strange_characters = all_puncts.replace(\"\\'\", '').replace(\".\", '').replace(\"-\", '').replace(\"—\", '')\n",
    "    for char in strange_characters:\n",
    "        if char in word:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "valid_word_bool = word_df['word'].apply(is_valid_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d069c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_bool = content_word_bool & (word_df['freq'] > NR_OF_SENTENCES) & valid_word_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83baf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(filtered_word_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e423c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded953c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ea6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mask_sentence(sentence, masked_word, tokenizer, detokenizer):\n",
    "    \"\"\"\n",
    "        sentence: the original sentence without preprocessing\n",
    "        masked_word: the word to be masked (in lowercase)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the location of the word in the sentence\n",
    "    lowercased_sentence = sentence.lower()\n",
    "    tokenized_sentence = tokenizer.tokenize(\n",
    "        lowercased_sentence, escape=False, aggressive_dash_splits=False\n",
    "    )\n",
    "    masked_word_location = None\n",
    "    prev_index = 0\n",
    "    for word in tokenized_sentence:\n",
    "        word_location = lowercased_sentence.find(word, prev_index)\n",
    "        if word == masked_word:\n",
    "            masked_word_location = word_location\n",
    "        else:\n",
    "            prev_index = word_location + len(word)\n",
    "    \n",
    "    assert masked_word_location is not None\n",
    "    \n",
    "    return sentence[:masked_word_location] + '[MASK]' + sentence[masked_word_location+len(masked_word):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MosesTokenizer(lang=src_lang)\n",
    "detokenizer = MosesDetokenizer(lang=src_lang)\n",
    "\n",
    "\n",
    "masked_data = pd.DataFrame(columns=['SRC', 'SRC_masked', 'original_word'])\n",
    "\n",
    "filtered_word_df = word_df[filtered_word_bool]\n",
    "\n",
    "for word_index, filtered_word_row in filtered_word_df.iterrows():\n",
    "    # Indices of the sentences that contains the word\n",
    "    sentence_indices = original_data.index[count_fit.transpose()[word_index].nonzero()[1]]\n",
    "    \n",
    "#     # Randomly select a fixed number of sentences\n",
    "#     sentence_indices = np.random.choice(a=sentence_indices, \n",
    "#                                         size=NR_OF_SENTENCES, \n",
    "#                                         replace=False)\n",
    "    \n",
    "    # Create a temporary df to store the sentences for this word\n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df['SRC_original_idx'] = sentence_indices\n",
    "    tmp_df['SRC'] = original_data.loc[sentence_indices, 'SRC'].values\n",
    "    tmp_df['original_word'] = filtered_word_row['word']\n",
    "    \n",
    "    # Mask the word in those sentences\n",
    "    tmp_df['SRC_masked'] = \\\n",
    "        original_data.loc[sentence_indices, 'SRC'].apply(\n",
    "        lambda x: mask_sentence(sentence=x, masked_word=filtered_word_row['word'], tokenizer=tokenizer, detokenizer=detokenizer)\n",
    "        ).values\n",
    "    \n",
    "    # Concat to the whole df\n",
    "    masked_data = pd.concat([masked_data, tmp_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data.shape\n",
    "masked_data = masked_data.dropna()\n",
    "masked_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc66bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_data.to_csv('data/masked_content_covost2_for_en2de_no_sentence_group.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_data.to_csv('data/masked_content_winoMT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_data.to_csv(f'data/masked_content_{data_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab31ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
