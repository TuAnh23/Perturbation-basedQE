{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c5cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import tarfile\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189b7d6",
   "metadata": {},
   "source": [
    "# Use EN samples from covost2 data \n",
    "\n",
    "(all data except for the pairs that has DE, just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ea70e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_data = pd.DataFrame(columns=['SRC'])\n",
    "\n",
    "data_dir = \"data/covost2/EN-translations\"\n",
    "\n",
    "deduplicated_each_dataset = 0\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".tar.gz\") and ('de' not in filename):\n",
    "        unzipped_file_name = filename.replace(\".tar.gz\", \"\")\n",
    "        \n",
    "        # Extract the file if not yet done so\n",
    "        if not os.path.exists(os.path.join(data_dir, unzipped_file_name)):\n",
    "            tar = tarfile.open(os.path.join(data_dir, filename))\n",
    "            tar.extractall(data_dir)\n",
    "            tar.close()\n",
    "            \n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_df['SRC'] = pd.read_csv(os.path.join(data_dir, unzipped_file_name), sep='\\t')['translation']\n",
    "\n",
    "        deduplicated_each_dataset = deduplicated_each_dataset + tmp_df.drop_duplicates(subset='SRC').shape[0]\n",
    "        \n",
    "        original_data = pd.concat([original_data, tmp_df], axis=0, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "print(f\"all: {original_data.shape}\")\n",
    "print(f\"deduplicated_each_dataset: {deduplicated_each_dataset}\")\n",
    "print(f\"deduplicated all: {original_data.drop_duplicates(subset='SRC').shape}\")\n",
    "\n",
    "original_data = original_data.drop_duplicates(subset='SRC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c20da",
   "metadata": {},
   "source": [
    "Filter out the errornously long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84136d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = original_data['SRC'].apply(lambda x: len(x))\n",
    "\n",
    "length_stats = sentence_lengths.describe(percentiles=[.25, .5, .75, .99])\n",
    "\n",
    "original_data = original_data[sentence_lengths < length_stats['99%']]\n",
    "\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de2fee",
   "metadata": {},
   "source": [
    "Remove empty sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d554c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_data[original_data['SRC'] != \"\"]\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5d214",
   "metadata": {},
   "source": [
    "Remove the begining and end quotes for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda843b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Remove the begining and end quotes \n",
    "    \"\"\"\n",
    "    if (sentence.startswith('\\\"') and sentence.endswith('\\\"')) or \\\n",
    "        (sentence.startswith('“') and sentence.endswith('”')):\n",
    "        return sentence[1:-1]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "original_data['SRC'] = original_data['SRC'].apply(lambda x: prepare_sentence(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d2b93",
   "metadata": {},
   "source": [
    "Reindex after filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data['SRC'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3de20",
   "metadata": {},
   "source": [
    "# Use EN samples from winoMT data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b5fa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>SRC</th>\n",
       "      <th>noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>The developer argued with the designer because...</td>\n",
       "      <td>developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>5</td>\n",
       "      <td>The developer argued with the designer because...</td>\n",
       "      <td>designer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>The mechanic gave the clerk a present because ...</td>\n",
       "      <td>mechanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>4</td>\n",
       "      <td>The mechanic gave the clerk a present because ...</td>\n",
       "      <td>clerk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>The mover said thank you to the housekeeper be...</td>\n",
       "      <td>mover</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  x                                                SRC       noun\n",
       "0  female  1  The developer argued with the designer because...  developer\n",
       "1    male  5  The developer argued with the designer because...   designer\n",
       "2  female  1  The mechanic gave the clerk a present because ...   mechanic\n",
       "3    male  4  The mechanic gave the clerk a present because ...      clerk\n",
       "4  female  1  The mover said thank you to the housekeeper be...      mover"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data = pd.read_csv('data/winoMT_src.csv', index_col=0)\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032d049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec10c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da031451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2760e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7085a9",
   "metadata": {},
   "source": [
    "### Perform stemming on the data\n",
    "\n",
    "This would help reduce the vocab size, easier to later on choose the word to perturb\n",
    "\n",
    "ABORT: it reduce the vocab from 88066 to 71362, so not that much, so doesnt worth it. Also stemming makes the word invalid, so cannot use POS afterward to filter it out.\n",
    "\n",
    "Lemmatization would require defining POS --> not preferable, since we would want chinese and china to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5fcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_sentence(stemmer, sentence):\n",
    "#     \"\"\" \n",
    "#     Return the stemmed sentence and \n",
    "#     a dictionary mapping the stem to the original word in the sentence\n",
    "#     \"\"\"\n",
    "#     tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "#     stem_word_dict = {}\n",
    "#     stemmed_tokenized_sentence = []\n",
    "    \n",
    "#     for word in tokenized_sentence:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stemmed_tokenized_sentence.append(stem)\n",
    "#         stem_word_dict[stem] = word\n",
    "        \n",
    "#     return ' '.join(stemmed_tokenized_sentence), stem_word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73131ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# original_data['StemSRC'], original_data['StemDict'] = \\\n",
    "#     zip(*original_data.apply(lambda x: stem_sentence(stemmer, x['SRC']), axis=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd28fa",
   "metadata": {},
   "source": [
    "### Invesitigate in the frequencies of words across sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f77e3f",
   "metadata": {},
   "source": [
    "Count the number of occurance in sentence of each word. Here we **only use the sentences where the words only occurs 1 time**, which is convenient to analyse on the influence of the word on the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9c305d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3888, 1878)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import find, csr_matrix\n",
    "\n",
    "corpus = original_data['SRC'].values\n",
    "vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "count_fit = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Only consider the single occurance of a word in a sentence\n",
    "count_fit[count_fit > 1] = 0\n",
    "\n",
    "count_fit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba29f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging execution time: 0.20114994049072266 seconds\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import time \n",
    "\n",
    "\n",
    "word_df = pd.DataFrame()\n",
    "word_df['word'] = vectorizer.get_feature_names_out()\n",
    "word_df['freq'] = np.asarray(count_fit.sum(axis=0)).flatten()\n",
    "\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_pos_tag(word, spacy_model):\n",
    "    doc = spacy_model(word)\n",
    "    return [t.pos_ for t in doc][0]\n",
    "\n",
    "def nltk_pos_tag(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "def get_entity_name(word, spacy_model):\n",
    "    \"\"\"\n",
    "    Function returning the NER output from spacy on a word\n",
    "    Return None if the word does not have any entity name\n",
    "    Labels and there descriptions:\n",
    "    ```\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    labels = nlp.get_pipe('ner').labels\n",
    "    for label in labels:\n",
    "        print(f'{label}: {spacy.explain(label)}')\n",
    "    ```\n",
    "        CARDINAL: Numerals that do not fall under another type\n",
    "        DATE: Absolute or relative dates or periods\n",
    "        EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "        FAC: Buildings, airports, highways, bridges, etc.\n",
    "        GPE: Countries, cities, states\n",
    "        LANGUAGE: Any named language\n",
    "        LAW: Named documents made into laws.\n",
    "        LOC: Non-GPE locations, mountain ranges, bodies of water\n",
    "        MONEY: Monetary values, including unit\n",
    "        NORP: Nationalities or religious or political groups\n",
    "        ORDINAL: \"first\", \"second\", etc.\n",
    "        ORG: Companies, agencies, institutions, etc.\n",
    "        PERCENT: Percentage, including \"%\"\n",
    "        PERSON: People, including fictional\n",
    "        PRODUCT: Objects, vehicles, foods, etc. (not services)\n",
    "        QUANTITY: Measurements, as of weight or distance\n",
    "        TIME: Times smaller than a day\n",
    "        WORK_OF_ART: Titles of books, songs, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = spacy_model(word)\n",
    "    for w in doc.ents:\n",
    "        return w.label_\n",
    "\n",
    "start = time.time()\n",
    "word_df['POS'] = word_df['word'].apply(lambda x: nltk_pos_tag(x))\n",
    "print(f\"POS tagging execution time: {time.time() - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc84c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8851bcd7",
   "metadata": {},
   "source": [
    "Have a look at the most frequent content words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916fd5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>protestors</td>\n",
       "      <td>2</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>dress</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>protective</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>protect</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>prosecution</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>drew</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>prompt</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>drives</td>\n",
       "      <td>2</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>zone</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>herself</td>\n",
       "      <td>1</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  freq  POS\n",
       "1284   protestors     2  NNS\n",
       "504         dress     2   NN\n",
       "1282   protective     2   NN\n",
       "1280      protect     2   NN\n",
       "1279  prosecution     2   NN\n",
       "505          drew     2   NN\n",
       "1274       prompt     2   NN\n",
       "508        drives     2  NNS\n",
       "1877         zone     2   NN\n",
       "788       herself     1   NN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 9999999)\n",
    "\n",
    "def is_content_tag(nltk_pos):\n",
    "    content_tags_prefix = ['NN'] #, 'V', 'JJ', 'PRP']  # Noun, verb, adj, adv (RB, but removed), pronoun\n",
    "    for prefix in content_tags_prefix:\n",
    "        if nltk_pos.startswith(prefix):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_stopword(word):\n",
    "    # Manually define some stopwords (words that dont contain much content, or errornous)\n",
    "    stopwords = ['is', 'are', 'was', 'were', 'am', 'be', \n",
    "                 'not', 'let',\n",
    "                 'have', 'has', 'had', \n",
    "                 'de', 'la', 'du', 're', 'sur', 'des', 'le', 'll', \n",
    "                 'oh', 'lot', 'les', 'ah', 'en', 've',\n",
    "                 'didn', 'bois']\n",
    "    return word in stopwords\n",
    "\n",
    "\n",
    "content_word_bool = word_df['POS'].apply(lambda x: is_content_tag(x)) \\\n",
    "    & (~word_df['word'].apply(lambda x: is_stopword(x)))\n",
    "\n",
    "word_df[content_word_bool].sort_values(\n",
    "    by='freq', ascending=False\n",
    ").tail(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e66a8",
   "metadata": {},
   "source": [
    "### Create input data where we mask a set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85a7e5",
   "metadata": {},
   "source": [
    "#### Set of regional words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c959e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_regional_tag(spacy_ner):\n",
    "#     regional_tags = ['GPE', 'LANGUAGE', 'NORP']\n",
    "#     return spacy_ner in regional_tags\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# word_df['NER'] = word_df['word'].apply(lambda x: get_entity_name(x, spacy_model))\n",
    "# print(f\"NER execution time: {time.time() - start} seconds\")\n",
    "\n",
    "\n",
    "# regional_word_bool = word_df['NER'].apply(lambda x: is_regional_tag(x)) \\\n",
    "#     & (~word_df['word'].apply(lambda x: is_stopword(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f987d",
   "metadata": {},
   "source": [
    "#### Set of all content words that is frequent in the inference data\n",
    "\n",
    "We select the words that appears in over 50 sentences.\n",
    "\n",
    "When we dont need to group sentences with the same masked word, we keep the word freq over sentences lower (10) just to filter out the weird rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570e704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_SENTENCES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cbd543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/10_vywcj3lb14yk3sn91jn3c0000gn/T/ipykernel_54490/3411812722.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  word_df[content_word_bool][word_df['freq'] > NR_OF_SENTENCES].sort_values('freq', ascending=False).head()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>someone</td>\n",
       "      <td>362</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>told</td>\n",
       "      <td>318</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>help</td>\n",
       "      <td>196</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>accountant</td>\n",
       "      <td>180</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>janitor</td>\n",
       "      <td>180</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  freq POS\n",
       "1523     someone   362  NN\n",
       "1693        told   318  NN\n",
       "782         help   196  NN\n",
       "22    accountant   180  NN\n",
       "894      janitor   180  NN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df[content_word_bool][word_df['freq'] > NR_OF_SENTENCES].sort_values('freq', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b76d13",
   "metadata": {},
   "source": [
    "Filter out the strange words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6cc73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Filter out the words that has all punctuations in it\n",
    "    all_puncts = True\n",
    "    for char in word:\n",
    "        if char not in string.punctuation:\n",
    "            all_puncts = False\n",
    "            break\n",
    "    if all_puncts:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    # Filter out the words with strange characters in it\n",
    "    # Strange characters are punctuations, except ' . -\n",
    "    strange_characters = string.punctuation.replace(\"\\'\", '').replace(\".\", '').replace(\"-\", '')\n",
    "    for char in strange_characters:\n",
    "        if char in word:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "valid_word_bool = word_df['word'].apply(is_valid_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d069c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_bool = content_word_bool & (word_df['freq'] > NR_OF_SENTENCES) & valid_word_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83baf82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(filtered_word_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5cd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e80ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mask_sentence(sentence, masked_word):\n",
    "    \"\"\"\n",
    "        sentence: the original sentence without preprocessing\n",
    "        masked_word: the word to be masked (in lowercase)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the location of the word in the sentence\n",
    "    word_locations = [m.start() for m in re.finditer(masked_word, sentence.lower())]\n",
    "    \n",
    "    if len(word_locations) == 1:\n",
    "        final_word_location = word_locations[0]\n",
    "    else:\n",
    "        # Make sure that it is actually a standalone word (e.g., 'HE' and not 'tHE')\n",
    "        final_word_location = None\n",
    "        for x in word_locations:\n",
    "            # Make sure the character before and after the word is not alphabet\n",
    "            if (x == 0 or (not sentence.lower()[x-1].isalpha())) and \\\n",
    "                (x + len(masked_word) == len(sentence) or (not sentence.lower()[x + len(masked_word)].isalpha())):\n",
    "                final_word_location = x\n",
    "                break\n",
    "                \n",
    "        if final_word_location is None:\n",
    "            print(sentence)\n",
    "            print(masked_word)\n",
    "\n",
    "        assert final_word_location is not None\n",
    "    \n",
    "    return sentence[:final_word_location] + '[MASK]' + sentence[final_word_location+len(masked_word):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17cb3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data = pd.DataFrame(columns=['SRC', 'SRC_masked', 'original_word'])\n",
    "\n",
    "filtered_word_df = word_df[filtered_word_bool]\n",
    "\n",
    "for word_index, filtered_word_row in filtered_word_df.iterrows():\n",
    "    # Indices of the sentences that contains the word\n",
    "    sentence_indices = count_fit.transpose()[word_index].nonzero()[1]\n",
    "    \n",
    "#     # Randomly select a fixed number of sentences\n",
    "#     sentence_indices = np.random.choice(a=sentence_indices, \n",
    "#                                         size=NR_OF_SENTENCES, \n",
    "#                                         replace=False)\n",
    "    \n",
    "    # Create a temporary df to store the sentences for this word\n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df['SRC_original_idx'] = sentence_indices\n",
    "    tmp_df['SRC'] = original_data.loc[sentence_indices, 'SRC'].values\n",
    "    tmp_df['original_word'] = filtered_word_row['word']\n",
    "    \n",
    "    # Mask the word in those sentences\n",
    "    tmp_df['SRC_masked'] = \\\n",
    "        original_data.loc[sentence_indices, 'SRC'].apply(\n",
    "        lambda x: mask_sentence(sentence=x, masked_word=filtered_word_row['word'])\n",
    "        ).values\n",
    "    \n",
    "    # Concat to the whole df\n",
    "    masked_data = pd.concat([masked_data, tmp_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dace7cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16738, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cba1033c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRC</th>\n",
       "      <th>SRC_masked</th>\n",
       "      <th>original_word</th>\n",
       "      <th>SRC_original_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The patient disclosed to the counselor that he had a history of substance abuse.</td>\n",
       "      <td>The patient disclosed to the counselor that he had a history of substance [MASK].</td>\n",
       "      <td>abuse</td>\n",
       "      <td>3258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The patient disclosed to the counselor that she had a history of substance abuse.</td>\n",
       "      <td>The patient disclosed to the counselor that she had a history of substance [MASK].</td>\n",
       "      <td>abuse</td>\n",
       "      <td>3259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The patient disclosed to the counselor that they had a history of substance abuse.</td>\n",
       "      <td>The patient disclosed to the counselor that they had a history of substance [MASK].</td>\n",
       "      <td>abuse</td>\n",
       "      <td>3260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Someone disclosed to the counselor that he had a history of substance abuse.</td>\n",
       "      <td>Someone disclosed to the counselor that he had a history of substance [MASK].</td>\n",
       "      <td>abuse</td>\n",
       "      <td>3261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Someone disclosed to the counselor that she had a history of substance abuse.</td>\n",
       "      <td>Someone disclosed to the counselor that she had a history of substance [MASK].</td>\n",
       "      <td>abuse</td>\n",
       "      <td>3262.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  SRC  \\\n",
       "0    The patient disclosed to the counselor that he had a history of substance abuse.   \n",
       "1   The patient disclosed to the counselor that she had a history of substance abuse.   \n",
       "2  The patient disclosed to the counselor that they had a history of substance abuse.   \n",
       "3        Someone disclosed to the counselor that he had a history of substance abuse.   \n",
       "4       Someone disclosed to the counselor that she had a history of substance abuse.   \n",
       "\n",
       "                                                                            SRC_masked  \\\n",
       "0    The patient disclosed to the counselor that he had a history of substance [MASK].   \n",
       "1   The patient disclosed to the counselor that she had a history of substance [MASK].   \n",
       "2  The patient disclosed to the counselor that they had a history of substance [MASK].   \n",
       "3        Someone disclosed to the counselor that he had a history of substance [MASK].   \n",
       "4       Someone disclosed to the counselor that she had a history of substance [MASK].   \n",
       "\n",
       "  original_word  SRC_original_idx  \n",
       "0         abuse            3258.0  \n",
       "1         abuse            3259.0  \n",
       "2         abuse            3260.0  \n",
       "3         abuse            3261.0  \n",
       "4         abuse            3262.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdc66bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_data.to_csv('data/masked_content_covost2_for_en2de_no_sentence_group.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "505c78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_data.to_csv('data/masked_content_winoMT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e49cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
