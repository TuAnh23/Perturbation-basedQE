{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2189b7d6",
   "metadata": {},
   "source": [
    "Use EN samples from covost2 data (all data except for the pairs that has DE, just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8ea70e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all: (312409, 1)\n",
      "deduplicated_each_dataset: 108694\n",
      "deduplicated all: (108453, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "original_data = pd.DataFrame(columns=['SRC'])\n",
    "\n",
    "data_dir = \"data/covost2/EN-translations\"\n",
    "\n",
    "deduplicated_each_dataset = 0\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".tar.gz\") and ('de' not in filename):\n",
    "        unzipped_file_name = filename.replace(\".tar.gz\", \"\")\n",
    "        \n",
    "        # Extract the file if not yet done so\n",
    "        if not os.path.exists(os.path.join(data_dir, unzipped_file_name)):\n",
    "            tar = tarfile.open(os.path.join(data_dir, filename))\n",
    "            tar.extractall(data_dir)\n",
    "            tar.close()\n",
    "            \n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_df['SRC'] = pd.read_csv(os.path.join(data_dir, unzipped_file_name), sep='\\t')['translation']\n",
    "\n",
    "        deduplicated_each_dataset = deduplicated_each_dataset + tmp_df.drop_duplicates(subset='SRC').shape[0]\n",
    "        \n",
    "        original_data = pd.concat([original_data, tmp_df], axis=0, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "print(f\"all: {original_data.shape}\")\n",
    "print(f\"deduplicated_each_dataset: {deduplicated_each_dataset}\")\n",
    "print(f\"deduplicated all: {original_data.drop_duplicates(subset='SRC').shape}\")\n",
    "\n",
    "original_data = original_data.drop_duplicates(subset='SRC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c20da",
   "metadata": {},
   "source": [
    "Filter out the errornously long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84136d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107358, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lengths = original_data['SRC'].apply(lambda x: len(x))\n",
    "\n",
    "length_stats = sentence_lengths.describe(percentiles=[.25, .5, .75, .99])\n",
    "\n",
    "original_data = original_data[sentence_lengths < length_stats['99%']]\n",
    "\n",
    "original_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7085a9",
   "metadata": {},
   "source": [
    "### Perform stemming on the data\n",
    "\n",
    "This would help reduce the vocab size, easier to later on choose the word to perturb\n",
    "\n",
    "ABORT: it reduce the vocab from 88066 to 71362, so not that much, so doesnt worth it. Also stemming makes the word invalid, so cannot use POS afterward to filter it out.\n",
    "\n",
    "Lemmatization would require defining POS --> not preferable, since we would want chinese and china to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5fcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_sentence(stemmer, sentence):\n",
    "#     \"\"\" \n",
    "#     Return the stemmed sentence and \n",
    "#     a dictionary mapping the stem to the original word in the sentence\n",
    "#     \"\"\"\n",
    "#     tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "#     stem_word_dict = {}\n",
    "#     stemmed_tokenized_sentence = []\n",
    "    \n",
    "#     for word in tokenized_sentence:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stemmed_tokenized_sentence.append(stem)\n",
    "#         stem_word_dict[stem] = word\n",
    "        \n",
    "#     return ' '.join(stemmed_tokenized_sentence), stem_word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73131ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# original_data['StemSRC'], original_data['StemDict'] = \\\n",
    "#     zip(*original_data.apply(lambda x: stem_sentence(stemmer, x['SRC']), axis=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd28fa",
   "metadata": {},
   "source": [
    "### Invesitigate in the frequencies of words across sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f77e3f",
   "metadata": {},
   "source": [
    "Count the number of occurance in sentence of each word. Here we **only use the sentences where the words only occurs 1 time**, which is convenient to analyse on the influence of the word on the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9c305d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107358, 46970)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import find, csr_matrix\n",
    "\n",
    "corpus = original_data['SRC'].values\n",
    "vectorizer = CountVectorizer()\n",
    "count_fit = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Only consider the single occurance of a word in a sentence\n",
    "count_fit[count_fit > 1] = 0\n",
    "\n",
    "count_fit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba29f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging execution time: 4.194782018661499 seconds\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import time \n",
    "\n",
    "\n",
    "word_df = pd.DataFrame()\n",
    "word_df['word'] = vectorizer.get_feature_names_out()\n",
    "word_df['freq'] = np.asarray(count_fit.sum(axis=0)).flatten()\n",
    "\n",
    "\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_pos_tag(word, spacy_model):\n",
    "    doc = spacy_model(word)\n",
    "    return [t.pos_ for t in doc][0]\n",
    "\n",
    "def nltk_pos_tag(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "def get_entity_name(word, spacy_model):\n",
    "    \"\"\"\n",
    "    Function returning the NER output from spacy on a word\n",
    "    Return None if the word does not have any entity name\n",
    "    Labels and there descriptions:\n",
    "    ```\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    labels = nlp.get_pipe('ner').labels\n",
    "    for label in labels:\n",
    "        print(f'{label}: {spacy.explain(label)}')\n",
    "    ```\n",
    "        CARDINAL: Numerals that do not fall under another type\n",
    "        DATE: Absolute or relative dates or periods\n",
    "        EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "        FAC: Buildings, airports, highways, bridges, etc.\n",
    "        GPE: Countries, cities, states\n",
    "        LANGUAGE: Any named language\n",
    "        LAW: Named documents made into laws.\n",
    "        LOC: Non-GPE locations, mountain ranges, bodies of water\n",
    "        MONEY: Monetary values, including unit\n",
    "        NORP: Nationalities or religious or political groups\n",
    "        ORDINAL: \"first\", \"second\", etc.\n",
    "        ORG: Companies, agencies, institutions, etc.\n",
    "        PERCENT: Percentage, including \"%\"\n",
    "        PERSON: People, including fictional\n",
    "        PRODUCT: Objects, vehicles, foods, etc. (not services)\n",
    "        QUANTITY: Measurements, as of weight or distance\n",
    "        TIME: Times smaller than a day\n",
    "        WORK_OF_ART: Titles of books, songs, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = spacy_model(word)\n",
    "    for w in doc.ents:\n",
    "        return w.label_\n",
    "\n",
    "start = time.time()\n",
    "word_df['POS'] = word_df['word'].apply(lambda x: nltk_pos_tag(x))\n",
    "print(f\"POS tagging execution time: {time.time() - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc84c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8851bcd7",
   "metadata": {},
   "source": [
    "Have a look at the most frequent content words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916fd5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27209</th>\n",
       "      <td>microlenecamptus</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8665</th>\n",
       "      <td>chooz</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24007</th>\n",
       "      <td>langfang</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28865</th>\n",
       "      <td>narsingdi</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28864</th>\n",
       "      <td>narrows</td>\n",
       "      <td>0</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18933</th>\n",
       "      <td>guanosine</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>banco</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41119</th>\n",
       "      <td>talbot</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28812</th>\n",
       "      <td>nanshu</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29917</th>\n",
       "      <td>ohlungen</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word  freq  POS\n",
       "27209  microlenecamptus     0   NN\n",
       "8665              chooz     0   NN\n",
       "24007          langfang     0   NN\n",
       "28865         narsingdi     0   NN\n",
       "28864           narrows     0  NNS\n",
       "18933         guanosine     0   NN\n",
       "3933              banco     0   NN\n",
       "41119            talbot     0   NN\n",
       "28812            nanshu     0   NN\n",
       "29917          ohlungen     0   NN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 9999999)\n",
    "\n",
    "def is_content_tag(nltk_pos):\n",
    "    content_tags_prefix = ['NN'] #, 'V', 'JJ', 'PRP']  # Noun, verb, adj, adv (RB, but removed), pronoun\n",
    "    for prefix in content_tags_prefix:\n",
    "        if nltk_pos.startswith(prefix):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_stopword(word):\n",
    "    # Manually define some stopwords (words that dont contain much content, or errornous)\n",
    "    stopwords = ['is', 'are', 'was', 'were', 'am', 'be', \n",
    "                 'not', 'let',\n",
    "                 'have', 'has', 'had', \n",
    "                 'de', 'la', 'du', 're', 'sur', 'des', 'le', 'll', \n",
    "                 'oh', 'lot', 'les', 'ah', 'en', 've',\n",
    "                 'didn', 'bois']\n",
    "    return word in stopwords\n",
    "\n",
    "\n",
    "content_word_bool = word_df['POS'].apply(lambda x: is_content_tag(x)) \\\n",
    "    & (~word_df['word'].apply(lambda x: is_stopword(x)))\n",
    "\n",
    "word_df[content_word_bool].sort_values(\n",
    "    by='freq', ascending=False\n",
    ").tail(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e66a8",
   "metadata": {},
   "source": [
    "### Create input data where we mask a set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85a7e5",
   "metadata": {},
   "source": [
    "#### Set of regional words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c959e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_regional_tag(spacy_ner):\n",
    "#     regional_tags = ['GPE', 'LANGUAGE', 'NORP']\n",
    "#     return spacy_ner in regional_tags\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# word_df['NER'] = word_df['word'].apply(lambda x: get_entity_name(x, spacy_model))\n",
    "# print(f\"NER execution time: {time.time() - start} seconds\")\n",
    "\n",
    "\n",
    "# regional_word_bool = word_df['NER'].apply(lambda x: is_regional_tag(x)) \\\n",
    "#     & (~word_df['word'].apply(lambda x: is_stopword(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f987d",
   "metadata": {},
   "source": [
    "#### Set of all content words that is frequent in the inference data\n",
    "\n",
    "We select the words that appears in over 50 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570e704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_SENTENCES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cbd543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/10_vywcj3lb14yk3sn91jn3c0000gn/T/ipykernel_6819/3411812722.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  word_df[content_word_bool][word_df['freq'] > NR_OF_SENTENCES].sort_values('freq', ascending=False).head()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40182</th>\n",
       "      <td>street</td>\n",
       "      <td>3600</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29620</th>\n",
       "      <td>number</td>\n",
       "      <td>2171</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13257</th>\n",
       "      <td>don</td>\n",
       "      <td>1858</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41866</th>\n",
       "      <td>thousand</td>\n",
       "      <td>1855</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>amendment</td>\n",
       "      <td>1848</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  freq POS\n",
       "40182     street  3600  NN\n",
       "29620     number  2171  NN\n",
       "13257        don  1858  NN\n",
       "41866   thousand  1855  NN\n",
       "2012   amendment  1848  NN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df[content_word_bool][word_df['freq'] > NR_OF_SENTENCES].sort_values('freq', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9d069c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_bool = content_word_bool & (word_df['freq'] > NR_OF_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83baf82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(filtered_word_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e68a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17948f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635d330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e80ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mask_sentence(sentence, masked_word):\n",
    "    \"\"\"\n",
    "        sentence: the original sentence without preprocessing\n",
    "        masked_word: the word to be masked (in lowercase)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the location of the word in the sentence\n",
    "    word_locations = [m.start() for m in re.finditer(masked_word, sentence.lower())]\n",
    "    \n",
    "    # Make sure that it is actually a standalone word (e.g., 'HE' and not 'tHE')\n",
    "    final_word_location = None\n",
    "    for x in word_locations:\n",
    "        # Make sure the character before and after the word is not alphabet\n",
    "        if (x == 0 or (not sentence.lower()[x-1].isalpha())) and \\\n",
    "            (x + len(masked_word) == len(sentence) or (not sentence.lower()[x + len(masked_word)].isalpha())):\n",
    "            final_word_location = x\n",
    "            break\n",
    "    \n",
    "    assert final_word_location is not None\n",
    "    \n",
    "    return sentence[:final_word_location] + '[MASK]' + sentence[final_word_location+len(masked_word):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17cb3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data = pd.DataFrame(columns=['SRC', 'SRC_masked', 'original_word'])\n",
    "\n",
    "filtered_word_df = word_df[filtered_word_bool]\n",
    "\n",
    "for word_index, filtered_word_row in filtered_word_df.iterrows():\n",
    "    # Indices of the sentences that contains the word\n",
    "    sentence_indices = count_fit.transpose()[word_index].nonzero()[1]\n",
    "    \n",
    "    # Randomly select a fixed number of sentences\n",
    "    sentence_indices = np.random.choice(a=sentence_indices, \n",
    "                                        size=NR_OF_SENTENCES, \n",
    "                                        replace=False)\n",
    "    \n",
    "    # Create a temporary df to store the sentences for this word\n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df['SRC'] = original_data.loc[sentence_indices, 'SRC'].values\n",
    "    tmp_df['original_word'] = filtered_word_row['word']\n",
    "    \n",
    "    # Mask the word in those sentences\n",
    "    tmp_df['SRC_masked'] = \\\n",
    "        original_data.loc[sentence_indices, 'SRC'].apply(\n",
    "        lambda x: mask_sentence(sentence=x, masked_word=filtered_word_row['word'])\n",
    "        ).values\n",
    "    \n",
    "    # Concat to the whole df\n",
    "    masked_data = pd.concat([masked_data, tmp_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dace7cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56250, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cba1033c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRC</th>\n",
       "      <th>SRC_masked</th>\n",
       "      <th>original_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Don’t accept.”</td>\n",
       "      <td>“Don’t [MASK].”</td>\n",
       "      <td>accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please forgive us, but we would systematically come back to these proposals even if you do not accept them.</td>\n",
       "      <td>Please forgive us, but we would systematically come back to these proposals even if you do not [MASK] them.</td>\n",
       "      <td>accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accept the sanitary logic.</td>\n",
       "      <td>[MASK] the sanitary logic.</td>\n",
       "      <td>accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I accept? I can not</td>\n",
       "      <td>I [MASK]? I can not</td>\n",
       "      <td>accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Be brave and proud enough to accept them.</td>\n",
       "      <td>Be brave and proud enough to [MASK] them.</td>\n",
       "      <td>accept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           SRC  \\\n",
       "0                                                                                              “Don’t accept.”   \n",
       "1  Please forgive us, but we would systematically come back to these proposals even if you do not accept them.   \n",
       "2                                                                                   Accept the sanitary logic.   \n",
       "3                                                                                          I accept? I can not   \n",
       "4                                                                    Be brave and proud enough to accept them.   \n",
       "\n",
       "                                                                                                    SRC_masked  \\\n",
       "0                                                                                              “Don’t [MASK].”   \n",
       "1  Please forgive us, but we would systematically come back to these proposals even if you do not [MASK] them.   \n",
       "2                                                                                   [MASK] the sanitary logic.   \n",
       "3                                                                                          I [MASK]? I can not   \n",
       "4                                                                    Be brave and proud enough to [MASK] them.   \n",
       "\n",
       "  original_word  \n",
       "0        accept  \n",
       "1        accept  \n",
       "2        accept  \n",
       "3        accept  \n",
       "4        accept  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdc66bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data.to_csv('data/masked_content_covost2_for_en2de.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c78f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
