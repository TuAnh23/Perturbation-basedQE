{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03046c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import random\n",
    "from difflib import SequenceMatcher\n",
    "from scipy import stats\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import edist.sed as sed\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6c851",
   "metadata": {},
   "source": [
    "German word2vec model Facebook https://fasttext.cc/docs/en/crawl-vectors.html (cc.de.300.bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2415041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://gitlab.ub.uni-bielefeld.de/bpaassen/python-edit-distances/-/blob/master/sed_demo.ipynb\n",
    "def levenshtein(s1, s2):\n",
    "    return sed.standard_sed(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f222dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/difflib.html\n",
    "    \n",
    "def changes_spread(original_tokenized, changed_tokenized, opcodes):\n",
    "    start_change = -1\n",
    "    end_change = -1\n",
    "    for opcode in opcodes:\n",
    "        if opcode[0] != 'equal':\n",
    "            start_change = opcode[1]\n",
    "            break\n",
    "    for opcode in reversed(opcodes):\n",
    "        if opcode[0] != 'equal':\n",
    "            end_change = opcode[2]\n",
    "            break\n",
    "    return max(0, end_change-start_change)/len(changed_tokenized)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8464b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_in_capital(sentence_tokenized, highlight_positions):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        sentence_tokenized: tokenzied sentence\n",
    "        highlight_positions: list of 2-sized tuples: [(p1, p2), (p3,p4), ...]\n",
    "            where we want to highlight sentence[p1:p2], sentence[p3:p4]\n",
    "    \"\"\"\n",
    "    highlighted_sentence = []\n",
    "    \n",
    "    last = 0  # index of the last position added to the new sentence\n",
    "    for (start, stop) in highlight_positions:\n",
    "        highlighted_sentence.extend(\n",
    "            sentence_tokenized[last:start] + \\\n",
    "            [w.upper() for w in sentence_tokenized[start:stop]]\n",
    "        )\n",
    "        last = stop\n",
    "    if last < len(sentence_tokenized):\n",
    "        highlighted_sentence.extend(\n",
    "            sentence_tokenized[last:]\n",
    "        )\n",
    "    return ' '.join(highlighted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de55b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_chunk_changed(original_tokenized, changed_tokenized, opcodes, \n",
    "                      chunk_max_length=1, spacy_model=None, w2v_model=None):\n",
    "    # Return the original and changed sentences with the chunk highlighted in capital\n",
    "    # Return whether this sentence has only two chunk changes within the max length. \n",
    "    # And return the distance between the two changed chunks\n",
    "    \n",
    "    is_two_chunk_changed = False\n",
    "    chunk_distance = pd.NA\n",
    "    is_same_subtree = pd.NA\n",
    "    changes_similarity = pd.NA\n",
    "    \n",
    "    \n",
    "    \n",
    "    changes_types = [o[0] for o in opcodes]\n",
    "    \n",
    "    # If not exactly two changes, return\n",
    "    if not (all(changes_type == 'replace' or changes_type == 'equal' for changes_type in changes_types) and \\\n",
    "        changes_types.count('replace') == 2):\n",
    "        return is_two_chunk_changed, chunk_distance, is_same_subtree, changes_similarity\n",
    "    \n",
    "    # Find the positions of the two changed chunks\n",
    "    i_replace = [i for i, change in enumerate(changes_types) if change == \"replace\"]\n",
    "    \n",
    "    # If two changed chunks not have length less than chunk_max_length, return\n",
    "    if not (opcodes[i_replace[0]][2] - opcodes[i_replace[0]][1] <= chunk_max_length and \\\n",
    "            opcodes[i_replace[1]][2] - opcodes[i_replace[1]][1] <= chunk_max_length):\n",
    "        return is_two_chunk_changed, chunk_distance, is_same_subtree, changes_similarity\n",
    "    \n",
    "    # At this point, this should be a valid two_chunk within length change\n",
    "    is_two_chunk_changed = True\n",
    "    \n",
    "    # Check if there is indeed an equal chunks in between of the two changed chunk\n",
    "    # Calculate the distance between two chunks = the equal chunk in between\n",
    "    i_equal_in_between = (i_replace[1] + i_replace[0]) // 2\n",
    "    assert opcodes[i_equal_in_between][0] == 'equal'\n",
    "    chunk_distance = opcodes[i_equal_in_between][2] - opcodes[i_equal_in_between][1]\n",
    "\n",
    "\n",
    "    if spacy_model is not None: \n",
    "        # In the two_chunk_changed case when chunk_max_length=1, i.e., only two words are changed \n",
    "        # comparing to the original translation\n",
    "        # Check if the two changed words are in the same sub tree of the dependency tree\n",
    "        if (opcodes[i_replace[0]][4] - opcodes[i_replace[0]][3] == 1 and \\\n",
    "            opcodes[i_replace[1]][4] - opcodes[i_replace[1]][3] == 1):\n",
    "            # Find the ancestors and children of the two changed words\n",
    "            doc = spacy_model(' '.join(changed_tokenized))\n",
    "            token1, token2 = None, None\n",
    "            family1, family2 = None, None\n",
    "            for token in doc:\n",
    "                if token.text == changed_tokenized[opcodes[i_replace[0]][3]]:\n",
    "                    token1 = token.text\n",
    "                    family1 = list(token.ancestors) + list(token.children)\n",
    "                    family1 = [t.text for t in family1]\n",
    "                elif token.text == changed_tokenized[opcodes[i_replace[1]][3]]:\n",
    "                    token2 = token.text\n",
    "                    family2 = list(token.ancestors) + list(token.children)\n",
    "                    family2 = [t.text for t in family2]\n",
    "\n",
    "            if token1 is None or token2 is None:\n",
    "                is_same_subtree = pd.NA\n",
    "            else:\n",
    "                if token1 in family2 or token2 in family1:\n",
    "                    is_same_subtree = True\n",
    "                else:\n",
    "                    is_same_subtree = False\n",
    "\n",
    "\n",
    "    # Calculate the senmatic similarity of the two changed words (cosine similarity in [-1, 1])\n",
    "    if w2v_model is not None:\n",
    "        # Can only calculate when only two single tokens are changed\n",
    "        if (opcodes[i_replace[0]][4] - opcodes[i_replace[0]][3] == 1 and \\\n",
    "            opcodes[i_replace[1]][4] - opcodes[i_replace[1]][3] == 1 and \\\n",
    "            opcodes[i_replace[0]][2] - opcodes[i_replace[0]][1] == 1 and \\\n",
    "            opcodes[i_replace[1]][2] - opcodes[i_replace[1]][1] == 1):\n",
    "\n",
    "            original_word_1 = original_tokenized[opcodes[i_replace[0]][1]]\n",
    "            changed_word_1 = changed_tokenized[opcodes[i_replace[0]][3]]\n",
    "\n",
    "            original_word_2 = original_tokenized[opcodes[i_replace[1]][1]]\n",
    "            changed_word_2 = changed_tokenized[opcodes[i_replace[1]][3]]\n",
    "\n",
    "            if original_word_1 in w2v_model.index_to_key and original_word_2 in w2v_model.index_to_key and \\\n",
    "                changed_word_1 in w2v_model.index_to_key and changed_word_2 in w2v_model.index_to_key:\n",
    "                changes_similarity = [{'original_word': original_word_1, \n",
    "                                       'changed_word': changed_word_1, \n",
    "                                       'semantic_similarity': w2v_model.similarity(original_word_1, changed_word_1)},\n",
    "                                      {'original_word': original_word_2,\n",
    "                                       'changed_word': changed_word_2,\n",
    "                                       'semantic_similarity': w2v_model.similarity(original_word_2, changed_word_2)}]\n",
    "\n",
    "\n",
    "    return is_two_chunk_changed, chunk_distance, is_same_subtree, changes_similarity\n",
    "    \n",
    "    \n",
    "def highlight_changes(original_tokenized, changed_tokenized, opcodes):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        original_tokenized: tokenized original sentence\n",
    "        changed_tokenized: tokenized changed sentence\n",
    "        opcodes: changes to get from `original_tokenized` to `changed_tokenized`\n",
    "    Returns:\n",
    "        original_sentence and changed_sentence with the changes highlighted in capital\n",
    "    \"\"\"\n",
    "    \n",
    "    highlighted_original_sentence_positions = []\n",
    "    highlighted_changed_sentence_positions = []\n",
    "    \n",
    "    for opcode in opcodes:\n",
    "        tag, i1, i2, j1, j2 = opcode[0], opcode[1], opcode[2], opcode[3], opcode[4]\n",
    "        \n",
    "        if tag != 'equal':\n",
    "            highlighted_original_sentence_positions.append((i1, i2))\n",
    "            highlighted_changed_sentence_positions.append((j1, j2))\n",
    "            \n",
    "    original_sentence_highlighted = highlight_in_capital(\n",
    "        sentence_tokenized=original_tokenized, \n",
    "        highlight_positions=highlighted_original_sentence_positions\n",
    "    )\n",
    "    \n",
    "    changed_sentence_highlighted = highlight_in_capital(\n",
    "        sentence_tokenized=changed_tokenized, \n",
    "        highlight_positions=highlighted_changed_sentence_positions\n",
    "    )\n",
    "    \n",
    "    return original_sentence_highlighted, changed_sentence_highlighted\n",
    "    \n",
    "    \n",
    "def calculate_change(original, changed):\n",
    "    # Return the original and changed sentences with the changes highlighted in capital\n",
    "    \n",
    "    original_tokenized = nltk.word_tokenize(original)\n",
    "    changed_tokenized = nltk.word_tokenize(changed)\n",
    "    \n",
    "    opcodes = SequenceMatcher(None, original_tokenized, changed_tokenized).get_opcodes()\n",
    "    \n",
    "    # Convert the opcodes (displayed by word index) to changes in words\n",
    "    changes = []\n",
    "    for opcode in opcodes:\n",
    "        tag, i1, i2, j1, j2 = opcode[0], opcode[1], opcode[2], opcode[3], opcode[4]\n",
    "        if tag != 'equal':\n",
    "            changes.append((tag, ' '.join(original_tokenized[i1:i2]), ' '.join(changed_tokenized[j1:j2])))\n",
    "    \n",
    "    return original_tokenized, changed_tokenized, opcodes, changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b46ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alignment(path_prefix):\n",
    "    alignment_file_path = f\"{path_prefix}_word_alignment.txt\"\n",
    "    if not os.path.isfile(alignment_file_path):\n",
    "        raise RuntimeError(\"Alignment file not exist.\")\n",
    "        \n",
    "    else:\n",
    "        with open(alignment_file_path) as f:\n",
    "            lines = [line.rstrip() for line in f]\n",
    "            \n",
    "        translation_alignment = []\n",
    "        for line in lines:\n",
    "            word_pairs = line.split()\n",
    "            word_pairs = [word_pair.split('<sep>') for word_pair in word_pairs]\n",
    "            translation_alignment.append(dict(word_pairs))\n",
    "        return translation_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d9c162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reason_of_change(alignment, changes, perturbed_src_word):\n",
    "    if type(changes) != list:\n",
    "        return pd.NA\n",
    "    elif perturbed_src_word not in alignment.keys():\n",
    "        changes[0]['change_type'] = None\n",
    "        changes[1]['change_type'] = None\n",
    "    elif alignment[perturbed_src_word] == changes[0]['changed_word'] and alignment[perturbed_src_word] == changes[1]['changed_word']:\n",
    "        # Both changes are due to perturbation --> weird --> pass\n",
    "        changes[0]['change_type'] = None\n",
    "        changes[1]['change_type'] = None\n",
    "    elif alignment[perturbed_src_word] != changes[0]['changed_word'] and alignment[perturbed_src_word] != changes[1]['changed_word']:\n",
    "        # Both changes NOT due to perturbation --> weird --> pass\n",
    "        changes[0]['change_type'] = None\n",
    "        changes[1]['change_type'] = None\n",
    "    elif alignment[perturbed_src_word] == changes[0]['changed_word']:\n",
    "        changes[0]['change_type'] = \"perturbed\"\n",
    "        changes[1]['change_type'] = \"not_perturbed\"\n",
    "    elif alignment[perturbed_src_word] == changes[1]['changed_word']:\n",
    "        changes[0]['change_type'] = \"not_perturbed\"\n",
    "        changes[1]['change_type'] = \"perturbed\"\n",
    "        \n",
    "    return changes\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842ed6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_not_perturbed_change(changes, spacy_model):\n",
    "    if type(changes) != list:\n",
    "        return pd.NA\n",
    "    elif changes[0]['change_type'] == \"not_perturbed\":\n",
    "        doc = spacy_model(changes[0]['changed_word'])\n",
    "        return [t.pos_ for t in doc][0]\n",
    "    elif changes[1]['change_type'] == \"not_perturbed\":\n",
    "        doc = spacy_model(changes[1]['changed_word'])\n",
    "        return [t.pos_ for t in doc][0]\n",
    "    return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b8355ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output_df(dataset, perturb_type, beam, replacement_strategy, analyse_feature=True, \n",
    "                   ignore_case=False, no_of_replacements=1, chunk_max_length=1, spacy_model=None, \n",
    "                   w2v_model=None, use_alignment=False, winoMT=False, ref_available=False,\n",
    "                   two_chunks_analysis=False):\n",
    "    if winoMT:\n",
    "        path_prefix = \"output/winoMT_asmetric/wmt19_winoMT_perturbed\"\n",
    "        output_df = pd.read_csv('output/winoMT_asmetric/wmt19_winoMT_perturbed_format.csv', index_col=0)  \n",
    "    else:\n",
    "        if no_of_replacements == 1:\n",
    "            path_prefix = f\"output/{dataset}/{replacement_strategy}/beam{beam}_perturb{perturb_type}/seed0/translations\"\n",
    "        else:\n",
    "            path_prefix = f\"output/{dataset}/{replacement_strategy}/beam{beam}_perturb{perturb_type}/seed0/translations_{no_of_replacements}replacements\"\n",
    "\n",
    "        output_df = pd.read_csv(f\"{path_prefix}.csv\", index_col=0)\n",
    "\n",
    "        # Join to get the translation of the original sentences as well\n",
    "        original_trans_path_prefix = \\\n",
    "            f\"output/{dataset}/{replacement_strategy}/beam{beam}_perturbNone/seed0/translations\"\n",
    "        output_df = output_df.join(pd.read_csv(\n",
    "            f\"{original_trans_path_prefix}.csv\", index_col=0\n",
    "        )['OriginalSRC-Trans'])\n",
    "        \n",
    "    if 'mustSHE' in dataset:\n",
    "        output_df = output_df.merge(pd.read_csv(\n",
    "            f\"data/MuST-SHE_v1.2/MuST-SHE-v1.2-data/tsv/MONOLINGUAL.fr_v1.2.tsv\",\n",
    "            sep='\\t')[['ID', 'CATEGORY']],\n",
    "            how='left', left_on='SRC_original_idx', right_on='ID'\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # Convert columns with sentences to str type\n",
    "    cols = ['SRC', 'REF', 'SRC_perturbed', 'SRC_perturbed-Trans', 'OriginalSRC-Trans']\n",
    "    if not ref_available:\n",
    "        cols.remove('REF')\n",
    "    output_df[cols] = output_df[cols].astype(str)\n",
    "    \n",
    "    if ignore_case:\n",
    "        output_df[cols] = output_df[cols].applymap(lambda x: x.lower())\n",
    "    \n",
    "    # Reorder the columns\n",
    "    if winoMT:\n",
    "        cols = ['SRC', 'REF', 'original_word', 'perturbed_word', 'SRC_perturbed', 'OriginalSRC-Trans', 'SRC_perturbed-Trans', 'Bias_sample']\n",
    "    elif no_of_replacements == 1:\n",
    "        cols = ['SRC', 'REF', 'original_word', 'perturbed_word', 'SRC_perturbed', 'OriginalSRC-Trans', 'SRC_perturbed-Trans', 'SRC_original_idx']\n",
    "    else:\n",
    "        cols = ['SRC_index', 'SRC', 'REF', 'original_word', 'perturbed_word', 'SRC_perturbed', 'OriginalSRC-Trans', 'SRC_perturbed-Trans', 'SRC_original_idx']\n",
    "    if not ref_available:\n",
    "        cols.remove('REF')\n",
    "    if 'mustSHE' in dataset:\n",
    "        cols.append('CATEGORY')\n",
    "    output_df = output_df[cols]\n",
    "    \n",
    "    if use_alignment:\n",
    "        if not winoMT:\n",
    "            original_alignment = load_alignment(original_trans_path_prefix)\n",
    "            output_df['original_trans_alignment'] = [alignment for alignment in original_alignment for _ in range(no_of_replacements)]\n",
    "        output_df['perturbed_trans_alignment'] = load_alignment(path_prefix)\n",
    "    \n",
    "    if analyse_feature:\n",
    "        print(f\"Original df shape: {output_df.shape}\")\n",
    "        output_df = output_df.dropna()\n",
    "        print(f\"After dropping none-perturbed sentences: {output_df.dropna().shape}\")\n",
    "        \n",
    "        \n",
    "        print('Calculating the changes between translations of original SRC and perturbed SRC ...')\n",
    "        # Calculate the changes, i.e., how to get from the original trans sentence \n",
    "        # to the changed trans sentence\n",
    "        output_df['tokenized_OriginalSRC-Trans'], output_df['tokenized_SRC_perturbed-Trans'], output_df['opcodes'], output_df['changes'] \\\n",
    "            = zip(*output_df.apply(\n",
    "                lambda x: calculate_change(x['OriginalSRC-Trans'], \n",
    "                                           x['SRC_perturbed-Trans']), axis=1\n",
    "            ))\n",
    "        \n",
    "        \n",
    "        print('Highlighting the changes ...')\n",
    "        # Highlight the changes in the trans sentences\n",
    "        output_df[\"OriginalSRC-Trans\"], output_df['SRC_perturbed-Trans'] \\\n",
    "            = zip(*output_df.apply(\n",
    "                lambda x: highlight_changes(\n",
    "                    x['tokenized_OriginalSRC-Trans'], \n",
    "                    x['tokenized_SRC_perturbed-Trans'], \n",
    "                    x['opcodes']), axis=1\n",
    "            ))\n",
    "        \n",
    "        \n",
    "        print('Calculating the edit distance ...')\n",
    "        if replacement_strategy == 'word2vec_similarity':\n",
    "            # SRC difference is the number of occurances of the word we perturb\n",
    "            output_df[\"SRC-edit_distance\"] = output_df.apply(lambda x: x['tokenized_OriginalSRC-Trans'].count(x['original_word']), axis=1)\n",
    "        else:\n",
    "            output_df[\"SRC-edit_distance\"] = 1\n",
    "        output_df['Trans-edit_distance'] =  output_df.apply(\n",
    "            lambda x: levenshtein(x['tokenized_OriginalSRC-Trans'], x['tokenized_SRC_perturbed-Trans']), axis=1)\n",
    "        output_df[\"#TransChanges-#SrcChanges\"] = output_df['Trans-edit_distance'] - output_df['SRC-edit_distance']\n",
    "        \n",
    "        output_df[\"#TransChanges-#SrcChanges/SentenceLength\"] = (output_df['Trans-edit_distance'] - output_df['SRC-edit_distance']) / output_df['SRC'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "        \n",
    "        output_df[\"ChangesSpread/SentenceLength\"] = output_df.apply(\n",
    "            lambda x: changes_spread(x['tokenized_OriginalSRC-Trans'], \n",
    "                                     x['tokenized_SRC_perturbed-Trans'], \n",
    "                                     x['opcodes']), axis=1)\n",
    "        \n",
    "        print(\"Two-chunks changed analysis\")\n",
    "        \n",
    "        if two_chunks_analysis:\n",
    "            # See if only two chunks within given max size are changed, \n",
    "            # and do some analysis on this special case\n",
    "            output_df['TwoChunksChanged'], output_df['ChunkDistance'], \\\n",
    "            output_df[\"is_same_subtree\"], output_df['changes_similarity'] \\\n",
    "                = zip(*output_df.apply(\n",
    "                    lambda x: two_chunk_changed(x['tokenized_OriginalSRC-Trans'],\n",
    "                                                x['tokenized_SRC_perturbed-Trans'],\n",
    "                                                x['opcodes'],\n",
    "                                                chunk_max_length=chunk_max_length,\n",
    "                                                spacy_model=spacy_model,\n",
    "                                                w2v_model=w2v_model), axis=1\n",
    "                ))\n",
    "\n",
    "        \n",
    "        print(\"Find out changes directly caused by perturbation using alignment\")\n",
    "        if use_alignment:\n",
    "            if two_chunks_analysis:\n",
    "                # In the case where two changes occurs and the two similarities is calculated, \n",
    "                # find out which change is due to the perturbation\n",
    "                output_df['changes_similarity'] = output_df.apply(\n",
    "                    lambda x: add_reason_of_change(\n",
    "                        alignment=x['perturbed_trans_alignment'],\n",
    "                        changes=x['changes_similarity'],\n",
    "                        perturbed_src_word=x['perturbed_word']\n",
    "                    ),\n",
    "                    axis=1\n",
    "                )\n",
    "            \n",
    "                if spacy_model is not None:\n",
    "                    # Add POS tagging of the not-perturbed change\n",
    "                    output_df['not_perturbed_TGT_change_type'] = output_df['changes_similarity'].apply(\n",
    "                        lambda x: pos_tag_not_perturbed_change(x, spacy_model))\n",
    "            \n",
    "            \n",
    "        print(\"Stats on some group changes\")\n",
    "        # Analyse on group of changes on the same sentence\n",
    "        if no_of_replacements > 1:\n",
    "            additional_col_1 = output_df.groupby(by=\"SRC_index\", axis=0)[['Trans-edit_distance', '#TransChanges-#SrcChanges']].std()\n",
    "            output_df = output_df.join(additional_col_1, rsuffix='--SD')\n",
    "            \n",
    "            if two_chunks_analysis:\n",
    "                additional_col_2 = output_df.groupby(by=\"SRC_index\", axis=0)[['TwoChunksChanged']].sum()\n",
    "                output_df = output_df.join(additional_col_2, rsuffix='--total')\n",
    "        \n",
    "    return output_df\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b86d1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df shape: (367200, 11)\n",
      "After dropping none-perturbed sentences: (367200, 11)\n",
      "Calculating the changes between translations of original SRC and perturbed SRC ...\n",
      "Highlighting the changes ...\n",
      "Calculating the edit distance ...\n",
      "Two-chunks changed analysis\n",
      "Find out changes directly caused by perturbation using alignment\n",
      "Stats on some group changes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRC_index</th>\n",
       "      <th>SRC</th>\n",
       "      <th>original_word</th>\n",
       "      <th>perturbed_word</th>\n",
       "      <th>SRC_perturbed</th>\n",
       "      <th>OriginalSRC-Trans</th>\n",
       "      <th>SRC_perturbed-Trans</th>\n",
       "      <th>SRC_original_idx</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>original_trans_alignment</th>\n",
       "      <th>perturbed_trans_alignment</th>\n",
       "      <th>tokenized_OriginalSRC-Trans</th>\n",
       "      <th>tokenized_SRC_perturbed-Trans</th>\n",
       "      <th>opcodes</th>\n",
       "      <th>changes</th>\n",
       "      <th>SRC-edit_distance</th>\n",
       "      <th>Trans-edit_distance</th>\n",
       "      <th>#TransChanges-#SrcChanges</th>\n",
       "      <th>#TransChanges-#SrcChanges/SentenceLength</th>\n",
       "      <th>ChangesSpread/SentenceLength</th>\n",
       "      <th>Trans-edit_distance--SD</th>\n",
       "      <th>#TransChanges-#SrcChanges--SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>268</td>\n",
       "      <td>So as a victims' rights attorney fighting to increase the prospect of justice for survivors across the country and as a survivor myself, I'm not here to say, \"Time's Up.\" I'm here to say, \"It's time.\" It's time for accountability to become the norm after gender violence.</td>\n",
       "      <td>accountability</td>\n",
       "      <td>violence</td>\n",
       "      <td>So as a victims'rights attorney fighting to increase the prospect of justice for survivors across the country and as a survivor myself, I'm not here to say, \" Time's Up. \" I'm here to say, \" It's time. \" It's time for violence to become the norm after gender violence.</td>\n",
       "      <td>Als OPFERANWALT , DER DARUM kämpft , die AUSSICHT auf Gerechtigkeit für Überlebende im ganzen Land zu verbessern , und als ÜBERLEBENDER selbst bin ich nicht hier , um zu sagen : `` Time 's Up '' . Ich bin hier , um zu sagen : `` IT ' S TIME '' . Es ist Zeit , dass RECHENSCHAFTSPFLICHT nach geschlechtsspezifischer Gewalt zur Norm wird .</td>\n",
       "      <td>Als OPFERANWÄLTIN , DIE DAFÜR kämpft , die AUSSICHTEN auf Gerechtigkeit für Überlebende im ganzen Land zu verbessern , und als ÜBERLEBENDE selbst bin ich nicht hier , um zu sagen : `` Time 's Up '' . Ich bin hier , um zu sagen : `` ES IST ZEIT '' . Es ist Zeit , dass GEWALT nach geschlechtsspezifischer Gewalt zur Norm wird .</td>\n",
       "      <td>fr-0436</td>\n",
       "      <td>4F</td>\n",
       "      <td>{'as': 'als', 'victims': 'Opferanwalt', 'attorney': 'Opferanwalt', 'fighting': 'kämpft', 'to': 'dass', 'increase': 'verbessern', 'the': 'zur', 'prospect': 'Aussicht', 'of': 'auf', 'justice': 'Gerechtigkeit', 'for': 'für', 'survivors': 'Überlebende', 'across': 'im', 'country': 'Land', 'and': 'und', 'survivor': 'Überlebender', 'myself': 'selbst', ',': ':', 'I': 'Ich', ''m': 'bin', 'not': 'nicht', 'here': 'hier', 'say': 'sagen', '``': '``', 'Time': 'Time', ''s': 'ist', 'Up': 'Up', '.': '.', '''': '''', 'It': 'Es', 'time': 'Zeit', 'accountability': 'Rechenschaftspflicht', 'become': 'wird', 'norm': 'Norm', 'after': 'nach', 'violence': 'Gewalt'}</td>\n",
       "      <td>{'as': 'als', 'victims'rights': 'Opferanwältin', 'attorney': 'Opferanwältin', 'fighting': 'kämpft', 'to': 'zu', 'increase': 'verbessern', 'the': 'zur', 'prospect': 'Aussichten', 'of': 'auf', 'justice': 'Gerechtigkeit', 'for': 'dass', 'survivors': 'Überlebende', 'across': 'im', 'country': 'Land', 'and': 'und', 'survivor': 'Überlebende', 'myself': 'selbst', 'I': 'Ich', ''m': 'bin', 'not': 'nicht', 'here': 'hier', 'say': 'sagen', ',': ':', '``': '``', 'Time': 'Time', ''s': 'ist', 'Up.': '.', 'It': 'Es', 'time.': '.', 'time': 'Zeit', 'violence': 'Gewalt', 'norm': 'Norm', 'after': 'nach', '.': '.'}</td>\n",
       "      <td>[Als, Opferanwalt, ,, der, darum, kämpft, ,, die, Aussicht, auf, Gerechtigkeit, für, Überlebende, im, ganzen, Land, zu, verbessern, ,, und, als, Überlebender, selbst, bin, ich, nicht, hier, ,, um, zu, sagen, :, ``, Time, 's, Up, '', ., Ich, bin, hier, ,, um, zu, sagen, :, ``, It, ', s, time, '', ., Es, ist, Zeit, ,, dass, Rechenschaftspflicht, nach, geschlechtsspezifischer, Gewalt, zur, Norm, wird, .]</td>\n",
       "      <td>[Als, Opferanwältin, ,, die, dafür, kämpft, ,, die, Aussichten, auf, Gerechtigkeit, für, Überlebende, im, ganzen, Land, zu, verbessern, ,, und, als, Überlebende, selbst, bin, ich, nicht, hier, ,, um, zu, sagen, :, ``, Time, 's, Up, '', ., Ich, bin, hier, ,, um, zu, sagen, :, ``, Es, ist, Zeit, '', ., Es, ist, Zeit, ,, dass, Gewalt, nach, geschlechtsspezifischer, Gewalt, zur, Norm, wird, .]</td>\n",
       "      <td>[(equal, 0, 1, 0, 1), (replace, 1, 2, 1, 2), (equal, 2, 3, 2, 3), (replace, 3, 5, 3, 5), (equal, 5, 8, 5, 8), (replace, 8, 9, 8, 9), (equal, 9, 21, 9, 21), (replace, 21, 22, 21, 22), (equal, 22, 47, 22, 47), (replace, 47, 51, 47, 50), (equal, 51, 58, 50, 57), (replace, 58, 59, 57, 58), (equal, 59, 66, 58, 65)]</td>\n",
       "      <td>[(replace, Opferanwalt, Opferanwältin), (replace, der darum, die dafür), (replace, Aussicht, Aussichten), (replace, Überlebender, Überlebende), (replace, It ' s time, Es ist Zeit), (replace, Rechenschaftspflicht, Gewalt)]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>1.224276</td>\n",
       "      <td>1.224276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SRC_index  \\\n",
       "1650        268   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                  SRC  \\\n",
       "1650  So as a victims' rights attorney fighting to increase the prospect of justice for survivors across the country and as a survivor myself, I'm not here to say, \"Time's Up.\" I'm here to say, \"It's time.\" It's time for accountability to become the norm after gender violence.   \n",
       "\n",
       "       original_word perturbed_word  \\\n",
       "1650  accountability       violence   \n",
       "\n",
       "                                                                                                                                                                                                                                                                     SRC_perturbed  \\\n",
       "1650  So as a victims'rights attorney fighting to increase the prospect of justice for survivors across the country and as a survivor myself, I'm not here to say, \" Time's Up. \" I'm here to say, \" It's time. \" It's time for violence to become the norm after gender violence.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                      OriginalSRC-Trans  \\\n",
       "1650  Als OPFERANWALT , DER DARUM kämpft , die AUSSICHT auf Gerechtigkeit für Überlebende im ganzen Land zu verbessern , und als ÜBERLEBENDER selbst bin ich nicht hier , um zu sagen : `` Time 's Up '' . Ich bin hier , um zu sagen : `` IT ' S TIME '' . Es ist Zeit , dass RECHENSCHAFTSPFLICHT nach geschlechtsspezifischer Gewalt zur Norm wird .   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                         SRC_perturbed-Trans  \\\n",
       "1650  Als OPFERANWÄLTIN , DIE DAFÜR kämpft , die AUSSICHTEN auf Gerechtigkeit für Überlebende im ganzen Land zu verbessern , und als ÜBERLEBENDE selbst bin ich nicht hier , um zu sagen : `` Time 's Up '' . Ich bin hier , um zu sagen : `` ES IST ZEIT '' . Es ist Zeit , dass GEWALT nach geschlechtsspezifischer Gewalt zur Norm wird .   \n",
       "\n",
       "     SRC_original_idx CATEGORY  \\\n",
       "1650          fr-0436       4F   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     original_trans_alignment  \\\n",
       "1650  {'as': 'als', 'victims': 'Opferanwalt', 'attorney': 'Opferanwalt', 'fighting': 'kämpft', 'to': 'dass', 'increase': 'verbessern', 'the': 'zur', 'prospect': 'Aussicht', 'of': 'auf', 'justice': 'Gerechtigkeit', 'for': 'für', 'survivors': 'Überlebende', 'across': 'im', 'country': 'Land', 'and': 'und', 'survivor': 'Überlebender', 'myself': 'selbst', ',': ':', 'I': 'Ich', ''m': 'bin', 'not': 'nicht', 'here': 'hier', 'say': 'sagen', '``': '``', 'Time': 'Time', ''s': 'ist', 'Up': 'Up', '.': '.', '''': '''', 'It': 'Es', 'time': 'Zeit', 'accountability': 'Rechenschaftspflicht', 'become': 'wird', 'norm': 'Norm', 'after': 'nach', 'violence': 'Gewalt'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     perturbed_trans_alignment  \\\n",
       "1650  {'as': 'als', 'victims'rights': 'Opferanwältin', 'attorney': 'Opferanwältin', 'fighting': 'kämpft', 'to': 'zu', 'increase': 'verbessern', 'the': 'zur', 'prospect': 'Aussichten', 'of': 'auf', 'justice': 'Gerechtigkeit', 'for': 'dass', 'survivors': 'Überlebende', 'across': 'im', 'country': 'Land', 'and': 'und', 'survivor': 'Überlebende', 'myself': 'selbst', 'I': 'Ich', ''m': 'bin', 'not': 'nicht', 'here': 'hier', 'say': 'sagen', ',': ':', '``': '``', 'Time': 'Time', ''s': 'ist', 'Up.': '.', 'It': 'Es', 'time.': '.', 'time': 'Zeit', 'violence': 'Gewalt', 'norm': 'Norm', 'after': 'nach', '.': '.'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                               tokenized_OriginalSRC-Trans  \\\n",
       "1650  [Als, Opferanwalt, ,, der, darum, kämpft, ,, die, Aussicht, auf, Gerechtigkeit, für, Überlebende, im, ganzen, Land, zu, verbessern, ,, und, als, Überlebender, selbst, bin, ich, nicht, hier, ,, um, zu, sagen, :, ``, Time, 's, Up, '', ., Ich, bin, hier, ,, um, zu, sagen, :, ``, It, ', s, time, '', ., Es, ist, Zeit, ,, dass, Rechenschaftspflicht, nach, geschlechtsspezifischer, Gewalt, zur, Norm, wird, .]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                 tokenized_SRC_perturbed-Trans  \\\n",
       "1650  [Als, Opferanwältin, ,, die, dafür, kämpft, ,, die, Aussichten, auf, Gerechtigkeit, für, Überlebende, im, ganzen, Land, zu, verbessern, ,, und, als, Überlebende, selbst, bin, ich, nicht, hier, ,, um, zu, sagen, :, ``, Time, 's, Up, '', ., Ich, bin, hier, ,, um, zu, sagen, :, ``, Es, ist, Zeit, '', ., Es, ist, Zeit, ,, dass, Gewalt, nach, geschlechtsspezifischer, Gewalt, zur, Norm, wird, .]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                      opcodes  \\\n",
       "1650  [(equal, 0, 1, 0, 1), (replace, 1, 2, 1, 2), (equal, 2, 3, 2, 3), (replace, 3, 5, 3, 5), (equal, 5, 8, 5, 8), (replace, 8, 9, 8, 9), (equal, 9, 21, 9, 21), (replace, 21, 22, 21, 22), (equal, 22, 47, 22, 47), (replace, 47, 51, 47, 50), (equal, 51, 58, 50, 57), (replace, 58, 59, 57, 58), (equal, 59, 66, 58, 65)]   \n",
       "\n",
       "                                                                                                                                                                                                                            changes  \\\n",
       "1650  [(replace, Opferanwalt, Opferanwältin), (replace, der darum, die dafür), (replace, Aussicht, Aussichten), (replace, Überlebender, Überlebende), (replace, It ' s time, Es ist Zeit), (replace, Rechenschaftspflicht, Gewalt)]   \n",
       "\n",
       "      SRC-edit_distance  Trans-edit_distance  #TransChanges-#SrcChanges  \\\n",
       "1650                  1                   10                          9   \n",
       "\n",
       "      #TransChanges-#SrcChanges/SentenceLength  ChangesSpread/SentenceLength  \\\n",
       "1650                                  0.142857                      0.892308   \n",
       "\n",
       "      Trans-edit_distance--SD  #TransChanges-#SrcChanges--SD  \n",
       "1650                 1.224276                       1.224276  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturb_type = 'content'\n",
    "dataset = f'masked_{perturb_type}_mustSHE'  # 'MuST-SHE-en2fr' 'IWSLT15-en2vi' 'wmt19-newstest2019-en2de'\n",
    "beam = 5\n",
    "replacement_strategy = 'masking_language_model'\n",
    "no_of_replacements = 30\n",
    "ignore_case = False  # Only Europarls needs ignore case\n",
    "chunk_max_length=1\n",
    "spacy_model = spacy.load(\"de_core_news_sm\")\n",
    "# Loading these models in is time consuming\n",
    "de_model = load_facebook_model(\"data/cc.de.300.bin\").wv\n",
    "# vi_model = load_facebook_model(\"data/cc.vi.300.bin\").wv\n",
    "winoMT = False\n",
    "\n",
    "# # This overwrite the above params\n",
    "# winoMT = True\n",
    "# perturb_type = 'pronoun'\n",
    "# no_of_replacements = 1\n",
    "\n",
    "output = read_output_df(dataset=dataset, perturb_type=perturb_type, beam=beam, \n",
    "                        replacement_strategy=replacement_strategy, ignore_case=ignore_case,\n",
    "                        no_of_replacements=no_of_replacements, chunk_max_length=chunk_max_length,\n",
    "                        spacy_model=spacy_model, w2v_model=de_model, use_alignment=True, \n",
    "                        winoMT=winoMT, analyse_feature=True, two_chunks_analysis=False)\n",
    "\n",
    "output[output['CATEGORY']=='4F'].head(1)\n",
    "\n",
    "# print('BLEU score: ')\n",
    "# sacrebleu.corpus_bleu(output['OriginalSRC-Trans'].tolist(), [output['REF'].tolist()]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd93d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0048b3d1",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "- On `wmt19-newstest2019-en2de, chunk_max_length=2`\n",
    "    - 902: change to 1 SRC word leads to fixed changes of an irrelevant word\n",
    "    - In many cases, the form of the verb (e.g., current or past tense) are changed --> harmful in the sense that it hurt performance score?\n",
    "    - Word not being translated \n",
    "    - Spoken/written style\n",
    "    - Time\n",
    "    \n",
    "    \n",
    "- On `IWSLT15-en2vi, adjective`\n",
    "    - 1003: change of 1 words consistently leads to change in subject\n",
    "    \n",
    "    - 1003, 145, 990 noun: same\n",
    "    - 236 noun: same, funny but not sure if it is wrong\n",
    "    - 308 verb same \n",
    "    \n",
    "--> Quantify the verb form change by stemming/lemmatization\n",
    "    \n",
    "Chúng, họ, gã, cô ấy, cô ta, anh ta, hắn\n",
    "\n",
    "Changes in the word \"you\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442d9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output[output['#TransChanges-#SrcChanges'] > 10].head(5)\n",
    "# output[output[\"ChangesSpread/SentenceLength\"] > 0.85].head(20)\n",
    "\n",
    "\n",
    "\n",
    "# Two chunks changed that consistently changed over the different replacement of a word\n",
    "\n",
    "\n",
    "# output[(output[\"TwoChunksChanged\"] == True) & (output[\"TwoChunksChanged--total\"] == 5)].sort_values(by='ChunkDistance', axis=0, ascending=False).head(1)\n",
    "# output[(output[\"TwoChunksChanged\"] == True)].sort_values(by='ChunkDistance', axis=0, ascending=False).head(100)\n",
    "\n",
    "# Two words changed that are not in the same subtree\n",
    "# output[(output[\"TwoChunksChanged\"] == True) & (output[\"is_same_subtree\"] == False) & (output[\"TwoChunksChanged--total\"] == 5)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IWSLT15-en2vi, noun\n",
    "# output.loc[[1003, 145, 990, 236]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976bde7",
   "metadata": {},
   "source": [
    "Sort the samples by the least similarity in changed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the 2-word-changed cases and similarity can be calculated\n",
    "def get_not_perturbed_change_similarity(changes):\n",
    "    for change in changes:\n",
    "        if change['change_type'] == 'not_perturbed':\n",
    "            return change['semantic_similarity']\n",
    "    return pd.NA\n",
    "\n",
    "analyse_df = output[\n",
    "    (output[\"TwoChunksChanged\"] == True) & output['changes_similarity'].notna() & output['not_perturbed_TGT_change_type'].isin(['NOUN', 'VERB', 'ADJ', 'PRON'])\n",
    "]\n",
    "analyse_df['similarity_not_perturbed'] = analyse_df['changes_similarity'].apply(\n",
    "    lambda x: get_not_perturbed_change_similarity(x)\n",
    ")\n",
    "analyse_df.sort_values(by='similarity_not_perturbed')[['SRC', \n",
    "                                                f'original_word', \n",
    "                                                f'perturbed_word',\n",
    "                                                'OriginalSRC-Trans',\n",
    "                                                f'SRC_perturbed-Trans',\n",
    "                                                'ChunkDistance',\n",
    "                                                'changes_similarity',\n",
    "                                                'similarity_not_perturbed',\n",
    "                                                'not_perturbed_TGT_change_type',\n",
    "#                                                 'Bias_sample'\n",
    "                                                      ]].head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c53bf",
   "metadata": {},
   "source": [
    "### Calculate metrics for detecting the bias samples\n",
    "\n",
    "High precision --> higher chance that the returned samples are bias --> save human time\n",
    "\n",
    "High recall --> more bias samples are retreat --> can detect more type of bias\n",
    "\n",
    "We focus on precision then (save human cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(' -------------------- Most-changes filter -------------------- ')\n",
    "q = 20  # Take the q% sentences with the highest changes\n",
    "no_changes_thresthold = np.percentile(output['#TransChanges-#SrcChanges'], 100-q)\n",
    "bias_prediction = output['#TransChanges-#SrcChanges'] > no_changes_thresthold\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "print(' -------------------- Most-spreaded_changes filter -------------------- ')\n",
    "q = 20  # Take the q% sentences with the highest spread\n",
    "spread_thresthold = np.percentile(output['ChangesSpread/SentenceLength'], 100-q)\n",
    "bias_prediction = output['ChangesSpread/SentenceLength'] > spread_thresthold\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "print(' -------------------- Two-changes filter -------------------- ')\n",
    "bias_prediction = output[\"TwoChunksChanged\"]\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "\n",
    "print(' -------------------- Two-faraway-changes filter -------------------- ')\n",
    "q = 20  # Take the q% sentences with the furthest distance between 2 changes \n",
    "distance_thresthold = np.nanpercentile(output['ChunkDistance'], 100-q)\n",
    "bias_prediction = output[\"TwoChunksChanged\"] & (output['ChunkDistance'] > distance_thresthold)\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "print(' -------------------- Two-changes-different-subtree filter -------------------- ')\n",
    "bias_prediction = output[\"TwoChunksChanged\"] & (output[\"is_same_subtree\"] == False)\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "\n",
    "print(' -------------------- Two-change-dissimilar filter -------------------- ')\n",
    "q = 90  # Take the q% sentences with the lowest similarity of the not-perturbed change\n",
    "output = output.join(analyse_df['similarity_not_perturbed'])\n",
    "similiarity_threshold = np.nanpercentile(output['similarity_not_perturbed'], q)\n",
    "\n",
    "bias_prediction = output[\"TwoChunksChanged\"] & (output['similarity_not_perturbed'] < similiarity_threshold)\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ab5c4",
   "metadata": {},
   "source": [
    "# Analyse on same original_word accross sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[[\n",
    "    'SRC_index', 'SRC', 'original_word', 'perturbed_word', 'SRC_perturbed',\n",
    "    'OriginalSRC-Trans', 'SRC_perturbed-Trans', '#TransChanges-#SrcChanges',\n",
    "    '#TransChanges-#SrcChanges/SentenceLength',\n",
    "    'ChangesSpread/SentenceLength', 'TwoChunksChanged', 'ChunkDistance',\n",
    "    'is_same_subtree', 'changes_similarity', 'perturbed_trans_alignment',\n",
    "    'not_perturbed_TGT_change_type', 'Trans-edit_distance--SD',\n",
    "    '#TransChanges-#SrcChanges--SD', 'TwoChunksChanged--total'\n",
    "]].groupby('original_word').mean().head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb9c49",
   "metadata": {},
   "source": [
    "### Most changes filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupped_by_word = output.groupby('original_word').mean()\n",
    "\n",
    "q = 10  # Take the q% groups with the highest changes\n",
    "no_changes_thresthold = np.percentile(groupped_by_word['#TransChanges-#SrcChanges'], 100-q)\n",
    "bias_prediction = groupped_by_word['#TransChanges-#SrcChanges'] > no_changes_thresthold\n",
    "\n",
    "bias_word_predicted = groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['#TransChanges-#SrcChanges'] > no_changes_thresthold)\n",
    "].head(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf0826",
   "metadata": {},
   "source": [
    "### Most-spreaded_changes filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e45372",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupped_by_word = output.groupby('original_word').mean()\n",
    "\n",
    "q = 10  # Take the q% sentences with the highest spread\n",
    "spread_thresthold = np.percentile(groupped_by_word['ChangesSpread/SentenceLength'], 100-q)\n",
    "bias_prediction = groupped_by_word['ChangesSpread/SentenceLength'] > spread_thresthold\n",
    "\n",
    "bias_word_predicted = groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['ChangesSpread/SentenceLength'] > spread_thresthold)\n",
    "].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069aeb58",
   "metadata": {},
   "source": [
    "### Two-faraway-changes filter\n",
    "\n",
    "ACTUALLY two-changes is not a bias filter. It's just an auxilary filter to avoid paraphrasing cases. Using this we will miss out on the cases where the model has both paraphrasing and \n",
    "\n",
    "Here we consider in each group: the number of sentences that has 2 changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_change_only_groupped_by_word = output[output[\"TwoChunksChanged\"]].groupby('original_word').mean()\n",
    "\n",
    "\n",
    "q = 20  # Take the q% sentences with the furthest distance between 2 changes \n",
    "distance_thresthold = np.percentile(two_change_only_groupped_by_word['ChunkDistance'], 100-q)\n",
    "bias_prediction = two_change_only_groupped_by_word['ChunkDistance'] > distance_thresthold\n",
    "\n",
    "\n",
    "bias_word_predicted = two_change_only_groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output[\"TwoChunksChanged\"] & \\\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['ChunkDistance'] > distance_thresthold)\n",
    "].head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cd5f9",
   "metadata": {},
   "source": [
    "### Two-changes-different-subtree filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876de271",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = output[output[\"TwoChunksChanged\"] & output['is_same_subtree'].notna()]\n",
    "tmp['not_same_subtree'] = 1 - tmp['is_same_subtree'].astype(int)\n",
    "two_change_only_groupped_by_word = tmp.groupby('original_word').sum()\n",
    "\n",
    "q = 20  # Take the q% groups with the highest number of different subtree changes\n",
    "count_thresthold = np.percentile(two_change_only_groupped_by_word['not_same_subtree'], 100-q)\n",
    "bias_prediction = two_change_only_groupped_by_word['ChunkDistance'] > count_thresthold\n",
    "\n",
    "\n",
    "bias_word_predicted = two_change_only_groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output[\"TwoChunksChanged\"] & \\\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['is_same_subtree'] == 0)\n",
    "].head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a02578",
   "metadata": {},
   "source": [
    "### Two-change-dissimilar filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ee3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.join(analyse_df['similarity_not_perturbed'])\n",
    "two_change_only_groupped_by_word = output[output[\"TwoChunksChanged\"]].groupby('original_word').mean()\n",
    "\n",
    "\n",
    "q = 20  # Take the q% sentences with the lowest similarity of the not-perturbed change\n",
    "similiarity_threshold = np.nanpercentile(two_change_only_groupped_by_word['similarity_not_perturbed'], q)\n",
    "bias_prediction = two_change_only_groupped_by_word['similarity_not_perturbed'] < similiarity_threshold\n",
    "\n",
    "\n",
    "bias_word_predicted = two_change_only_groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output[\"TwoChunksChanged\"] & \\\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['similarity_not_perturbed'] < similiarity_threshold)\n",
    "].head(2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52f9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11449a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6de148",
   "metadata": {},
   "source": [
    "## Find patterns\n",
    "\n",
    "when a word A is replaced with B, then the change C happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4534859",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[['SRC_index', 'SRC', 'original_word', 'perturbed_word', 'SRC_perturbed',\n",
    "       'OriginalSRC-Trans', 'SRC_perturbed-Trans', 'changes']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260159f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def lower_remove_non_alphabet(input_str):\n",
    "    translation = input_str.maketrans(dict.fromkeys(string.punctuation, ' '))\n",
    "    return input_str.translate(translation).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ad595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_due_to_perturbation(change, original_word, perturbed_word, \n",
    "                           perturbed_trans_alignment_dict, original_trans_alignment_dict):\n",
    "    \"\"\"\n",
    "    A change in translation is directly due to perturbation if the (aligned) translation of perturbed_word\n",
    "    is in changed_part AND the (aligned) translation of original_word is in original_part\n",
    "    \n",
    "    Params:\n",
    "        change: tuple of (change_type, original_trans_part, changed_trans_part)\n",
    "        original_word: original word in the SRC that was perturbed\n",
    "        perturbed_word: the replacement of the original word\n",
    "        perturbed_trans_alignment_dict: {src_word1:trans_word1, src_word2:trans_word2,...} of the perturbed trans\n",
    "        original_trans_alignment_dict: {src_word1:trans_word1, src_word2:trans_word2,...} of the original trans\n",
    "    Return: bool, pd.NA in failed alignment case\n",
    "    \"\"\"\n",
    "    # Turn everything to lowercase, and remove any non-alphabet characters\n",
    "    change_type, original_trans_part, changed_trans_part = \\\n",
    "        change[0], lower_remove_non_alphabet(change[1]), lower_remove_non_alphabet(change[2])\n",
    "    perturbed_trans_alignment_dict = dict(\n",
    "        (lower_remove_non_alphabet(k).replace(' ', ''), lower_remove_non_alphabet(v).replace(' ', '')) for k,v in perturbed_trans_alignment_dict.items()\n",
    "    )\n",
    "    original_trans_alignment_dict = dict(\n",
    "        (lower_remove_non_alphabet(k).replace(' ', ''), lower_remove_non_alphabet(v).replace(' ', '')) for k,v in original_trans_alignment_dict.items()\n",
    "    )\n",
    "    original_word = lower_remove_non_alphabet(original_word)\n",
    "    perturbed_word = lower_remove_non_alphabet(perturbed_word)\n",
    "    \n",
    "\n",
    "    perturbed_word_appears_in_new_trans = pd.NA\n",
    "    if perturbed_word in perturbed_trans_alignment_dict.keys():\n",
    "        perturbed_word_trans = perturbed_trans_alignment_dict[perturbed_word]\n",
    "        if perturbed_word_trans in changed_trans_part.split():\n",
    "            perturbed_word_appears_in_new_trans = True\n",
    "        else:\n",
    "            perturbed_word_appears_in_new_trans = False\n",
    "            \n",
    "    # Missed-translation, or name-specific case\n",
    "    if perturbed_word in changed_trans_part.split():\n",
    "        perturbed_word_appears_in_new_trans = True\n",
    "            \n",
    "\n",
    "    original_word_appears_in_old_trans = pd.NA\n",
    "    if original_word in original_trans_alignment_dict.keys():\n",
    "        original_word_trans = original_trans_alignment_dict[original_word]\n",
    "        if original_word_trans in original_trans_part.split():\n",
    "            original_word_appears_in_old_trans = True\n",
    "        else:\n",
    "            original_word_appears_in_old_trans = False\n",
    "        \n",
    "        if perturbed_word in perturbed_trans_alignment_dict.keys():\n",
    "            if original_word == 'fort' and perturbed_word == 'île' and change == ('replace', 'Fort-de-France', 'Île-de-France'):\n",
    "                print('-------------------------')\n",
    "                print(change)\n",
    "                print('-' + original_word_trans + '-')\n",
    "                print('-' + perturbed_word_trans + '-')\n",
    "                print(original_word_appears_in_old_trans)\n",
    "                print(perturbed_word_appears_in_new_trans)\n",
    "                \n",
    "    # Missed-translation, or name-specific case\n",
    "    if original_word in original_trans_part.split():\n",
    "        original_word_appears_in_old_trans = True\n",
    "            \n",
    "    # If perturbed_word_appears_in_new_trans or original_word_appears_in_old_trans is true, then \n",
    "    # is_due_to_perturbation is true\n",
    "    if (not pd.isnull(perturbed_word_appears_in_new_trans)) and \\\n",
    "        (not pd.isnull(original_word_appears_in_old_trans)):\n",
    "        return (perturbed_word_appears_in_new_trans or original_word_appears_in_old_trans)\n",
    "    elif (pd.isnull(perturbed_word_appears_in_new_trans)) and \\\n",
    "        (not pd.isnull(original_word_appears_in_old_trans)):\n",
    "        if original_word_appears_in_old_trans:\n",
    "            return True\n",
    "        else:\n",
    "            return pd.NA\n",
    "    elif (not pd.isnull(perturbed_word_appears_in_new_trans)) and \\\n",
    "        (pd.isnull(original_word_appears_in_old_trans)):\n",
    "        if perturbed_word_appears_in_new_trans:\n",
    "            return True\n",
    "        else:\n",
    "            return pd.NA\n",
    "    else:\n",
    "        return pd.NA\n",
    "    \n",
    "    \n",
    "def filter_changes(group_df):\n",
    "    changes = []\n",
    "    \n",
    "    for index, row in group_df.iterrows():\n",
    "        for change in row['changes']:\n",
    "            # Filter out the changes caused by perturbation\n",
    "            is_due_to_perturbation_out = is_due_to_perturbation(\n",
    "                                            change, \n",
    "                                            row['original_word'], \n",
    "                                            row['perturbed_word'], \n",
    "                                            row['perturbed_trans_alignment'],\n",
    "                                            row['original_trans_alignment']\n",
    "                                        )\n",
    "            if pd.isnull(is_due_to_perturbation_out) or is_due_to_perturbation_out:\n",
    "                continue\n",
    "                \n",
    "            # Filter out the weird <unk>\n",
    "            if change == ('delete', '< unk >', '') or change == ('insert', '', '< unk >'):\n",
    "                continue\n",
    "                \n",
    "            # Filter out the changes that are not content-related\n",
    "            all_pos_tags = [t.pos_ for t in spacy_model(change[1])] + [t.pos_ for t in spacy_model(change[2])]\n",
    "            content_related_tags = 'NOUN', 'VERB', 'ADJ', 'PRON'\n",
    "            overlap = not set(all_pos_tags).isdisjoint(content_related_tags)\n",
    "            if not overlap:\n",
    "                continue\n",
    "                \n",
    "            changes.append(change)\n",
    "            \n",
    "            \n",
    "    return changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def find_max_freq_change(group_df):\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        group_df: the group of results that has the same original_word and perturbed_word\n",
    "    \"\"\"\n",
    "    assert group_df['original_word'].value_counts().shape[0] == 1  # Because this function is for a single group\n",
    "    assert group_df['perturbed_word'].value_counts().shape[0] == 1  # Because this function is for a single group\n",
    "    \n",
    "    # Filter out the changes that are not directly due to perturbation\n",
    "    all_changes = filter_changes(group_df)\n",
    "    \n",
    "    freq_changes = Counter(all_changes)\n",
    "    \n",
    "    if len(freq_changes.most_common()) == 0:\n",
    "        return 0\n",
    "    return freq_changes.most_common(1)[0][1]\n",
    "\n",
    "change_freq = output.groupby(\n",
    "    ['original_word', 'perturbed_word'], as_index=False\n",
    ").apply(find_max_freq_change).rename(columns={None: 'max_change_freq'}).sort_values(\n",
    "    by='max_change_freq', ascending=False)\n",
    "    \n",
    "\n",
    "change_freq = change_freq[change_freq['perturbed_word'].apply(lambda x: x.isalpha())]\n",
    "\n",
    "change_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cca6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = output.groupby(['original_word', 'perturbed_word'])\n",
    "groups_as_list = [(original_perturb, group) for original_perturb, group in groups]\n",
    "re_ordered_groupes = [groups_as_list[i] for i in change_freq.index.values]\n",
    "\n",
    "for original_perturb, group in re_ordered_groupes:\n",
    "    print(\"----------------------\")\n",
    "    print(f\"original SRC word: {original_perturb[0]}\")\n",
    "    print(f\"perturbed SRC word: {original_perturb[1]}\")\n",
    "    all_changes = filter_changes(group)\n",
    "    freq_changes = Counter(all_changes)\n",
    "    print(freq_changes.most_common(2))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c1316b0",
   "metadata": {},
   "source": [
    "original SRC word: excuse\n",
    "perturbed SRC word: Trust\n",
    "[(('insert', '', 'mir'), 11), (('insert', '', 'uns'), 4)]\n",
    "\n",
    "original SRC word: communist\n",
    "perturbed SRC word: Nationalist\n",
    "[(('delete', Chinas, ''), 11),\n",
    "\n",
    "\n",
    "\n",
    "original SRC word: usa\n",
    "perturbed SRC word: is\n",
    "[(('insert', '', 'kalifornischen'), 10),\n",
    "\n",
    "\n",
    "original SRC word: please\n",
    "perturbed SRC word: you\n",
    "[(('delete', 'Sie', ''), 7), \n",
    "\n",
    "\n",
    "original SRC word: restaurant\n",
    "perturbed SRC word: bar\n",
    "[(('replace', 'es', 'sie'), 5), (('replace', 'einem', 'einer'), 2)]\n",
    "\n",
    "original SRC word: hurry\n",
    "perturbed SRC word: shut\n",
    "[(('replace', 'sich', 'den Mund'), 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363d5bb",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "--> starts to make sense, yet have not seen bias (even gender bias)\n",
    "\n",
    "--> A bigger dataset for inference could help?\n",
    "\n",
    "Some correlation is good, some correlation is bad. Is it a good idea to prevent these correlation??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42568741",
   "metadata": {},
   "source": [
    "# Filter per sentence with different replacements\n",
    "\n",
    "\n",
    "**Note**: can use [sequence alignments](https://stackoverflow.com/questions/5055839/word-level-edit-distance-of-a-sentence) to align the sentences on the target side only. ([code](https://gist.github.com/slowkow/06c6dba9180d013dfd82bec217d22eb5))\n",
    "\n",
    "Pros: could be easier than SRC-TGT alignment\n",
    "\n",
    "Cons: in the case where more output different sentence structure yet same meaning. <br>\n",
    "E.g., \"Today I think the cat is nice\" -- \"I think the cat is nice today\"\n",
    "SRC-TGT alignment would probably see these as the same, but edit distance cannot, bc it only has del, insert, substitute operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8f50bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cast_to_index(string_index):\n",
    "    \"\"\"\n",
    "    In a aligned tuple, the items could either be the index of a word, or the character '-' denoting \n",
    "    \"\"\"\n",
    "    # removes blank spaces\n",
    "    string_index = string_index.strip()\n",
    "    \n",
    "    if string_index == '-':\n",
    "        return pd.NA\n",
    "    else:\n",
    "        return int(string_index)\n",
    "\n",
    "def edist_alignment(tokenized_sentence1, tokenized_sentence2):\n",
    "    \"\"\"\n",
    "    Return the list of tuples of aligned indices\n",
    "    \"\"\"\n",
    "    \n",
    "    alignment = sed.standard_sed_backtrace(tokenized_sentence1, tokenized_sentence2)\n",
    "    # Reformat the output from editst\n",
    "    alignment = str(alignment).replace('[', '').replace(']', '').split(', ')\n",
    "    alignment = [x.split('vs.') for x in alignment]\n",
    "    alignment = [(cast_to_index(x[0]), cast_to_index(x[1])) for x in alignment]\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "def reorder_according_to_alignment(tokenized_sentence1, tokenized_sentence2, alignment):\n",
    "    \"\"\"\n",
    "    Given the alignment tuples, reorder the second sentence to align to the first sentence\n",
    "    \"\"\"\n",
    "    reordered_tokenized_sentence2 = [pd.NA] * len(tokenized_sentence1)\n",
    "    for alignment_tuple in alignment:\n",
    "        sentence1_idx, sentence2_idx = alignment_tuple\n",
    "        if (not pd.isnull(sentence1_idx)) and (not pd.isnull(sentence2_idx)):\n",
    "            reordered_tokenized_sentence2[sentence1_idx] = tokenized_sentence2[sentence2_idx]\n",
    "    return reordered_tokenized_sentence2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57c2d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_tag(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "def is_content_tag(nltk_pos):\n",
    "    content_tags_prefix = ['NN', 'V', 'JJ', 'PRP']  # Noun, verb, adj, adv (RB, but removed), pronoun\n",
    "    for prefix in content_tags_prefix:\n",
    "        if nltk_pos.startswith(prefix):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def uniquify(df_columns):\n",
    "    \"\"\"\n",
    "    Add suffix to distinguish duplicated colunms' names\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "\n",
    "    for item in df_columns:\n",
    "        fudge = 1\n",
    "        newitem = item\n",
    "\n",
    "        while newitem in seen:\n",
    "            fudge += 1\n",
    "            newitem = \"{}_{}\".format(item, fudge)\n",
    "\n",
    "        yield newitem\n",
    "        seen.add(newitem)\n",
    "        \n",
    "        \n",
    "def align_src_tgt_translations(sentence_df):\n",
    "    # Convert everything to lowercase\n",
    "    sentence_df = sentence_df.copy()\n",
    "    sentence_df['SRC'] = sentence_df['SRC'].apply(lambda x: x.lower())\n",
    "    sentence_df['original_trans_alignment'] = sentence_df['original_trans_alignment'].apply(\n",
    "        lambda x: dict(\n",
    "            (k.lower(), v.lower()) for k,v in x.items()\n",
    "        )\n",
    "    )\n",
    "    sentence_df['perturbed_trans_alignment'] = sentence_df['perturbed_trans_alignment'].apply(\n",
    "        lambda x: dict(\n",
    "            (k.lower(), v.lower()) for k,v in x.items()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    original_word = sentence_df['original_word'].values[0]\n",
    "    original_src = sentence_df['SRC'].values[0]\n",
    "    original_trans_alignment = sentence_df['original_trans_alignment'].values[0]\n",
    "    \n",
    "    original_src_tokenized = nltk.word_tokenize(original_src)\n",
    "    original_word_index = original_src_tokenized.index(original_word)\n",
    "    original_src_tokenized[original_word_index] = '[MASK]'\n",
    "\n",
    "    result_df = pd.DataFrame(\n",
    "        index=[original_word]+sentence_df['perturbed_word'].tolist(), \n",
    "        columns=original_src_tokenized\n",
    "    )\n",
    "    \n",
    "    # Add the original translation \n",
    "    result_df.loc[original_word] = original_trans_alignment\n",
    "    result_df.loc[original_word, '[MASK]'] = \\\n",
    "        original_trans_alignment[original_word] if original_word in original_trans_alignment.keys() else pd.NA\n",
    "    \n",
    "    # Add the perturbed translation\n",
    "    for index, row in sentence_df.iterrows():\n",
    "        perturbed_word = row['perturbed_word']\n",
    "        perturbed_trans_alignment = row['perturbed_trans_alignment']\n",
    "        result_df.loc[perturbed_word] = perturbed_trans_alignment\n",
    "        result_df.loc[perturbed_word, '[MASK]'] = \\\n",
    "            perturbed_trans_alignment[perturbed_word] if perturbed_word in perturbed_trans_alignment.keys() else pd.NA\n",
    "    \n",
    "    # Fix columns with same name (due to word occurs twice in a sentence)\n",
    "    result_df.columns = uniquify(result_df.columns)\n",
    "    \n",
    "    return result_df\n",
    "    \n",
    "    \n",
    "def align_translations_tgt_only(sentence_df):\n",
    "    \"\"\"\n",
    "    Align all perturbed translations with the original translation\n",
    "    \"\"\"\n",
    "    original_word = sentence_df['original_word'].values[0]\n",
    "    original_trans_tokenized = sentence_df['tokenized_OriginalSRC-Trans'].values[0]\n",
    "    \n",
    "    result_df = pd.DataFrame(\n",
    "        index=[original_word]+sentence_df['perturbed_word'].tolist(), columns=original_trans_tokenized\n",
    "    )\n",
    "    \n",
    "    # Add the original translation \n",
    "    result_df.loc[original_word] = original_trans_tokenized\n",
    "    \n",
    "    # Add the perturbed translation\n",
    "    for index, row in sentence_df.iterrows():\n",
    "        perturbed_word = row['perturbed_word']\n",
    "        alignment = edist_alignment(original_trans_tokenized, row['tokenized_SRC_perturbed-Trans'])\n",
    "        result_df.loc[perturbed_word] = reorder_according_to_alignment(\n",
    "            original_trans_tokenized, row['tokenized_SRC_perturbed-Trans'], alignment\n",
    "        )\n",
    "    \n",
    "    # Fix columns with same name (due to word occurs twice in a sentence)\n",
    "    result_df.columns = uniquify(result_df.columns)\n",
    "    \n",
    "    return result_df\n",
    "        \n",
    "        \n",
    "def align_translations(sentence_df, align_type=\"src-trans\"):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        sentence_df: df containing the different unmasking results of a masked sentence, along with the translations\n",
    "        align_type: \"src-trans\" align the translations with the source sentence, using awesome-align\n",
    "                    \"trans-only\" align the translations with eachother, using edit distance\n",
    "    Returns:\n",
    "        result_df: the aligned translations of different perturbed src\n",
    "    \"\"\"\n",
    "    \n",
    "    count_original_word = sentence_df['original_word'].value_counts()\n",
    "    assert count_original_word.shape[0] == 1  # Because this function is for a single group\n",
    "\n",
    "    if align_type == \"src-trans\":\n",
    "        return align_src_tgt_translations(sentence_df)\n",
    "    elif align_type == \"trans-only\":\n",
    "        return align_translations_tgt_only(sentence_df)\n",
    "    else:\n",
    "        raise RuntimeError('Invalid alignment type')\n",
    "    \n",
    "    \n",
    "def analyse_single_sentence_perturbed_word(sentence_perturbed_word_df, align_type=\"trans-only\"):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        sentence_perturbed_word_df: df containing the different unmasking results of a masked sentence, along with the translations\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    nr_replacements = sentence_perturbed_word_df.shape[0]\n",
    "    \n",
    "    \n",
    "    aligned_trans = align_translations(sentence_perturbed_word_df, align_type=\"trans-only\")\n",
    "    \n",
    "    result = {'perturbed_or_noise_words': [], \n",
    "              'words_with_clustered_trans': {}, \n",
    "              'words_with_single_trans': {}}\n",
    "    \n",
    "    for col in aligned_trans.columns:\n",
    "        # We only care about the content word (e.g., not a, an, the,...)\n",
    "        word = col.split('_')[0]\n",
    "        if is_content_tag(nltk_pos_tag(word)):\n",
    "            count_unique_translated_words = aligned_trans[col].value_counts()\n",
    "            nr_unique_words = count_unique_translated_words.shape[0]\n",
    "            \n",
    "            if nr_unique_words >= 5:\n",
    "                # If number of unique translations are large,\n",
    "                # then this is the column of the perturbed word or noise\n",
    "                result['perturbed_or_noise_words'].append(col)\n",
    "            elif 2 <= nr_unique_words and nr_unique_words < 5:\n",
    "                # TODO: TEMPORARYLY LEAVING OUT SIMILARITY CALCULATION\n",
    "#                 # Report the word and the minimum similarity between pair-wise unique translations\n",
    "#                 unique_words = count_unique_translated_words.index.tolist()\n",
    "#                 all_similarities = []\n",
    "#                 for i in range(0, len(unique_words)):\n",
    "#                     for j in range(i, len(unique_words)):\n",
    "#                         all_similarities.append(de_model.similarity(unique_words[i], unique_words[j]))\n",
    "                result['words_with_clustered_trans'][col] = count_unique_translated_words.to_dict()\n",
    "            elif nr_unique_words == 1:\n",
    "                result['words_with_single_trans'][col] = count_unique_translated_words.index[0]\n",
    "            \n",
    "            \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea226b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_single_sentence(sentence_df, align_type=\"trans-only\"):\n",
    "    count_original_sentence_idx = sentence_df['SRC_original_idx'].value_counts()\n",
    "    assert count_original_sentence_idx.shape[0] == 1  # Because this function is for a single group\n",
    "    \n",
    "    groups_by_perturbed_word = sentence_df.groupby(\"original_word\", as_index=False)\n",
    "    \n",
    "\n",
    "    collect_results = {}\n",
    "    for original_word, group_by_perturbed_word in groups_by_perturbed_word:\n",
    "        collect_results[original_word] = analyse_single_sentence_perturbed_word(group_by_perturbed_word, align_type=\"trans-only\")\n",
    "        \n",
    "    \n",
    "    # For ambiguous words, find the perturbed words that makes its trans ambiguous,\n",
    "    # and the perturbed words that makes its trans consistence\n",
    "    ambiguous_words = set(\n",
    "        sum([list(x['words_with_clustered_trans'].keys()) for x in collect_results.values()],\n",
    "            [])\n",
    "    )\n",
    "    \n",
    "    result = {}\n",
    "    for ambiguous_word in ambiguous_words:\n",
    "        no_effect_words = []\n",
    "        effect_words = []\n",
    "        \n",
    "        for original_word, collected_result in collect_results.items():\n",
    "            if ambiguous_word in collected_result['words_with_clustered_trans']:\n",
    "                effect_words.append(original_word)\n",
    "            elif ambiguous_word in collected_result['words_with_single_trans']:\n",
    "                no_effect_words.append(original_word)\n",
    "    \n",
    "        \n",
    "        result[ambiguous_word] = {'no_effect_words': no_effect_words,\n",
    "                                  'effect_words': effect_words}\n",
    "    \n",
    "    \n",
    "    return result\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ee30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ecafdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis = output.groupby('SRC_index').apply(lambda x: analyse_single_sentence_perturbed_word(x))  #.rename(columns={None: 'influenced_words'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e73f2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_pickle('analyse_winoMT.pkl')\n",
    "# output = pd.read_pickle('analyse_winoMT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f20bf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRC</th>\n",
       "      <th>SRC_original_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>After all, I was the so-called expert there, the former 20-year television news anchor and life and business coach.</td>\n",
       "      <td>fr-0843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>AB: I was 24 years old and at the top of my game when a freak summersault while downhill skiing paralyzed me.</td>\n",
       "      <td>fr-0082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>Now, as you may have noticed, I'm six-feet tall, and when I arrived at my chosen university and realized our men's Division III basketball team averaged five-foot-eight, I abandoned the on-campus scene and went online.</td>\n",
       "      <td>fr-0615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>When I resisted this idea of being surveilled by my ex-husband, he really didn't approve of this and threw me out of his house, along with my six-month-old son, Abdullah.</td>\n",
       "      <td>fr-0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>I was betting that I'd be able to find everything else I could possible want to wear once I got here to Palm Springs.</td>\n",
       "      <td>fr-0116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                            SRC  \\\n",
       "390                                                                                                         After all, I was the so-called expert there, the former 20-year television news anchor and life and business coach.   \n",
       "720                                                                                                               AB: I was 24 years old and at the top of my game when a freak summersault while downhill skiing paralyzed me.   \n",
       "750  Now, as you may have noticed, I'm six-feet tall, and when I arrived at my chosen university and realized our men's Division III basketball team averaged five-foot-eight, I abandoned the on-campus scene and went online.   \n",
       "780                                                  When I resisted this idea of being surveilled by my ex-husband, he really didn't approve of this and threw me out of his house, along with my six-month-old son, Abdullah.   \n",
       "870                                                                                                       I was betting that I'd be able to find everything else I could possible want to wear once I got here to Palm Springs.   \n",
       "\n",
       "    SRC_original_idx  \n",
       "390          fr-0843  \n",
       "720          fr-0082  \n",
       "750          fr-0615  \n",
       "780          fr-0366  \n",
       "870          fr-0116  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some missing info samples from mustSHE\n",
    "output[output['CATEGORY']=='1F'][['SRC', 'SRC_original_idx']].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6691df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original SRC sentence: \n",
      " SRC    AB: I was 24 years old and at the top of my game when a freak summersault while downhill skiing paralyzed me.\n",
      "Name: fr-0082, dtype: object\n",
      "\n",
      "{'Jahre': {'effect_words': ['old'],\n",
      "           'no_effect_words': ['ab',\n",
      "                               'downhill',\n",
      "                               'freak',\n",
      "                               'game',\n",
      "                               'i',\n",
      "                               'me',\n",
      "                               'my',\n",
      "                               'paralyzed',\n",
      "                               'skiing',\n",
      "                               'summersault',\n",
      "                               'top']},\n",
      " 'Sommersault': {'effect_words': ['ab',\n",
      "                                  'downhill',\n",
      "                                  'freak',\n",
      "                                  'game',\n",
      "                                  'i',\n",
      "                                  'me',\n",
      "                                  'my',\n",
      "                                  'old',\n",
      "                                  'skiing',\n",
      "                                  'summersault',\n",
      "                                  'top',\n",
      "                                  'years'],\n",
      "                 'no_effect_words': ['paralyzed']},\n",
      " 'Spiels': {'effect_words': ['my', 'top'],\n",
      "            'no_effect_words': ['ab',\n",
      "                                'downhill',\n",
      "                                'freak',\n",
      "                                'i',\n",
      "                                'me',\n",
      "                                'old',\n",
      "                                'paralyzed',\n",
      "                                'skiing',\n",
      "                                'summersault',\n",
      "                                'years']},\n",
      " 'ausgeflippter': {'effect_words': ['me'],\n",
      "                   'no_effect_words': ['ab',\n",
      "                                       'downhill',\n",
      "                                       'game',\n",
      "                                       'i',\n",
      "                                       'my',\n",
      "                                       'old',\n",
      "                                       'paralyzed',\n",
      "                                       'skiing',\n",
      "                                       'top',\n",
      "                                       'years']},\n",
      " 'beim': {'effect_words': ['skiing'],\n",
      "          'no_effect_words': ['ab',\n",
      "                              'downhill',\n",
      "                              'freak',\n",
      "                              'game',\n",
      "                              'i',\n",
      "                              'me',\n",
      "                              'my',\n",
      "                              'old',\n",
      "                              'paralyzed',\n",
      "                              'summersault',\n",
      "                              'top',\n",
      "                              'years']},\n",
      " 'der': {'effect_words': ['game'],\n",
      "         'no_effect_words': ['ab',\n",
      "                             'downhill',\n",
      "                             'freak',\n",
      "                             'i',\n",
      "                             'me',\n",
      "                             'my',\n",
      "                             'old',\n",
      "                             'paralyzed',\n",
      "                             'skiing',\n",
      "                             'summersault',\n",
      "                             'years']},\n",
      " 'ein': {'effect_words': ['summersault'],\n",
      "         'no_effect_words': ['ab',\n",
      "                             'downhill',\n",
      "                             'freak',\n",
      "                             'game',\n",
      "                             'i',\n",
      "                             'me',\n",
      "                             'my',\n",
      "                             'old',\n",
      "                             'paralyzed',\n",
      "                             'skiing',\n",
      "                             'top',\n",
      "                             'years']},\n",
      " 'lähmte': {'effect_words': ['me', 'skiing'],\n",
      "            'no_effect_words': ['ab',\n",
      "                                'downhill',\n",
      "                                'freak',\n",
      "                                'game',\n",
      "                                'i',\n",
      "                                'my',\n",
      "                                'old',\n",
      "                                'summersault',\n",
      "                                'top',\n",
      "                                'years']},\n",
      " 'meines': {'effect_words': ['game', 'top'],\n",
      "            'no_effect_words': ['ab',\n",
      "                                'downhill',\n",
      "                                'freak',\n",
      "                                'i',\n",
      "                                'me',\n",
      "                                'old',\n",
      "                                'paralyzed',\n",
      "                                'skiing',\n",
      "                                'summersault',\n",
      "                                'years']},\n",
      " 'war': {'effect_words': ['i'],\n",
      "         'no_effect_words': ['ab',\n",
      "                             'downhill',\n",
      "                             'freak',\n",
      "                             'game',\n",
      "                             'me',\n",
      "                             'my',\n",
      "                             'old',\n",
      "                             'paralyzed',\n",
      "                             'skiing',\n",
      "                             'summersault',\n",
      "                             'top',\n",
      "                             'years']}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 9999999)\n",
    "\n",
    "sentence_idx = 'fr-0082'\n",
    "print(f\"Original SRC sentence: \\n {output[['SRC', 'SRC_original_idx']].drop_duplicates().set_index('SRC_original_idx').loc[sentence_idx]}\")\n",
    "print()\n",
    "\n",
    "pprint.pprint(analyse_single_sentence(output[output['SRC_original_idx'] == sentence_idx], align_type=\"trans-only\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce851487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perturbed_or_noise_words': ['Skifahren'],\n",
      " 'words_with_clustered_trans': {'Sommersault': {'Sommersault': 9,\n",
      "                                                'Sommersturz': 14,\n",
      "                                                'Summersault': 8},\n",
      "                                'beim': {'beim': 12, 'während': 19},\n",
      "                                'lähmte': {'Abfahrt': 2, 'lähmte': 29}},\n",
      " 'words_with_single_trans': {'AB': 'AB',\n",
      "                             'Ich': 'Ich',\n",
      "                             'Jahre': 'Jahre',\n",
      "                             'Spiels': 'Spiels',\n",
      "                             'Spitze': 'Spitze',\n",
      "                             'als': 'als',\n",
      "                             'alt': 'alt',\n",
      "                             'ausgeflippter': 'ausgeflippter',\n",
      "                             'der': 'der',\n",
      "                             'ein': 'ein',\n",
      "                             'meines': 'meines',\n",
      "                             'mich': 'mich',\n",
      "                             'und': 'und',\n",
      "                             'war': 'war'}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>:</th>\n",
       "      <th>Ich</th>\n",
       "      <th>war</th>\n",
       "      <th>24</th>\n",
       "      <th>Jahre</th>\n",
       "      <th>alt</th>\n",
       "      <th>und</th>\n",
       "      <th>an</th>\n",
       "      <th>der</th>\n",
       "      <th>Spitze</th>\n",
       "      <th>meines</th>\n",
       "      <th>Spiels</th>\n",
       "      <th>,</th>\n",
       "      <th>als</th>\n",
       "      <th>mich</th>\n",
       "      <th>ein</th>\n",
       "      <th>ausgeflippter</th>\n",
       "      <th>Sommersault</th>\n",
       "      <th>beim</th>\n",
       "      <th>Skifahren</th>\n",
       "      <th>lähmte</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>skiing</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Skifahren</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skier</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>Abfahrt</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>downhill</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hiking</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Bergsteigen</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climbing</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Summersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Abfahrtsklettern</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walking</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Bergabgehen</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swimming</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Abfahrtsschwimmen</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slalom</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>während</td>\n",
       "      <td>des</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>driving</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>riding</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racing</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Summersault</td>\n",
       "      <td>während</td>\n",
       "      <td>eines</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>there</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nearly</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Summersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Abfahrtslauf</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolling</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Summersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Bergabrollen</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##s</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skating</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Skaten</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dipping</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Summersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Abtauchen</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ing</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Summersault</td>\n",
       "      <td>beim</td>\n",
       "      <td>Bergabfahren</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Summersault</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>beim</td>\n",
       "      <td>Abfahrtstraining</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cycling</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>beim</td>\n",
       "      <td>Downhill-Radfahren</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersault</td>\n",
       "      <td>während</td>\n",
       "      <td>eines</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>Abfahrt</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completely</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Sommersturz</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>below</th>\n",
       "      <td>AB</td>\n",
       "      <td>:</td>\n",
       "      <td>Ich</td>\n",
       "      <td>war</td>\n",
       "      <td>24</td>\n",
       "      <td>Jahre</td>\n",
       "      <td>alt</td>\n",
       "      <td>und</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Spitze</td>\n",
       "      <td>meines</td>\n",
       "      <td>Spiels</td>\n",
       "      <td>,</td>\n",
       "      <td>als</td>\n",
       "      <td>mich</td>\n",
       "      <td>ein</td>\n",
       "      <td>ausgeflippter</td>\n",
       "      <td>Summersault</td>\n",
       "      <td>während</td>\n",
       "      <td>der</td>\n",
       "      <td>lähmte</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines  \\\n",
       "skiing      AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "skier       AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "had         AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "downhill    AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "hiking      AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       ",           AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "climbing    AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "slope       AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "walking     AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "swimming    AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "course      AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "slalom      AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "driving     AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "riding      AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "racing      AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "there       AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "nearly      AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "running     AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "rolling     AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "##s         AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "skating     AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "dipping     AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "##ing       AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "high        AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "training    AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "cycling     AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "racer       AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "has         AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "completely  AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "-           AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "below       AB  :  Ich  war  24  Jahre  alt  und  an  der  Spitze  meines   \n",
       "\n",
       "            Spiels  ,  als  mich  ein  ausgeflippter  Sommersault     beim  \\\n",
       "skiing      Spiels  ,  als  mich  ein  ausgeflippter  Sommersault     beim   \n",
       "skier       Spiels  ,  als  mich  ein  ausgeflippter  Sommersault  während   \n",
       "had         Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "downhill    Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "hiking      Spiels  ,  als  mich  ein  ausgeflippter  Sommersault     beim   \n",
       ",           Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "climbing    Spiels  ,  als  mich  ein  ausgeflippter  Summersault     beim   \n",
       "slope       Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "walking     Spiels  ,  als  mich  ein  ausgeflippter  Sommersault     beim   \n",
       "swimming    Spiels  ,  als  mich  ein  ausgeflippter  Sommersault     beim   \n",
       "course      Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "slalom      Spiels  ,  als  mich  ein  ausgeflippter  Sommersault  während   \n",
       "driving     Spiels  ,  als  mich  ein  ausgeflippter  Sommersault  während   \n",
       "riding      Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "racing      Spiels  ,  als  mich  ein  ausgeflippter  Summersault  während   \n",
       "there       Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "nearly      Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "running     Spiels  ,  als  mich  ein  ausgeflippter  Summersault     beim   \n",
       "rolling     Spiels  ,  als  mich  ein  ausgeflippter  Summersault     beim   \n",
       "##s         Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "skating     Spiels  ,  als  mich  ein  ausgeflippter  Sommersault     beim   \n",
       "dipping     Spiels  ,  als  mich  ein  ausgeflippter  Summersault     beim   \n",
       "##ing       Spiels  ,  als  mich  ein  ausgeflippter  Summersault     beim   \n",
       "high        Spiels  ,  als  mich  ein  ausgeflippter  Summersault  während   \n",
       "training    Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz     beim   \n",
       "cycling     Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz     beim   \n",
       "racer       Spiels  ,  als  mich  ein  ausgeflippter  Sommersault  während   \n",
       "has         Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "completely  Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "-           Spiels  ,  als  mich  ein  ausgeflippter  Sommersturz  während   \n",
       "below       Spiels  ,  als  mich  ein  ausgeflippter  Summersault  während   \n",
       "\n",
       "                     Skifahren   lähmte  .  \n",
       "skiing               Skifahren   lähmte  .  \n",
       "skier                      der   lähmte  .  \n",
       "had                        der  Abfahrt  .  \n",
       "downhill                   der   lähmte  .  \n",
       "hiking             Bergsteigen   lähmte  .  \n",
       ",                          der   lähmte  .  \n",
       "climbing      Abfahrtsklettern   lähmte  .  \n",
       "slope                      der   lähmte  .  \n",
       "walking            Bergabgehen   lähmte  .  \n",
       "swimming     Abfahrtsschwimmen   lähmte  .  \n",
       "course                     der   lähmte  .  \n",
       "slalom                     des   lähmte  .  \n",
       "driving                    der   lähmte  .  \n",
       "riding                     der   lähmte  .  \n",
       "racing                   eines   lähmte  .  \n",
       "there                      der   lähmte  .  \n",
       "nearly                     der   lähmte  .  \n",
       "running           Abfahrtslauf   lähmte  .  \n",
       "rolling           Bergabrollen   lähmte  .  \n",
       "##s                        der   lähmte  .  \n",
       "skating                 Skaten   lähmte  .  \n",
       "dipping              Abtauchen   lähmte  .  \n",
       "##ing             Bergabfahren   lähmte  .  \n",
       "high                       der   lähmte  .  \n",
       "training      Abfahrtstraining   lähmte  .  \n",
       "cycling     Downhill-Radfahren   lähmte  .  \n",
       "racer                    eines   lähmte  .  \n",
       "has                        der  Abfahrt  .  \n",
       "completely                 der   lähmte  .  \n",
       "-                          der   lähmte  .  \n",
       "below                      der   lähmte  .  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_word = 'skiing'\n",
    "\n",
    "\n",
    "sentence_df = output[output['SRC_original_idx'] == sentence_idx]\n",
    "sentence_single_perturbed_word_df = sentence_df[sentence_df['original_word'] == original_word]\n",
    "\n",
    "\n",
    "pprint.pprint(analyse_single_sentence_perturbed_word(sentence_single_perturbed_word_df, align_type=\"trans-only\"))\n",
    "align_translations(sentence_single_perturbed_word_df, align_type=\"trans-only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eac687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a97e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690e37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ef9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523895b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5736d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915f01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019f6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d08bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ff9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1c93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225343d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fbab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ce163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test to see if SRC_similarity is higher than Trans_similarity\n",
    "print(output[\"Trans-edit_distance\"].mean() - output[\"SRC-edit_distance\"].mean())\n",
    "stats.ttest_rel(output[\"SRC-edit_distance\"], \n",
    "                output[\"Trans-edit_distance\"], \n",
    "                alternative='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187eb64",
   "metadata": {},
   "source": [
    "Tiny pvalue --> Indeed SRC-edit_distance is significantly lower than Trans-edit_distance\n",
    "\n",
    "\n",
    "(Careful with this tho, bc with number of samples too large then statistical test does not make sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada80a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(output[\"#TransChanges-#SrcChanges\"], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49917779",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"ChangesSpread/SentenceLength\"].describe())\n",
    "output[\"ChangesSpread/SentenceLength\"].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c12040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bfa2ffb",
   "metadata": {},
   "source": [
    "Some changes seems to have the same meaning but different phrasing, e.g., noun index 24, 36, 47\n",
    "\n",
    "Both for en-de and en-vi\n",
    "\n",
    "\n",
    "Kind of bias: en-vi adjective sample 82\n",
    "\n",
    "Should we cherry-pick examples? Or cherry-pick the replacement?\n",
    "\n",
    "\n",
    "Or narrow down scope of perturbation? (e.g., on countries, jobs, gender, ...?)\n",
    "\n",
    "\n",
    "\n",
    "Some cherry-picked examples anyway:\n",
    "\n",
    "- He comes from England --> Ông ấy đến từ Anh\n",
    "- He comes from Vietnam --> Hắn đến từ Việt Nam\n",
    "- He comes from North Korea --> Hắn đến từ Bắc Triều Tiên\n",
    "\n",
    "\n",
    "\n",
    "- He is european --> Hắn là người Châu Âu\n",
    "- He is asian --> Anh ấy là người châu Á.\n",
    "\n",
    "\n",
    "\n",
    "- He has black hair --> Hắn có tóc đen.\n",
    "- He has blonde hair --> Anh ấy có tóc vàng\n",
    "\n",
    "\n",
    "But if we limit this then it would hurt the model overal performance as well? \n",
    "\n",
    "*Jan: some kind of loss to minimize the number of changes, but not completely forbidden the changes*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e35e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ca514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755682c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be18d809",
   "metadata": {},
   "source": [
    "# Translation quality vs #changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "output[\"OriginalTran_Quality\"] = output.apply(\n",
    "    lambda x: sentence_gleu([nltk.word_tokenize(x['REF'])], nltk.word_tokenize(x['OriginalSRC-Trans'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.plot.scatter(x='OriginalTran_Quality', y=\"#TransChanges-#SrcChanges/SentenceLength\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(output['OriginalTran_Quality'], output[\"#TransChanges-#SrcChanges/SentenceLength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(output[\"OriginalTran_Quality\"], bins='sturges')\n",
    "bin_boundaries = hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f74bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use bins with same number of samples instead of equal-sized bins\n",
    "\n",
    "# results, bin_boundaries = pd.qcut(output[\"OriginalTran_Quality\"], q=5, retbins=True)\n",
    "# bin_boundaries\n",
    "\n",
    "\n",
    "# Remove bins with too few samples\n",
    "cut_point = 99999\n",
    "for i, value in enumerate(hist[0]):\n",
    "    if value < 5:\n",
    "        cut_point = i\n",
    "        break\n",
    "        \n",
    "bin_boundaries = bin_boundaries[:i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_boundaries\n",
    "\n",
    "X = output['OriginalTran_Quality']\n",
    "Y = output[\"#TransChanges-#SrcChanges/SentenceLength\"]\n",
    "\n",
    "x_plot = [(bin_boundaries[i] + bin_boundaries[i+1])/2 for i in range(0, len(bin_boundaries)-1)]\n",
    "y_plot = [stats.trim_mean(Y[(bin_boundaries[i] < X) & (X < bin_boundaries[i+1])], 0.1) for i in range(0, len(bin_boundaries)-1)]\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('OriginalTrans_Quality')\n",
    "plt.ylabel('Avg_changes')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cb6e1ad",
   "metadata": {},
   "source": [
    "bins = [(bin_boundaries[i], bin_boundaries[i+1]) for i in range(0, len(bin_boundaries)-1)]\n",
    "\n",
    "X = output['OriginalTran_Quality']\n",
    "Y = output[\"#TransChanges-#SrcChanges\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.boxplot([Y[(bin_i[0] < X) & (X < bin_i[1])] for bin_i in bins])\n",
    "# ax.set_xticklabels(bins)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_xlabel('OriginalTran_Quality')\n",
    "ax.set_ylabel('#changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd6f40",
   "metadata": {},
   "source": [
    "Most of the time downward trend (not as clear for en-de with verb, adverb, pronoun; en-vi adverb, pronoun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602006a",
   "metadata": {},
   "source": [
    "**Note**: the plot has outliers removed in both X and Y dimensions, by removing too small bins (X) and trimmed-mean (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f10e72",
   "metadata": {},
   "source": [
    "# #changes vs translation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(output[\"#TransChanges-#SrcChanges\"], bins=20)\n",
    "bin_boundaries = hist[1]\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3529f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use bins with same number of samples instead of equal-sized bins\n",
    "# results, bin_boundaries = pd.qcut(output[\"#TransChanges-#SrcChanges\"], q=5, retbins=True)\n",
    "# bin_boundaries\n",
    "\n",
    "\n",
    "# Remove bins with too few samples\n",
    "cut_point = 99999\n",
    "for i, value in enumerate(hist[0]):\n",
    "    if value < 10:\n",
    "        cut_point = i\n",
    "        break\n",
    "        \n",
    "bin_boundaries = bin_boundaries[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb682d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_boundaries\n",
    "\n",
    "X = output['#TransChanges-#SrcChanges']\n",
    "Y = output[\"OriginalTran_Quality\"]\n",
    "\n",
    "x_plot = [(bin_boundaries[i] + bin_boundaries[i+1])/2 for i in range(0, len(bin_boundaries)-1)]\n",
    "\n",
    "y_plot = [stats.trim_mean(Y[(bin_boundaries[i] <= X) & (X <= bin_boundaries[i+1])], 0.1) for i in range(0, len(bin_boundaries)-1)]\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('Avg_changes')\n",
    "plt.ylabel('OriginalTran_Quality')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edfd7c1d",
   "metadata": {},
   "source": [
    "bins = [(bin_boundaries[i], bin_boundaries[i+1]) for i in range(0, len(bin_boundaries)-1)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.boxplot([Y[(bin_i[0] < X) & (X < bin_i[1])] for bin_i in bins])\n",
    "# ax.set_xticklabels(bins)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_xlabel('#changes')\n",
    "ax.set_ylabel('OriginalTran_Quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c740f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30078a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55ac724c",
   "metadata": {},
   "source": [
    "# SentenceLength vs #changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['OriginalSRC-length'] = output.apply(\n",
    "    lambda x: len(nltk.word_tokenize(x['SRC'])), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a88887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.plot.scatter(x='OriginalSRC-length', y=\"#TransChanges-#SrcChanges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f0011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65022c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(output['OriginalSRC-length'], output[\"#TransChanges-#SrcChanges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(output[\"OriginalSRC-length\"], bins=20)\n",
    "bin_boundaries = hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192befe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bins with too few samples\n",
    "cut_point = 99999\n",
    "for i, value in enumerate(hist[0]):\n",
    "    if value < 10:\n",
    "        cut_point = i\n",
    "        break\n",
    "        \n",
    "bin_boundaries = bin_boundaries[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ad438",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = output['OriginalSRC-length']\n",
    "Y = output[\"#TransChanges-#SrcChanges\"]\n",
    "\n",
    "x_plot = [(bin_boundaries[i] + bin_boundaries[i+1])/2 for i in range(0, len(bin_boundaries)-1)]\n",
    "y_plot = [stats.trim_mean(Y[(bin_boundaries[i] < X) & (X < bin_boundaries[i+1])], 0.1) for i in range(0, len(bin_boundaries)-1)]\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('OriginalSRC-length')\n",
    "plt.ylabel('Avg_changes')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c18891ee",
   "metadata": {},
   "source": [
    "bins = [(bin_boundaries[i], bin_boundaries[i+1]) for i in range(0, len(bin_boundaries)-1)]\n",
    "\n",
    "X = output['OriginalSRC-length']\n",
    "Y = output[\"#TransChanges-#SrcChanges\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.boxplot([Y[(bin_i[0] < X) & (X < bin_i[1])] for bin_i in bins])\n",
    "# ax.set_xticklabels(bins)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_xlabel('OriginalSRC-length')\n",
    "ax.set_ylabel('#changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5a948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7635704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bc35529",
   "metadata": {},
   "source": [
    "# Beam_size vs #changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_dict = {}\n",
    "beam_values = [1,2,3,4,5]\n",
    "for beam in beam_values:\n",
    "    beam_dict[beam] = read_output_df(dataset, perturb_type, beam, replacement_strategy)\n",
    "    # Make sure the df all have the same index\n",
    "    if beam > 1:\n",
    "        assert beam_dict[beam].index.equals(beam_dict[beam].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63368394",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(beam_values,\n",
    "              [stats.trim_mean(beam_dict[x]['#TransChanges-#SrcChanges'], 0.1) for x in beam_values])\n",
    "plt.xlabel('beam')\n",
    "plt.ylabel('mean_changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5ff79",
   "metadata": {},
   "source": [
    "The mean might not saying anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76076635",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.boxplot([beam_dict[x]['#TransChanges-#SrcChanges'] for x in beam_values])\n",
    "ax.set_xticklabels(beam_values)\n",
    "ax.set_xlabel('beam')\n",
    "ax.set_ylabel('#changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34085167",
   "metadata": {},
   "source": [
    "# Perturbed word type vs #changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40211a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_type_dict = {}\n",
    "word_type_values = [\"noun\", \"verb\", \"adjective\", \"adverb\", \"pronoun\"]\n",
    "for word_type in word_type_values:\n",
    "    word_type_dict[word_type] = read_output_df(dataset, perturb_type=word_type, beam=beam, replacement_strategy=replacement_strategy)\n",
    "\n",
    "    \n",
    "print('--------------------------------')\n",
    "print('word type    -   trimmed-mean #changes')\n",
    "\n",
    "for word_type in word_type_values:\n",
    "    print(f\"{word_type} - {stats.trim_mean(word_type_dict[word_type]['#TransChanges-#SrcChanges'], 0.1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.boxplot([word_type_dict[x]['#TransChanges-#SrcChanges'] for x in word_type_values])\n",
    "ax.set_xticklabels(word_type_values)\n",
    "ax.set_xlabel('word_type')\n",
    "ax.set_ylabel('#changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b06547",
   "metadata": {},
   "source": [
    "# #Changes per sentence across word types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ceef7",
   "metadata": {},
   "source": [
    "See if the chaos changes are sentence-specific. Excluding perturbing pronouns bc not many samples have pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c13866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sentences that has multiple word types perturbed\n",
    "word_type_values = [\"noun\", \"verb\", \"adjective\", \"adverb\"]\n",
    "index_intersection = word_type_dict[word_type_values[0]].index\n",
    "for i in range(1, len(word_type_values)):\n",
    "    index_intersection = \\\n",
    "        index_intersection.intersection(word_type_dict[word_type_values[i]].index)\n",
    "\n",
    "len(index_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3094f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_per_word_type = pd.DataFrame()\n",
    "for word_type in word_type_values:\n",
    "    changes_per_word_type[word_type] = word_type_dict[word_type][\"#TransChanges-#SrcChanges\"].loc[index_intersection]\n",
    "    \n",
    "# Count the number of samples where the changes in trans always bigger than changes in SRC\n",
    "changes_per_word_type[(changes_per_word_type['noun'] > 0) & (changes_per_word_type['verb'] > 0) & \\\n",
    "                      (changes_per_word_type['adjective'] > 0) & (changes_per_word_type['adverb'] > 0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba41e95",
   "metadata": {},
   "source": [
    "Small portion of rows --> not sentence-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ace820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy import displacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"He is from Vietnam\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(f\"{'Node (from)-->':<15} {'Relation':^10} {'-->Node (to)':>15}\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"{:<15} {:^10} {:>15}\".format(str(token.head.text), str(token.dep_), str(token.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42699574",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(f\"Token: {token.text}\")\n",
    "    print(f\"Ancestors: {list(token.ancestors)}\")\n",
    "    print(f\"Children: {list(token.children)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy import displacy \n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "sentence = \"Er kommt aus Vietnam\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(f\"{'Node (from)-->':<15} {'Relation':^10} {'-->Node (to)':>15}\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"{:<15} {:^10} {:>15}\".format(str(token.head.text), str(token.dep_), str(token.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47c9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347d716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f080ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9975e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
