{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed052ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import random\n",
    "from difflib import SequenceMatcher\n",
    "from scipy import stats\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import edist.sed as sed\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42568741",
   "metadata": {},
   "source": [
    "### Align different translations, find which words affect the translation of which words\n",
    "\n",
    "\n",
    "**Note**: can use [sequence alignments](https://stackoverflow.com/questions/5055839/word-level-edit-distance-of-a-sentence) to align the sentences on the target side only. ([code](https://gist.github.com/slowkow/06c6dba9180d013dfd82bec217d22eb5))\n",
    "\n",
    "Pros: could be easier than SRC-TGT alignment\n",
    "\n",
    "Cons: in the case where more output different sentence structure yet same meaning. <br>\n",
    "E.g., \"Today I think the cat is nice\" -- \"I think the cat is nice today\"\n",
    "SRC-TGT alignment would probably see these as the same, but edit distance cannot, bc it only has del, insert, substitute operations.\n",
    "\n",
    "\n",
    "Provided in functions `analyse_single_sentence_single_perturbed_word()` and  `analyse_single_sentence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a767f2",
   "metadata": {},
   "source": [
    "# Quality analysis\n",
    "\n",
    "- Have to use WMT21 data, bc models for 2021 is available. Also they have clear evaluation script\n",
    "- Have to do some manual fix so that the translation tokenization match completely with the tokenization of the labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d1aad",
   "metadata": {},
   "source": [
    "### Word-level\n",
    "\n",
    "- A translated word is uncertain if changing other words in the SRC sentence affect its translations. The assumption is the the translation of one word should only depends on a few others word, but not too many.\n",
    "    - E.g., My mother, who had a difficult childhood, is a great doctor.\n",
    "    - The gender form of \"doctor\" should only change if we change the word \"mother\".\n",
    "\n",
    "- Hyperparam: The number of SRC words that effect the translations of the target word\n",
    "    - Currently: If a translated word has 3 or more effecting SRC word, mark as \"BAD\"\n",
    "\n",
    "- Metrics: Matthews correlation coefficient \n",
    "    \"It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\"\n",
    "    \n",
    "- Unsupervised baseline: since our approach is based on the uncertainty of a translated word, the baseline could be the word-level log probablity generated by the NMT model itself. (i.e., word with low certainty --> BAD)\n",
    "\n",
    "Current score: 0.267\n",
    "\n",
    "Score of WMT21 shared task: baseline 0.370, best 0.510\n",
    "\n",
    "WMT21 baseline: multilingual transformer-based Predictor-Estimator approach for both sentence level and word level\n",
    "\n",
    "\n",
    "**How to do hyperparameter tuning if our goal is not to use the training data? Or just using the dev set is oke?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733f83d",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Uses word-level log probablity generated by the NMT model itself. (i.e., word with low certainty --> BAD).\n",
    "\n",
    "Note that the NMT model output subwords and subword log probs, so for a word `A=a1a2` made of subwords `a1`, `a2`:\n",
    "\n",
    "prob(a1a2 | things before it, SRC) = prob(a1 | things before it, SRC) *  prob(a2 | a1, things before it, SRC) \n",
    "\n",
    "$$log(prob(A)) = log(prob(a1)) + log(prob(a2))$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Some times it output subwords that contains 2 tokens (i.e., the opposite of the above case). This does not happen very often\n",
    "\n",
    "Then we approximate:\n",
    "$$log(prob(a1)) = log(prob(a2)) = log(prob(A))/2$$\n",
    "\n",
    "**Is this correct???**\n",
    "\n",
    "\n",
    "\n",
    "Current best hyperparams setting on dev en-de:\n",
    "\n",
    "Our approach: hyperparams: \n",
    "- Perturbing [allTokens, allWords, allContentWords]\n",
    "- Word threshold: [1,2,3,4]\n",
    "\n",
    "Nmt word probas:\n",
    "- Threshold 0.5\n",
    "\n",
    "#### Word-level QE on translation\n",
    "\n",
    "- Our perturbation approach:\n",
    "    - **AllTokens, word threshold 3: 0.2716**\n",
    "    - AllWords, word threshold 3: 0.2647\n",
    "    - Content, word threshold 2: 0.2667\n",
    "- Use nmt model word probas:\n",
    "    - threshold 0.5: 0.2590\n",
    "    \n",
    "    \n",
    "#### Word-level QE on SRC\n",
    "- Our perturbation approach:\n",
    "    - **AllTokens, word threshold 3: 0.1308**\n",
    "    - AllWords, word threshold 3: 0.1275\n",
    "    - Content, word threshold 2: 0.1285\n",
    "- Use nmt model word probas:\n",
    "    - threshold 0.5: 0.2053\n",
    "    \n",
    "--> Worse than using nmt model word probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_and_analyse_ambiguous_trans import analyse_single_sentence_single_perturbed_word, analyse_single_sentence, align_translations\n",
    "from quality_estimation import nr_effecting_src_words_eval, nmt_log_prob_eval, flatten_list, load_gold_labels, replace_unknown\n",
    "\n",
    "dataset = 'WMT21_DA_dev'\n",
    "data_root_path = '../data'\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'de'\n",
    "effecting_words_threshold = 3\n",
    "nmt_log_prob_threshold = 0.5\n",
    "task = 'trans_word_level_eval'\n",
    "perturbed_trans_df_path = f'../analyse_output/{dataset}_{src_lang}2{tgt_lang}_MultiplePerSentence_allTokens/analyse_{dataset}_{src_lang}2{tgt_lang}_MultiplePerSentence_allTokens.pkl'\n",
    "\n",
    "perturb_based_pred = nr_effecting_src_words_eval(perturbed_trans_df_path, effecting_words_threshold, task)\n",
    "nmt_prob_based_pred = nmt_log_prob_eval(dataset, data_root_path, src_lang, tgt_lang, nmt_log_prob_threshold, perturbed_trans_df_path, task)\n",
    "gold_labels = load_gold_labels(dataset, data_root_path, src_lang, tgt_lang, task)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb16b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score ,accuracy_score\n",
    "\n",
    "labels = ['OK', 'BAD']\n",
    "recall = recall_score(flatten_list(gold_labels), \n",
    "                                      flatten_list(nmt_prob_based_pred), labels=labels, pos_label='BAD')\n",
    "precision = precision_score(flatten_list(gold_labels), \n",
    "                                      flatten_list(nmt_prob_based_pred), labels=labels, pos_label='BAD')\n",
    "\n",
    "print(f\"Recall wrt BAD: {recall}\")\n",
    "print(f\"Precision wrt BAD: {precision}\")\n",
    "print(f\"Matthews_corrcoef: {matthews_corrcoef(flatten_list(gold_labels), flatten_list(nmt_prob_based_pred))}\")\n",
    "\n",
    "print(f\"Percentage of unknown labels: {accuracy_score(flatten_list(nmt_prob_based_pred), ['unknown']*len(flatten_list(gold_labels)))*100}\")\n",
    "\n",
    "dist = ConfusionMatrixDisplay(confusion_matrix(flatten_list(gold_labels), \n",
    "                                               flatten_list(nmt_prob_based_pred), labels=labels),\n",
    "                             display_labels=labels)\n",
    "dist.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(flatten_list(gold_labels), \n",
    "                                      flatten_list(perturb_based_pred), labels=labels, pos_label='BAD')\n",
    "precision = precision_score(flatten_list(gold_labels), \n",
    "                                      flatten_list(perturb_based_pred), labels=labels, pos_label='BAD')\n",
    "\n",
    "print(f\"Recall wrt BAD: {recall}\")\n",
    "print(f\"Precision wrt BAD: {precision}\")\n",
    "print(f\"Matthews_corrcoef: {matthews_corrcoef(flatten_list(gold_labels), flatten_list(perturb_based_pred))}\")\n",
    "print(f\"Percentage of unknown labels: {accuracy_score(flatten_list(perturb_based_pred), ['unknown']*len(flatten_list(gold_labels)))*100}\")\n",
    "\n",
    "\n",
    "dist = ConfusionMatrixDisplay(confusion_matrix(flatten_list(gold_labels), \n",
    "                                               flatten_list(perturb_based_pred), labels=labels),\n",
    "                             display_labels=labels)\n",
    "dist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff094c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c9ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b49868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b9d3c28",
   "metadata": {},
   "source": [
    "Looking into the percentage of same predictions between `perturb_based_pred` and `nmt_prob_based_pred` to see if our approach is actually any different.\n",
    "\n",
    "0.6835, ok so not too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(flatten_list(gold_labels), flatten_list(perturb_based_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8180e",
   "metadata": {},
   "source": [
    "### Analyse some specific samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_trans_df = pd.read_pickle(perturbed_trans_df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_idx = 3\n",
    "sentence_df = perturbed_trans_df[perturbed_trans_df['SRC_original_idx'] == sentence_idx]\n",
    "original_SRC = sentence_df['SRC'].values[0]\n",
    "original_translation = sentence_df['SRC-Trans'].values[0]\n",
    "tok_original_translation = sentence_df['tokenized_SRC-Trans'].values[0]\n",
    "tok_original_SRC = sentence_df['tokenized_SRC'].values[0]\n",
    "\n",
    "my_pred = np.array(perturb_based_pred[sentence_idx])\n",
    "nmt_pred = np.array(nmt_prob_based_pred[sentence_idx])\n",
    "gold = np.array(gold_labels[sentence_idx])\n",
    "\n",
    "\n",
    "# BAD words correctly predicted by perturb_based_pred\n",
    "# bad_word_indices = np.nonzero(np.logical_and(my_pred=='BAD', gold=='BAD'))\n",
    "\n",
    "# # OK words predicted by perturb_based_pred as BAD\n",
    "bad_word_indices = np.nonzero(np.logical_and(my_pred=='BAD', gold=='OK'))\n",
    "\n",
    "\n",
    "# # BAD words correctly predicted by perturb_based_pred, but not by nmt_prob_based_pred\n",
    "# bad_word_indices = np.nonzero(np.logical_and(np.logical_and(my_pred=='BAD', gold=='OK'), gold=='BAD'))\n",
    "\n",
    "print(len(bad_word_indices[0]))\n",
    "word_idx = bad_word_indices[0][1]\n",
    "\n",
    "\n",
    "if task == 'src_word_level_eval':\n",
    "    word = tok_original_SRC[word_idx]\n",
    "    align_type = \"src-trans\"\n",
    "elif task == 'trans_word_level_eval':\n",
    "    word = tok_original_translation[word_idx]\n",
    "    align_type = \"trans-only\"\n",
    "else:\n",
    "    raise RuntimeError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "print(f\"Original SRC sentence:\\n{original_SRC}\")\n",
    "print(f\"Original trans:\\n{original_translation}\")\n",
    "print(f\"BAD word: {word}\")\n",
    "print()\n",
    "\n",
    "pprint.pprint(analyse_single_sentence(sentence_df, align_type=align_type, return_word_index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553dd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_word = 'Churchyard'\n",
    "\n",
    "\n",
    "sentence_single_perturbed_word_df = sentence_df[sentence_df['original_word'] == original_word]\n",
    "\n",
    "\n",
    "pprint.pprint(analyse_single_sentence_single_perturbed_word(sentence_single_perturbed_word_df, align_type=align_type))\n",
    "align_translations(sentence_single_perturbed_word_df, align_type=align_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ceb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e8859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d4628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736e316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65080ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea718be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924d189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167d9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e642f401",
   "metadata": {},
   "source": [
    "## Sentence level\n",
    "\n",
    "Approximations:\n",
    "- Negative corr with DA:\n",
    "    - Changes edit distance \n",
    "    - Changes edit distance / length\n",
    "    - Changes spread\n",
    "    - Changes spread / length\n",
    "    - Number of BAD tokens\n",
    "   \n",
    "Metrics: Pearson correlation coefficient: \"Correlations of -1 or +1 imply an exact linear relationship\"\n",
    "\n",
    "DA scores:\n",
    "Scores: highest 0.18\n",
    "WMT21 scores: baseline 0.403, best 0.584\n",
    "\n",
    "\n",
    "HTER scores: \n",
    "Scores: highest 0.28\n",
    "WMT21 scores: baseline 0.529, best 0.653\n",
    "\n",
    "**Can try to apply some function to the prediction, but then that's not unsupervised anymore**\n",
    "\n",
    "**Can use as feature for QE model, but again not unsupervised anymore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "approximations = output[\n",
    "    [\"SRC_original_idx\", \n",
    "     \"Trans-edit_distance\", \n",
    "     \"#TransChanges/SentenceLength\",\n",
    "     \"ChangesSpread\",\n",
    "     \"ChangesSpread/SentenceLength\"\n",
    "    ]\n",
    "].groupby(\"SRC_original_idx\").mean()\n",
    "\n",
    "approximations['word_level_agg'] = [x.count('BAD') for x in word_tag]\n",
    "\n",
    "\n",
    "for col in approximations.columns:\n",
    "    # Normalize the apporximations, invert the sign\n",
    "    approximations[col] = -approximations[col]\n",
    "    approximations[col] = zscore(approximations[col].values)\n",
    "    \n",
    "    \n",
    "approximations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29433315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_analysed_file = \"analyse_WMT22_MQM_en2de.pkl\"\n",
    "# output = pd.read_pickle(trans_analysed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ecd4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ccb08e1",
   "metadata": {},
   "source": [
    "Gold labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019f6cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'WMT21_DA' in trans_analysed_file:\n",
    "    with open(\"../data/wmt-qe-2021-data/en-de-test21/goldlabels/test21.hter\", 'r') as f:\n",
    "        da_scores = f.readlines()\n",
    "        da_scores = [float(da_score.replace('\\n', '')) for da_score in da_scores]\n",
    "    gold_lables = da_scores\n",
    "elif 'WMT22_MQM' in trans_analysed_file:\n",
    "    with open(\"../data/wmt-qe-2022-data/test_data-gold_labels/task1_mqm/en-de/test.2022.en-de.mqm_z_score\", 'r') as f:\n",
    "        mqm_scores = f.readlines()\n",
    "        mqm_scores = [float(mqm_score.replace('\\n', '')) for mqm_score in mqm_scores]\n",
    "    gold_lables = mqm_scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c3d69",
   "metadata": {},
   "source": [
    "Evaluation on gold labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d08bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in approximations.columns:\n",
    "    print(f\"-----------------{col}-----------------\")\n",
    "    print(pearsonr(gold_lables, approximations[col].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17193a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in approximations.columns:\n",
    "    plot_df = pd.DataFrame({'true': gold_lables, 'pred': approximations[col].values})\n",
    "#     plot_df = plot_df.sort_values('pred')\n",
    "    \n",
    "    X = plot_df['pred']\n",
    "    Y = plot_df['true']\n",
    "    \n",
    "    plt.figure()\n",
    "    hist = plt.hist(Y, bins=20)\n",
    "    bin_boundaries = hist[1]\n",
    "    \n",
    "#     # Remove bins with too few samples\n",
    "#     cut_point = 99999\n",
    "#     for i, value in enumerate(hist[0]):\n",
    "#         if value < 5:\n",
    "#             cut_point = i\n",
    "#             break\n",
    "\n",
    "#     bin_boundaries = bin_boundaries[:cut_point]\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    x_plot = [(bin_boundaries[i] + bin_boundaries[i+1])/2 for i in range(0, len(bin_boundaries)-1)]\n",
    "    y_plot = [stats.trim_mean(Y[(bin_boundaries[i] < X) & (X < bin_boundaries[i+1])], 0.1) for i in range(0, len(bin_boundaries)-1)]\n",
    "    plt.plot(x_plot, y_plot)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('gold_lables')\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_analysed_file = \"tmp_storages/analyse_WMT21_DA_dev_en2de_MultiplePerSentence_allWords.pkl\"\n",
    "output = pd.read_pickle(trans_analysed_file)\n",
    "\n",
    "# original_src_errornous_idxs = [17, 122, 306, 817, 908, 940]\n",
    "\n",
    "\n",
    "def fix_tokenization(tokenized_sentence, dataset):\n",
    "    # Only for WMT21_DA_en2de data\n",
    "    # Some of the sentences is tokenized differently in the labeled data. I.e., the last dot is not tokenized\n",
    "    # Fix in order to syncronize with the labeled data\n",
    "    if tokenized_sentence[-1] != '.':\n",
    "        str_sentence = ' '.join(tokenized_sentence)\n",
    "        str_sentence = str_sentence[:-1] + ' .'\n",
    "        return str_sentence.split()\n",
    "    else:\n",
    "        return tokenized_sentence\n",
    "\n",
    "# output['tokenized_SRC'] = output.apply(\n",
    "#                 lambda x: fix_tokenization(\n",
    "#                     x['tokenized_SRC']\n",
    "#                 ) if x['SRC_original_idx'] in original_src_errornous_idxs else x['tokenized_SRC'],\n",
    "#                 axis=1\n",
    "#             )\n",
    "\n",
    "\n",
    "toks = output.groupby('SRC_original_idx').first()['tokenized_SRC'].tolist()\n",
    "toks = [' '.join(tok) for tok in toks]\n",
    "\n",
    "with open('/Users/tuanh/Desktop/tmp.txt', 'w') as f:\n",
    "    for x in toks:\n",
    "        f.writelines(x + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020e200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba597cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
