{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed052ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import random\n",
    "from difflib import SequenceMatcher\n",
    "from scipy import stats\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import edist.sed as sed\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42568741",
   "metadata": {},
   "source": [
    "### Align different translations, find which words affect the translation of which words\n",
    "\n",
    "\n",
    "**Note**: can use [sequence alignments](https://stackoverflow.com/questions/5055839/word-level-edit-distance-of-a-sentence) to align the sentences on the target side only. ([code](https://gist.github.com/slowkow/06c6dba9180d013dfd82bec217d22eb5))\n",
    "\n",
    "Pros: could be easier than SRC-TGT alignment\n",
    "\n",
    "Cons: in the case where more output different sentence structure yet same meaning. <br>\n",
    "E.g., \"Today I think the cat is nice\" -- \"I think the cat is nice today\"\n",
    "SRC-TGT alignment would probably see these as the same, but edit distance cannot, bc it only has del, insert, substitute operations.\n",
    "\n",
    "\n",
    "Provided in functions `analyse_single_sentence_single_perturbed_word()` and  `analyse_single_sentence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a767f2",
   "metadata": {},
   "source": [
    "# Quality analysis\n",
    "\n",
    "- Have to use WMT21 data, bc models for 2021 is available. Also they have clear evaluation script\n",
    "- Have to do some manual fix so that the translation tokenization match completely with the tokenization of the labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d1aad",
   "metadata": {},
   "source": [
    "### Word-level\n",
    "\n",
    "- A translated word is uncertain if changing other words in the SRC sentence affect its translations. The assumption is the the translation of one word should only depends on a few others word, but not too many.\n",
    "    - E.g., My mother, who had a difficult childhood, is a great doctor.\n",
    "    - The gender form of \"doctor\" should only change if we change the word \"mother\".\n",
    "\n",
    "- Hyperparam: The number of SRC words that effect the translations of the target word\n",
    "    - Currently: If a translated word has 3 or more effecting SRC word, mark as \"BAD\"\n",
    "\n",
    "- Metrics: Matthews correlation coefficient \n",
    "    \"It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\"\n",
    "    \n",
    "- Unsupervised baseline: since our approach is based on the uncertainty of a translated word, the baseline could be the word-level log probablity generated by the NMT model itself. (i.e., word with low certainty --> BAD)\n",
    "\n",
    "Current score: 0.2825\n",
    "\n",
    "Score of WMT21 shared task: baseline 0.370, best 0.510\n",
    "\n",
    "WMT21 baseline: multilingual transformer-based Predictor-Estimator approach for both sentence level and word level\n",
    "\n",
    "\n",
    "**How to do hyperparameter tuning if our goal is not to use the training data? Or just using the dev set is oke?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733f83d",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Uses word-level log probablity generated by the NMT model itself. (i.e., word with low certainty --> BAD).\n",
    "\n",
    "Note that the NMT model output subwords and subword log probs, so for a word `A=a1a2` made of subwords `a1`, `a2`:\n",
    "\n",
    "prob(a1a2 | things before it, SRC) = prob(a1 | things before it, SRC) *  prob(a2 | a1, things before it, SRC) \n",
    "\n",
    "$$log(prob(A)) = log(prob(a1)) + log(prob(a2))$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Some times it output subwords that contains 2 tokens (i.e., the opposite of the above case). This does not happen very often\n",
    "\n",
    "Then we approximate:\n",
    "$$log(prob(a1)) = log(prob(a2)) = log(prob(A))/2$$\n",
    "\n",
    "**Is this correct???**\n",
    "\n",
    "\n",
    "\n",
    "Current best hyperparams setting on dev en-de:\n",
    "\n",
    "Our approach: hyperparams: \n",
    "- Perturbing [allTokens, allWords, allContentWords*]\n",
    "- effecting_words_thresholds: [1,2*,3,4]\n",
    "- consistence_trans_portion_thresholds: [0.6, 0.7, 0.8 0.85 0.9 0.95*]\n",
    "- uniques_portion_for_noiseORperturbed_thresholds: [0.35 0.4* 0.45, 0.6, 0.8]\n",
    "\n",
    "Nmt word probas:\n",
    "- Thresholds [0.4, 0.45*, 0.5, 0.55, 0.6]\n",
    "\n",
    "#### Word-level QE on translation\n",
    "\n",
    "- Our perturbation approach:\n",
    "    \n",
    "- Use nmt model word probas:\n",
    "    \n",
    "    \n",
    "    \n",
    "#### Word-level QE on SRC\n",
    "- Our perturbation approach:\n",
    "    \n",
    "- Use nmt model word probas:\n",
    "    \n",
    "    \n",
    "--> Worse than using nmt model word probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_and_analyse_ambiguous_trans import analyse_single_sentence_single_perturbed_word, analyse_single_sentence, align_translations, uniquify\n",
    "from tune_quality_estimation import nr_effecting_src_words_eval, nmt_log_prob_eval, flatten_list, load_gold_labels, replace_unknown, get_nmt_word_log_probs, get_nmt_word_log_probs_avg_perturbed\n",
    "\n",
    "dataset = 'WMT21_DA_test'\n",
    "data_root_path = '../data'\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'ja'\n",
    "\n",
    "beam = 5\n",
    "seed = 0\n",
    "replacement_strategy = 'masking_language_model'\n",
    "no_of_replacements = 30\n",
    "unmasking_model = \"roberta-base\"\n",
    "mask_type = f'MultiplePerSentence_allTokens'\n",
    "\n",
    "effecting_words_threshold = 4\n",
    "consistence_trans_portion_threshold = 0.95\n",
    "uniques_portion_for_noiseORperturbed_threshold = 0.8\n",
    "nmt_log_prob_threshold = 0.6\n",
    "task = 'trans_word_level_eval'\n",
    "perturbed_trans_df_path = f'../analyse_output/{dataset}_{src_lang}2{tgt_lang}/{mask_type}_{unmasking_model}/analyse_{dataset}_{src_lang}2{tgt_lang}_{mask_type}.pkl'\n",
    "original_translation_output_dir = f'../output/{dataset}_{src_lang}2{tgt_lang}/original'\n",
    "perturbed_translation_output_dir = f'../output/{dataset}_{src_lang}2{tgt_lang}/{replacement_strategy}_{unmasking_model}/beam{beam}_perturb{mask_type}/{no_of_replacements}replacements/seed{seed}'\n",
    "\n",
    "alignment_tool = 'Tercom'\n",
    "\n",
    "perturb_based_pred, perturb_based_details = nr_effecting_src_words_eval(perturbed_trans_df_path, task=task,\n",
    "                                                 effecting_words_threshold=effecting_words_threshold,\n",
    "                                                 consistence_trans_portion_threshold=consistence_trans_portion_threshold,\n",
    "                                                 uniques_portion_for_noiseORperturbed_threshold=uniques_portion_for_noiseORperturbed_threshold,\n",
    "                                                 return_details=True,\n",
    "                                                 alignment_tool=alignment_tool)\n",
    "nmt_prob_based_pred = nmt_log_prob_eval(dataset, data_root_path, src_lang, tgt_lang, nmt_log_prob_threshold, perturbed_trans_df_path, task,\n",
    "                                        original_translation_output_dir)\n",
    "gold_labels = load_gold_labels(dataset, data_root_path, src_lang, tgt_lang, task)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a8c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from read_and_analyse_df import read_output_df\n",
    "# output = read_output_df(df_root_path='../output', data_root_path=data_root_path,\n",
    "#                             dataset=f\"{dataset}_{src_lang}2{tgt_lang}\",\n",
    "#                             src_lang=src_lang, tgt_lang=tgt_lang, mask_type=mask_type,\n",
    "#                             beam=beam, replacement_strategy=replacement_strategy, ignore_case=False,\n",
    "#                             no_of_replacements=no_of_replacements, seed=seed,\n",
    "#                             spacy_model=None, w2v_model=None,\n",
    "#                             use_src_tgt_alignment=False, tokenize_sentences=True,\n",
    "#                             winoMT=False, analyse_feature=[\"edit_distance\", \"change_spread\"])\n",
    "\n",
    "# output.to_pickle(f'../analyse_output/WMT21_DA_dev_{src_lang}2{tgt_lang}/{mask_type}_{unmasking_model}/analyse_{dataset}_{src_lang}2{tgt_lang}_MultiplePerSentence_content.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb16b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "def report_metrics(pred, gold_labels): \n",
    "    labels = ['OK', 'BAD']\n",
    "    recall = recall_score(flatten_list(gold_labels), \n",
    "                          flatten_list(pred), labels=labels, pos_label='BAD')\n",
    "    precision = precision_score(flatten_list(gold_labels), \n",
    "                                flatten_list(pred), labels=labels, pos_label='BAD')\n",
    "    f1_bad = f1_score(flatten_list(gold_labels), \n",
    "                  flatten_list(pred), labels=labels, pos_label='BAD')\n",
    "    f1_ok = f1_score(flatten_list(gold_labels), \n",
    "                  flatten_list(pred), labels=labels, pos_label='OK')\n",
    "\n",
    "    print(f\"Recall wrt BAD: {recall}\")\n",
    "    print(f\"Precision wrt BAD: {precision}\")\n",
    "    print(f\"F1 wrt BAD: {f1_bad}\")\n",
    "    print(f\"F1 wrt OK: {f1_ok}\")\n",
    "    print(f\"Matthews_corrcoef: {matthews_corrcoef(flatten_list(gold_labels), flatten_list(pred))}\")\n",
    "\n",
    "    print(f\"Percentage of unknown labels: {accuracy_score(flatten_list(pred), ['unknown']*len(flatten_list(gold_labels)))*100}\")\n",
    "\n",
    "    dist = ConfusionMatrixDisplay(confusion_matrix(flatten_list(gold_labels), \n",
    "                                                   flatten_list(pred), labels=labels),\n",
    "                                 display_labels=labels)\n",
    "    dist.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(pred=nmt_prob_based_pred, gold_labels=gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff094c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(pred=perturb_based_pred, gold_labels=gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c9ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b49868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b9d3c28",
   "metadata": {},
   "source": [
    "Looking into the percentage of same predictions between `perturb_based_pred` and `nmt_prob_based_pred` to see if our approach is actually any different.\n",
    "\n",
    "0.7021 --> actually detecting the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(flatten_list(gold_labels), flatten_list(perturb_based_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb9665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18eeb275",
   "metadata": {},
   "source": [
    "Combinining the two classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16379a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersec_pred = [[\n",
    "    'BAD' if i=='BAD' and j=='BAD' else\n",
    "    'OK' if i=='OK' and j=='OK' else\n",
    "    'OK' for i,j in zip(x,y)] \n",
    "    for x, y in zip(perturb_based_pred, nmt_prob_based_pred)]\n",
    " \n",
    "report_metrics(pred=intersec_pred, gold_labels=gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6389884",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_pred = [[\n",
    "    'BAD' if i=='BAD' and j=='BAD' else\n",
    "    'OK' if i=='OK' and j=='OK' else\n",
    "    'BAD' for i,j in zip(x,y)] \n",
    "    for x, y in zip(perturb_based_pred, nmt_prob_based_pred)]\n",
    " \n",
    "report_metrics(pred=union_pred, gold_labels=gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec75811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8489a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d232d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158b835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_trans_df = pd.read_pickle(perturbed_trans_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e03896",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_trans_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96dabd8",
   "metadata": {},
   "source": [
    "Use our method to output a confidence score instead of binary OK/BAD. Result not as good as the current setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pred = []\n",
    "for sentence_i in range(len(perturb_based_details)):\n",
    "    prob_pred_per_sentence = []\n",
    "    for word_i in range(len(perturb_based_details[sentence_i])):\n",
    "        sentence_df = perturbed_trans_df[perturbed_trans_df['SRC_original_idx'] == sentence_i]\n",
    "        original_src_length = len(sentence_df['tokenized_SRC'].values[0])\n",
    "        \n",
    "        nr_effecting_src = len(perturb_based_details[sentence_i][word_i]['effecting_words'])\n",
    "        nr_no_effecting_src = len(perturb_based_details[sentence_i][word_i]['no_effecting_words'])\n",
    "        \n",
    "        prob = (nr_no_effecting_src - nr_effecting_src) / (original_src_length - 1)\n",
    "        \n",
    "        prob_pred_per_sentence.append(prob)\n",
    "    prob_pred.append(prob_pred_per_sentence)\n",
    "        \n",
    "label_pred = [['BAD' if x < 0.3 else 'OK' for x in y] for y in prob_pred]\n",
    "\n",
    "report_metrics(pred=label_pred, gold_labels=gold_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f4943",
   "metadata": {},
   "source": [
    "Ensemble (average) our confidence score with the nmt log prob score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad407a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_log_prob = get_nmt_word_log_probs(dataset, data_root_path, src_lang, tgt_lang, original_translation_output_dir)\n",
    "nmt_prob = [[2**x for x in y] for y in nmt_log_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f649726",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_prob = [[(x+y)/2 for x, y in zip(m,n)] for m,n in zip(prob_pred, nmt_prob)]\n",
    "ensemble_label = [['BAD' if x < 0.35 else 'OK' for x in y] for y in ensemble_prob]\n",
    "report_metrics(pred=ensemble_label, gold_labels=gold_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffba875",
   "metadata": {},
   "source": [
    "**Result**: \n",
    "\n",
    "- Confidence ensemble (this is already tuned to the test set):\n",
    "```\n",
    "Recall wrt BAD: 0.6180944755804644\n",
    "Precision wrt BAD: 0.32573839662447257\n",
    "F1 wrt BAD: 0.42663719259463945\n",
    "F1 wrt OK: 0.8348850163125647\n",
    "Matthews_corrcoef: 0.3053271505877078\n",
    "Percentage of unknown labels: 0.0\n",
    "```\n",
    "\n",
    "- Simple union ensemble (tuned to dev set):\n",
    "```\n",
    "Recall wrt BAD: 0.7065652522017614\n",
    "Precision wrt BAD: 0.29748862295634587\n",
    "F1 wrt BAD: 0.4186929189894437\n",
    "F1 wrt OK: 0.7952884173593417\n",
    "Matthews_corrcoef: 0.30143924152318785\n",
    "Percentage of unknown labels: 0.0\n",
    "```\n",
    "\n",
    "Confidence ensemble is a bit better but not significantly better.\n",
    "\n",
    "Also ensemble score is not significantly better than our approach alone:\n",
    "```\n",
    "Recall wrt BAD: 0.5720576461168935\n",
    "Precision wrt BAD: 0.3307104836843323\n",
    "F1 wrt BAD: 0.4191230385687051\n",
    "F1 wrt OK: 0.8449888467107579\n",
    "Matthews_corrcoef: 0.2946319823713003\n",
    "Percentage of unknown labels: 0.0\n",
    "```\n",
    "and it makes we looose the blackbox power --> skip ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610e41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f1a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f59d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e75a1a",
   "metadata": {},
   "source": [
    "### Average probs from the perturbations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb8c0033",
   "metadata": {},
   "source": [
    "avg_nmt_log_prob = get_nmt_word_log_probs_avg_perturbed(dataset, data_root_path, src_lang, tgt_lang, \n",
    "                                                        original_translation_output_dir, \n",
    "                                                        perturbed_translation_output_dir, \n",
    "                                                        perturbed_trans_df_path,\n",
    "                                                        alignment_tool='Tercom')\n",
    "avg_nmt_prob = [[2**x for x in y] for y in avg_nmt_log_prob]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30e58664",
   "metadata": {},
   "source": [
    "avg_nmt_label = [['BAD' if x < 0.5 else 'OK' for x in y] for y in avg_nmt_prob]\n",
    "report_metrics(pred=avg_nmt_label, gold_labels=gold_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bb84fc2",
   "metadata": {},
   "source": [
    "ensemble_prob = [[(x+y)/2 for x, y in zip(m,n)] for m,n in zip(prob_pred, avg_nmt_prob)]\n",
    "ensemble_label = [['BAD' if x < 0.35 else 'OK' for x in y] for y in ensemble_prob]\n",
    "report_metrics(pred=ensemble_label, gold_labels=gold_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3db28b04",
   "metadata": {},
   "source": [
    "accuracy_score(flatten_list(nmt_prob_based_pred), flatten_list(avg_nmt_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29550f",
   "metadata": {},
   "source": [
    "Not much difference from using only the prob of the original translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfce80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23e694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af4284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ff8180e",
   "metadata": {},
   "source": [
    "### Analyse some specific samples"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19d56ca6",
   "metadata": {},
   "source": [
    "from read_and_analyse_df import read_output_df\n",
    "from align_and_analyse_ambiguous_trans import analyse_single_sentence_single_perturbed_word, analyse_single_sentence, align_translations, uniquify\n",
    "from quality_estimation import nr_effecting_src_words_eval, nmt_log_prob_eval, flatten_list, load_gold_labels, replace_unknown, get_nmt_word_log_probs, get_nmt_word_log_probs_avg_perturbed\n",
    "\n",
    "dataset = 'cherry-picked'\n",
    "data_root_path = '../data'\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'de'\n",
    "\n",
    "beam = 5\n",
    "seed = 0\n",
    "no_of_replacements = 30\n",
    "unmasking_model = \"bert-base-cased\"\n",
    "replacement_strategy = f'masking_language_model_{unmasking_model}'\n",
    "mask_type = f'MultiplePerSentence_content'\n",
    "\n",
    "effecting_words_threshold = 2\n",
    "consistence_trans_portion_threshold = 0.95\n",
    "uniques_portion_for_noiseORperturbed_threshold = 0.45\n",
    "nmt_log_prob_threshold = 0.45\n",
    "# task = 'trans_word_level_eval'\n",
    "# perturbed_trans_df_path = f'../analyse_output/{dataset}_{src_lang}2{tgt_lang}/{mask_type}_{unmasking_model}/analyse_{dataset}_{src_lang}2{tgt_lang}_MultiplePerSentence_content.pkl'\n",
    "# original_translation_output_dir = f'../output/{dataset}_{src_lang}2{tgt_lang}/original'\n",
    "# perturbed_translation_output_dir = f'../output/{dataset}_{src_lang}2{tgt_lang}/{replacement_strategy}_{unmasking_model}/beam{beam}_perturb{mask_type}/{no_of_replacements}replacements/seed{seed}'\n",
    "\n",
    "alignment_tool = 'Tercom'\n",
    "\n",
    "perturbed_trans_df = read_output_df(df_root_path='../output', data_root_path=data_root_path,\n",
    "                            dataset=f\"{dataset}_{src_lang}2{tgt_lang}\",\n",
    "                            src_lang=src_lang, tgt_lang=tgt_lang, mask_type=mask_type,\n",
    "                            beam=beam, replacement_strategy=replacement_strategy, ignore_case=False,\n",
    "                            no_of_replacements=no_of_replacements, seed=seed,\n",
    "                            spacy_model=None, w2v_model=None,\n",
    "                            use_src_tgt_alignment=False, tokenize_sentences=True,\n",
    "                            winoMT=False, analyse_feature=[\"edit_distance\", \"change_spread\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7879d6f3",
   "metadata": {},
   "source": [
    "sentence_idx = 1\n",
    "sentence_df = perturbed_trans_df[perturbed_trans_df['SRC_original_idx'] == sentence_idx]\n",
    "original_SRC = sentence_df['SRC'].values[0]\n",
    "original_translation = sentence_df['SRC-Trans'].values[0]\n",
    "tok_original_translation = sentence_df['tokenized_SRC-Trans'].values[0]\n",
    "tok_original_SRC = sentence_df['tokenized_SRC'].values[0]\n",
    "word_idx = 2\n",
    "word = tok_original_translation[word_idx]\n",
    "align_type = \"trans-only\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4fd3276",
   "metadata": {},
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "print(f\"Original SRC sentence:\\n{original_SRC}\")\n",
    "print(f\"Original trans:\\n{original_translation}\")\n",
    "print(f\"BAD word: {word}\")\n",
    "print()\n",
    "\n",
    "pprint.pprint(analyse_single_sentence(\n",
    "    sentence_df, align_type=align_type, return_word_index=False,\n",
    "    consistence_trans_portion_threshold=consistence_trans_portion_threshold,\n",
    "    uniques_portion_for_noiseORperturbed_threshold=uniques_portion_for_noiseORperturbed_threshold,\n",
    "    alignment_tool=alignment_tool\n",
    ")[word])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c192646",
   "metadata": {},
   "source": [
    "original_word = 'new'\n",
    "\n",
    "\n",
    "groups_by_perturbed_word = sentence_df.groupby(\"SRC_masked_index\", as_index=False)\n",
    "original_words = [group_by_perturbed_word.iloc[0]['original_word']\n",
    "                  for _, group_by_perturbed_word in groups_by_perturbed_word]\n",
    "groups_by_perturbed_word = [group_by_perturbed_word for _, group_by_perturbed_word in groups_by_perturbed_word]\n",
    "original_words = list(uniquify(original_words))\n",
    "\n",
    "\n",
    "sentence_single_perturbed_word_df = groups_by_perturbed_word[original_words.index(original_word)]\n",
    "\n",
    "\n",
    "pprint.pprint(analyse_single_sentence_single_perturbed_word(\n",
    "    sentence_single_perturbed_word_df, align_type=align_type,\n",
    "    consistence_trans_portion_threshold=consistence_trans_portion_threshold,\n",
    "    uniques_portion_for_noiseORperturbed_threshold=uniques_portion_for_noiseORperturbed_threshold,\n",
    "    alignment_tool=alignment_tool\n",
    "))\n",
    "align_translations(sentence_single_perturbed_word_df, align_type=align_type, alignment_tool=alignment_tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a690d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d74418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d58ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03435510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b30850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16628256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528955af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeba406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb62bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6857de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636aa05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9a09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b61ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15fe46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc0b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_trans_df = pd.read_pickle(perturbed_trans_df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = 1\n",
    "count = 0\n",
    "for sentence_idx in range(len(gold_labels)):\n",
    "    sentence_df = perturbed_trans_df[perturbed_trans_df['SRC_original_idx'] == sentence_idx]\n",
    "    original_SRC = sentence_df['SRC'].values[0]\n",
    "    original_translation = sentence_df['SRC-Trans'].values[0]\n",
    "    tok_original_translation = sentence_df['tokenized_SRC-Trans'].values[0]\n",
    "    tok_original_SRC = sentence_df['tokenized_SRC'].values[0]\n",
    "\n",
    "    my_pred = np.array(perturb_based_pred[sentence_idx])\n",
    "    nmt_pred = np.array(nmt_prob_based_pred[sentence_idx])\n",
    "    gold = np.array(gold_labels[sentence_idx])\n",
    "\n",
    "\n",
    "    # BAD words correctly predicted by perturb_based_pred\n",
    "    bad_word_indices = np.nonzero(np.logical_and(my_pred=='BAD', gold=='BAD'))\n",
    "\n",
    "#     # OK words predicted by perturb_based_pred as BAD\n",
    "#     bad_word_indices = np.nonzero(np.logical_and(my_pred=='BAD', gold=='OK'))\n",
    "    \n",
    "#     # BAD words predicted by perturb_based_pred as OK\n",
    "#     bad_word_indices = np.nonzero(np.logical_and(my_pred=='OK', gold=='BAD'))\n",
    "\n",
    "\n",
    "    # # BAD words correctly predicted by perturb_based_pred, but not by nmt_prob_based_pred\n",
    "    # bad_word_indices = np.nonzero(np.logical_and(np.logical_and(my_pred=='BAD', gold=='OK'), gold=='BAD'))\n",
    "    \n",
    "    if len(bad_word_indices[0]) > 0:\n",
    "        \n",
    "        print(len(bad_word_indices[0]))\n",
    "        count = count + 1\n",
    "        if count == stop:\n",
    "            word_idx = bad_word_indices[0][0]\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c491f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_idx = 332\n",
    "# sentence_df = perturbed_trans_df[perturbed_trans_df['SRC_original_idx'] == sentence_idx]\n",
    "# original_SRC = sentence_df['SRC'].values[0]\n",
    "# original_translation = sentence_df['SRC-Trans'].values[0]\n",
    "# tok_original_translation = sentence_df['tokenized_SRC-Trans'].values[0]\n",
    "# tok_original_SRC = sentence_df['tokenized_SRC'].values[0]\n",
    "# my_pred = np.array(perturb_based_pred[sentence_idx])\n",
    "# bad_word_indices = np.nonzero(my_pred=='BAD')\n",
    "# word_idx = bad_word_indices[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == 'src_word_level_eval':\n",
    "    word = tok_original_SRC[word_idx]\n",
    "    align_type = \"src-trans\"\n",
    "elif task == 'trans_word_level_eval':\n",
    "    word = tok_original_translation[word_idx]\n",
    "    align_type = \"trans-only\"\n",
    "else:\n",
    "    raise RuntimeError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "print(f\"Original SRC sentence:\\n{original_SRC}\")\n",
    "print(f\"Original trans:\\n{original_translation}\")\n",
    "print(f\"BAD word: {word}\")\n",
    "print()\n",
    "\n",
    "pprint.pprint(analyse_single_sentence(\n",
    "    sentence_df, align_type=align_type, return_word_index=False,\n",
    "    consistence_trans_portion_threshold=consistence_trans_portion_threshold,\n",
    "    uniques_portion_for_noiseORperturbed_threshold=uniques_portion_for_noiseORperturbed_threshold,\n",
    "    alignment_tool=alignment_tool\n",
    ")[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553dd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_word = 'December'\n",
    "\n",
    "\n",
    "groups_by_perturbed_word = sentence_df.groupby(\"SRC_masked_index\", as_index=False)\n",
    "original_words = [group_by_perturbed_word.iloc[0]['original_word']\n",
    "                  for _, group_by_perturbed_word in groups_by_perturbed_word]\n",
    "groups_by_perturbed_word = [group_by_perturbed_word for _, group_by_perturbed_word in groups_by_perturbed_word]\n",
    "original_words = list(uniquify(original_words))\n",
    "\n",
    "\n",
    "sentence_single_perturbed_word_df = groups_by_perturbed_word[original_words.index(original_word)]\n",
    "\n",
    "\n",
    "pprint.pprint(analyse_single_sentence_single_perturbed_word(\n",
    "    sentence_single_perturbed_word_df, align_type=align_type,\n",
    "    consistence_trans_portion_threshold=consistence_trans_portion_threshold,\n",
    "    uniques_portion_for_noiseORperturbed_threshold=0.8,\n",
    "    alignment_tool=alignment_tool\n",
    "))\n",
    "tmp_df = align_translations(sentence_single_perturbed_word_df, align_type=align_type, alignment_tool=alignment_tool)\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.iloc[[1,2,5,6,22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df[sentence_df['SRC_masked_index'] == 14].iloc[[1,2,5,6,22]]['SRC_perturbed-Trans'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91738ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79703d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1611250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc909f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8610e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07215d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dc168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b90273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_single_perturbed_word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd496c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16022fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b5f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5e3086e",
   "metadata": {},
   "source": [
    "### Finding patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65080ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bad_word_dict = defaultdict(lambda: {'freq': 0, 'sentence_idxs': [], 'effecting_SRC_words': []})  # bad_word: {'freq': freq, 'effecting_SRC_words': list}\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for sentence_idx in range(len(gold_labels)):\n",
    "    sentence_df = perturbed_trans_df[perturbed_trans_df['SRC_original_idx'] == sentence_idx]\n",
    "    tok_original_translation = sentence_df['tokenized_SRC-Trans'].values[0]\n",
    "    all_words.append(tok_original_translation)\n",
    "    \n",
    "for sentence_idx in range(len(gold_labels)):\n",
    "    for word_idx in range(len(gold_labels[sentence_idx])):\n",
    "        # Only considers the correctly predicted BAD ones\n",
    "        if perturb_based_pred[sentence_idx][word_idx] == 'BAD' and gold_labels[sentence_idx][word_idx] == 'BAD':\n",
    "            bad_word = all_words[sentence_idx][word_idx]\n",
    "            bad_word_dict[bad_word]['freq'] = bad_word_dict[bad_word]['freq'] + 1\n",
    "            bad_word_dict[bad_word]['sentence_idxs'].append(sentence_idx)\n",
    "            bad_word_dict[bad_word]['effecting_SRC_words'].extend(\n",
    "                perturb_based_details[sentence_idx][word_idx]['effecting_words']\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def list_to_frequency(a_list):\n",
    "    \"\"\" ['a', 'a', 'a', 'b', 'b'] --> {'a': 3, 'b': 2}\n",
    "    \"\"\"\n",
    "    return dict(Counter(a_list).most_common())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779686f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_word_df = pd.DataFrame(bad_word_dict).transpose().sort_values(by='freq', ascending=False)\n",
    "bad_word_df['effecting_SRC_words'] = bad_word_df['effecting_SRC_words'].apply(lambda x: list_to_frequency(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154213e",
   "metadata": {},
   "source": [
    "Filter out the functional words (only keep 'NOUN', 'VERB', 'ADJ', 'PRON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44844664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tgt_word_pos_tag(word, spacy_model):\n",
    "    doc = spacy_model(word)\n",
    "    return doc[0].pos_\n",
    "\n",
    "def is_content_tag(spacy_tag):\n",
    "    if spacy_tag in ['NOUN', 'VERB', 'ADJ', 'PRON']:\n",
    "        return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d42a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spacy_model = spacy.load(\"de_core_news_sm\")\n",
    "pattern_df = \\\n",
    "    bad_word_df[bad_word_df.index.to_series().apply(lambda x: is_content_tag(tgt_word_pos_tag(x, spacy_model)))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ed7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdf456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2cb11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd83395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d94548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea718be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924d189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167d9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e642f401",
   "metadata": {},
   "source": [
    "## Sentence level\n",
    "\n",
    "Approximations:\n",
    "- Negative corr with DA:\n",
    "    - Changes edit distance \n",
    "    - Changes edit distance / length\n",
    "    - Changes spread\n",
    "    - Changes spread / length\n",
    "    - Number of BAD tokens\n",
    "   \n",
    "Metrics: Pearson correlation coefficient: \"Correlations of -1 or +1 imply an exact linear relationship\"\n",
    "\n",
    "DA scores:\n",
    "Scores: highest 0.18\n",
    "WMT21 scores: baseline 0.403, best 0.584\n",
    "\n",
    "\n",
    "HTER scores: \n",
    "Scores: highest 0.28\n",
    "WMT21 scores: baseline 0.529, best 0.653\n",
    "\n",
    "**Can try to apply some function to the prediction, but then that's not unsupervised anymore**\n",
    "\n",
    "**Can use as feature for QE model, but again not unsupervised anymore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "approximations = output[\n",
    "    [\"SRC_original_idx\", \n",
    "     \"Trans-edit_distance\", \n",
    "     \"#TransChanges/SentenceLength\",\n",
    "     \"ChangesSpread\",\n",
    "     \"ChangesSpread/SentenceLength\"\n",
    "    ]\n",
    "].groupby(\"SRC_original_idx\").mean()\n",
    "\n",
    "approximations['word_level_agg'] = [x.count('BAD') for x in word_tag]\n",
    "\n",
    "\n",
    "for col in approximations.columns:\n",
    "    # Normalize the apporximations, invert the sign\n",
    "    approximations[col] = -approximations[col]\n",
    "    approximations[col] = zscore(approximations[col].values)\n",
    "    \n",
    "    \n",
    "approximations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29433315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_analysed_file = \"analyse_WMT22_MQM_en2de.pkl\"\n",
    "# output = pd.read_pickle(trans_analysed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ecd4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ccb08e1",
   "metadata": {},
   "source": [
    "Gold labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019f6cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'WMT21_DA' in trans_analysed_file:\n",
    "    with open(\"../data/wmt-qe-2021-data/en-de-test21/goldlabels/test21.hter\", 'r') as f:\n",
    "        da_scores = f.readlines()\n",
    "        da_scores = [float(da_score.replace('\\n', '')) for da_score in da_scores]\n",
    "    gold_lables = da_scores\n",
    "elif 'WMT22_MQM' in trans_analysed_file:\n",
    "    with open(\"../data/wmt-qe-2022-data/test_data-gold_labels/task1_mqm/en-de/test.2022.en-de.mqm_z_score\", 'r') as f:\n",
    "        mqm_scores = f.readlines()\n",
    "        mqm_scores = [float(mqm_score.replace('\\n', '')) for mqm_score in mqm_scores]\n",
    "    gold_lables = mqm_scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c3d69",
   "metadata": {},
   "source": [
    "Evaluation on gold labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d08bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in approximations.columns:\n",
    "    print(f\"-----------------{col}-----------------\")\n",
    "    print(pearsonr(gold_lables, approximations[col].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17193a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in approximations.columns:\n",
    "    plot_df = pd.DataFrame({'true': gold_lables, 'pred': approximations[col].values})\n",
    "#     plot_df = plot_df.sort_values('pred')\n",
    "    \n",
    "    X = plot_df['pred']\n",
    "    Y = plot_df['true']\n",
    "    \n",
    "    plt.figure()\n",
    "    hist = plt.hist(Y, bins=20)\n",
    "    bin_boundaries = hist[1]\n",
    "    \n",
    "#     # Remove bins with too few samples\n",
    "#     cut_point = 99999\n",
    "#     for i, value in enumerate(hist[0]):\n",
    "#         if value < 5:\n",
    "#             cut_point = i\n",
    "#             break\n",
    "\n",
    "#     bin_boundaries = bin_boundaries[:cut_point]\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    x_plot = [(bin_boundaries[i] + bin_boundaries[i+1])/2 for i in range(0, len(bin_boundaries)-1)]\n",
    "    y_plot = [stats.trim_mean(Y[(bin_boundaries[i] < X) & (X < bin_boundaries[i+1])], 0.1) for i in range(0, len(bin_boundaries)-1)]\n",
    "    plt.plot(x_plot, y_plot)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('gold_lables')\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68997af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654316dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2eb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed5324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4b5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_analysed_file = \"tmp_storages/analyse_WMT21_DA_dev_en2de_MultiplePerSentence_allWords.pkl\"\n",
    "output = pd.read_pickle(trans_analysed_file)\n",
    "\n",
    "# original_src_errornous_idxs = [17, 122, 306, 817, 908, 940]\n",
    "\n",
    "\n",
    "def fix_tokenization(tokenized_sentence, dataset):\n",
    "    # Only for WMT21_DA_en2de data\n",
    "    # Some of the sentences is tokenized differently in the labeled data. I.e., the last dot is not tokenized\n",
    "    # Fix in order to syncronize with the labeled data\n",
    "    if tokenized_sentence[-1] != '.':\n",
    "        str_sentence = ' '.join(tokenized_sentence)\n",
    "        str_sentence = str_sentence[:-1] + ' .'\n",
    "        return str_sentence.split()\n",
    "    else:\n",
    "        return tokenized_sentence\n",
    "\n",
    "# output['tokenized_SRC'] = output.apply(\n",
    "#                 lambda x: fix_tokenization(\n",
    "#                     x['tokenized_SRC']\n",
    "#                 ) if x['SRC_original_idx'] in original_src_errornous_idxs else x['tokenized_SRC'],\n",
    "#                 axis=1\n",
    "#             )\n",
    "\n",
    "\n",
    "toks = output.groupby('SRC_original_idx').first()['tokenized_SRC'].tolist()\n",
    "toks = [' '.join(tok) for tok in toks]\n",
    "\n",
    "with open('/Users/tuanh/Desktop/tmp.txt', 'w') as f:\n",
    "    for x in toks:\n",
    "        f.writelines(x + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020e200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba597cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc861e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
