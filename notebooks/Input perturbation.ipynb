{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03046c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import random\n",
    "from difflib import SequenceMatcher\n",
    "from scipy import stats\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import edist.sed as sed\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from read_and_analyse_df import read_output_df\n",
    "from align_and_analyse_ambiguous_trans import align_translations, analyse_single_sentence_single_perturbed_word, analyse_single_sentence\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b86d1e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize everything ...\n",
      "Original df shape: (426720, 15)\n",
      "After dropping none-perturbed sentences: (426720, 15)\n",
      "Calculating the changes between translations of original SRC and perturbed SRC ...\n",
      "Highlighting the changes ...\n",
      "Calculating the edit distance ...\n"
     ]
    }
   ],
   "source": [
    "mask_type = 'MultiplePerSentence_allWords'\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'de'\n",
    "dataset = f'WMT21_DA_dev_{src_lang}2{tgt_lang}'  # 'MuST-SHE-en2fr' 'IWSLT15-en2vi' 'wmt19-newstest2019-en2de'\n",
    "beam = 5\n",
    "replacement_strategy = 'masking_language_model'\n",
    "no_of_replacements = 30\n",
    "ignore_case = False  # Only Europarls needs ignore case\n",
    "chunk_max_length=1\n",
    "spacy_model = spacy.load(\"de_core_news_sm\")\n",
    "# Loading these models in is time consuming\n",
    "# de_model = load_facebook_model(\"../data/cc.de.300.bin\").wv\n",
    "# vi_model = load_facebook_model(\"../data/cc.vi.300.bin\").wv\n",
    "winoMT = False\n",
    "\n",
    "\n",
    "if winoMT:\n",
    "    perturb_type = 'pronoun'\n",
    "    no_of_replacements = 1\n",
    "\n",
    "output = read_output_df(df_root_path='../output', dataset=dataset, src_lang=src_lang, tgt_lang=tgt_lang, mask_type=mask_type, \n",
    "                        beam=beam, replacement_strategy=replacement_strategy, ignore_case=ignore_case,\n",
    "                        no_of_replacements=no_of_replacements, chunk_max_length=chunk_max_length,\n",
    "                        spacy_model=spacy_model, w2v_model=None, use_src_tgt_alignment=False, \n",
    "                        winoMT=winoMT, tokenize_sentences=True, \n",
    "                        analyse_feature=[\n",
    "                                    'highlight_changes',   # Highlight the changes in the translation in capital\n",
    "                                    'edit_distance',\n",
    "                                    'change_spread',       # Longest distance between 2 changes\n",
    "                                    ])\n",
    "\n",
    "# print('BLEU score: ')\n",
    "# sacrebleu.corpus_bleu(output['SRC-Trans'].tolist(), [output['REF'].tolist()]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd93d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_pickle(f'tmp_storages/analyse_{dataset}_{mask_type}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048b3d1",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "- On `wmt19-newstest2019-en2de, chunk_max_length=2`\n",
    "    - 902: change to 1 SRC word leads to fixed changes of an irrelevant word\n",
    "    - In many cases, the form of the verb (e.g., current or past tense) are changed --> harmful in the sense that it hurt performance score?\n",
    "    - Word not being translated \n",
    "    - Spoken/written style\n",
    "    - Time\n",
    "    \n",
    "    \n",
    "- On `IWSLT15-en2vi, adjective`\n",
    "    - 1003: change of 1 words consistently leads to change in subject\n",
    "    \n",
    "    - 1003, 145, 990 noun: same\n",
    "    - 236 noun: same, funny but not sure if it is wrong\n",
    "    - 308 verb same \n",
    "    \n",
    "--> Quantify the verb form change by stemming/lemmatization\n",
    "    \n",
    "Chúng, họ, gã, cô ấy, cô ta, anh ta, hắn\n",
    "\n",
    "Changes in the word \"you\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442d9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output[output['#TransChanges-#SrcChanges'] > 10].head(5)\n",
    "# output[output[\"ChangesSpread/SentenceLength\"] > 0.85].head(20)\n",
    "\n",
    "\n",
    "\n",
    "# Two chunks changed that consistently changed over the different replacement of a word\n",
    "\n",
    "\n",
    "# output[(output[\"TwoChunksChanged\"] == True) & (output[\"TwoChunksChanged--total\"] == 5)].sort_values(by='ChunkDistance', axis=0, ascending=False).head(1)\n",
    "# output[(output[\"TwoChunksChanged\"] == True)].sort_values(by='ChunkDistance', axis=0, ascending=False).head(100)\n",
    "\n",
    "# Two words changed that are not in the same subtree\n",
    "# output[(output[\"TwoChunksChanged\"] == True) & (output[\"is_same_subtree\"] == False) & (output[\"TwoChunksChanged--total\"] == 5)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IWSLT15-en2vi, noun\n",
    "# output.loc[[1003, 145, 990, 236]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976bde7",
   "metadata": {},
   "source": [
    "Sort the samples by the least similarity in changed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the 2-word-changed cases and similarity can be calculated\n",
    "def get_not_perturbed_change_similarity(changes):\n",
    "    for change in changes:\n",
    "        if change['change_type'] == 'not_perturbed':\n",
    "            return change['semantic_similarity']\n",
    "    return pd.NA\n",
    "\n",
    "analyse_df = output[\n",
    "    (output[\"TwoChunksChanged\"] == True) & output['changes_similarity'].notna() & output['not_perturbed_TGT_change_type'].isin(['NOUN', 'VERB', 'ADJ', 'PRON'])\n",
    "]\n",
    "analyse_df['similarity_not_perturbed'] = analyse_df['changes_similarity'].apply(\n",
    "    lambda x: get_not_perturbed_change_similarity(x)\n",
    ")\n",
    "analyse_df.sort_values(by='similarity_not_perturbed')[['SRC', \n",
    "                                                f'original_word', \n",
    "                                                f'perturbed_word',\n",
    "                                                'SRC-Trans',\n",
    "                                                f'SRC_perturbed-Trans',\n",
    "                                                'ChunkDistance',\n",
    "                                                'changes_similarity',\n",
    "                                                'similarity_not_perturbed',\n",
    "                                                'not_perturbed_TGT_change_type',\n",
    "#                                                 'Bias_sample'\n",
    "                                                      ]].head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c53bf",
   "metadata": {},
   "source": [
    "### Calculate metrics for detecting the bias samples\n",
    "\n",
    "High precision --> higher chance that the returned samples are bias --> save human time\n",
    "\n",
    "High recall --> more bias samples are retreat --> can detect more type of bias\n",
    "\n",
    "We focus on precision then (save human cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(' -------------------- Most-changes filter -------------------- ')\n",
    "q = 20  # Take the q% sentences with the highest changes\n",
    "no_changes_thresthold = np.percentile(output['#TransChanges-#SrcChanges'], 100-q)\n",
    "bias_prediction = output['#TransChanges-#SrcChanges'] > no_changes_thresthold\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "print(' -------------------- Most-spreaded_changes filter -------------------- ')\n",
    "q = 20  # Take the q% sentences with the highest spread\n",
    "spread_thresthold = np.percentile(output['ChangesSpread/SentenceLength'], 100-q)\n",
    "bias_prediction = output['ChangesSpread/SentenceLength'] > spread_thresthold\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "print(' -------------------- Two-changes filter -------------------- ')\n",
    "bias_prediction = output[\"TwoChunksChanged\"]\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "\n",
    "print(' -------------------- Two-faraway-changes filter -------------------- ')\n",
    "q = 20  # Take the q% sentences with the furthest distance between 2 changes \n",
    "distance_thresthold = np.nanpercentile(output['ChunkDistance'], 100-q)\n",
    "bias_prediction = output[\"TwoChunksChanged\"] & (output['ChunkDistance'] > distance_thresthold)\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "print(' -------------------- Two-changes-different-subtree filter -------------------- ')\n",
    "bias_prediction = output[\"TwoChunksChanged\"] & (output[\"is_same_subtree\"] == False)\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)\n",
    "\n",
    "\n",
    "print(' -------------------- Two-change-dissimilar filter -------------------- ')\n",
    "q = 90  # Take the q% sentences with the lowest similarity of the not-perturbed change\n",
    "output = output.join(analyse_df['similarity_not_perturbed'])\n",
    "similiarity_threshold = np.nanpercentile(output['similarity_not_perturbed'], q)\n",
    "\n",
    "bias_prediction = output[\"TwoChunksChanged\"] & (output['similarity_not_perturbed'] < similiarity_threshold)\n",
    "results = classification_report(\n",
    "    y_true=output['Bias_sample'], y_pred=bias_prediction, \n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ab5c4",
   "metadata": {},
   "source": [
    "# Analyse on same original_word accross sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[[\n",
    "    'SRC_masked_index', 'SRC', 'original_word', 'perturbed_word', 'SRC_perturbed',\n",
    "    'SRC-Trans', 'SRC_perturbed-Trans', '#TransChanges-#SrcChanges',\n",
    "    '#TransChanges-#SrcChanges/SentenceLength',\n",
    "    'ChangesSpread/SentenceLength', 'TwoChunksChanged', 'ChunkDistance',\n",
    "    'is_same_subtree', 'changes_similarity', 'perturbed_trans_alignment',\n",
    "    'not_perturbed_TGT_change_type', 'Trans-edit_distance--SD',\n",
    "    '#TransChanges-#SrcChanges--SD', 'TwoChunksChanged--total'\n",
    "]].groupby('original_word').mean().head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb9c49",
   "metadata": {},
   "source": [
    "### Most changes filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupped_by_word = output.groupby('original_word').mean()\n",
    "\n",
    "q = 10  # Take the q% groups with the highest changes\n",
    "no_changes_thresthold = np.percentile(groupped_by_word['#TransChanges-#SrcChanges'], 100-q)\n",
    "bias_prediction = groupped_by_word['#TransChanges-#SrcChanges'] > no_changes_thresthold\n",
    "\n",
    "bias_word_predicted = groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['#TransChanges-#SrcChanges'] > no_changes_thresthold)\n",
    "].head(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf0826",
   "metadata": {},
   "source": [
    "### Most-spreaded_changes filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e45372",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupped_by_word = output.groupby('original_word').mean()\n",
    "\n",
    "q = 10  # Take the q% sentences with the highest spread\n",
    "spread_thresthold = np.percentile(groupped_by_word['ChangesSpread/SentenceLength'], 100-q)\n",
    "bias_prediction = groupped_by_word['ChangesSpread/SentenceLength'] > spread_thresthold\n",
    "\n",
    "bias_word_predicted = groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['ChangesSpread/SentenceLength'] > spread_thresthold)\n",
    "].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069aeb58",
   "metadata": {},
   "source": [
    "### Two-faraway-changes filter\n",
    "\n",
    "ACTUALLY two-changes is not a bias filter. It's just an auxilary filter to avoid paraphrasing cases. Using this we will miss out on the cases where the model has both paraphrasing and \n",
    "\n",
    "Here we consider in each group: the number of sentences that has 2 changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_change_only_groupped_by_word = output[output[\"TwoChunksChanged\"]].groupby('original_word').mean()\n",
    "\n",
    "\n",
    "q = 20  # Take the q% sentences with the furthest distance between 2 changes \n",
    "distance_thresthold = np.percentile(two_change_only_groupped_by_word['ChunkDistance'], 100-q)\n",
    "bias_prediction = two_change_only_groupped_by_word['ChunkDistance'] > distance_thresthold\n",
    "\n",
    "\n",
    "bias_word_predicted = two_change_only_groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output[\"TwoChunksChanged\"] & \\\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['ChunkDistance'] > distance_thresthold)\n",
    "].head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cd5f9",
   "metadata": {},
   "source": [
    "### Two-changes-different-subtree filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876de271",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = output[output[\"TwoChunksChanged\"] & output['is_same_subtree'].notna()]\n",
    "tmp['not_same_subtree'] = 1 - tmp['is_same_subtree'].astype(int)\n",
    "two_change_only_groupped_by_word = tmp.groupby('original_word').sum()\n",
    "\n",
    "q = 20  # Take the q% groups with the highest number of different subtree changes\n",
    "count_thresthold = np.percentile(two_change_only_groupped_by_word['not_same_subtree'], 100-q)\n",
    "bias_prediction = two_change_only_groupped_by_word['ChunkDistance'] > count_thresthold\n",
    "\n",
    "\n",
    "bias_word_predicted = two_change_only_groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output[\"TwoChunksChanged\"] & \\\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['is_same_subtree'] == 0)\n",
    "].head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a02578",
   "metadata": {},
   "source": [
    "### Two-change-dissimilar filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ee3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.join(analyse_df['similarity_not_perturbed'])\n",
    "two_change_only_groupped_by_word = output[output[\"TwoChunksChanged\"]].groupby('original_word').mean()\n",
    "\n",
    "\n",
    "q = 20  # Take the q% sentences with the lowest similarity of the not-perturbed change\n",
    "similiarity_threshold = np.nanpercentile(two_change_only_groupped_by_word['similarity_not_perturbed'], q)\n",
    "bias_prediction = two_change_only_groupped_by_word['similarity_not_perturbed'] < similiarity_threshold\n",
    "\n",
    "\n",
    "bias_word_predicted = two_change_only_groupped_by_word[bias_prediction].index.values\n",
    "\n",
    "output[\n",
    "    output[\"TwoChunksChanged\"] & \\\n",
    "    output['original_word'].isin(bias_word_predicted) & \\\n",
    "    (output['similarity_not_perturbed'] < similiarity_threshold)\n",
    "].head(2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52f9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11449a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6de148",
   "metadata": {},
   "source": [
    "## Find patterns\n",
    "\n",
    "when a word A is replaced with B, then the change C happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4534859",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[['SRC_masked_index', 'SRC', 'original_word', 'perturbed_word', 'SRC_perturbed',\n",
    "       'SRC-Trans', 'SRC_perturbed-Trans', 'changes']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260159f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def lower_remove_non_alphabet(input_str):\n",
    "    translation = input_str.maketrans(dict.fromkeys(string.punctuation, ' '))\n",
    "    return input_str.translate(translation).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ad595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_due_to_perturbation(change, original_word, perturbed_word, \n",
    "                           perturbed_trans_alignment_dict, original_trans_alignment_dict):\n",
    "    \"\"\"\n",
    "    A change in translation is directly due to perturbation if the (aligned) translation of perturbed_word\n",
    "    is in changed_part AND the (aligned) translation of original_word is in original_part\n",
    "    \n",
    "    Params:\n",
    "        change: tuple of (change_type, original_trans_part, changed_trans_part)\n",
    "        original_word: original word in the SRC that was perturbed\n",
    "        perturbed_word: the replacement of the original word\n",
    "        perturbed_trans_alignment_dict: {src_word1:trans_word1, src_word2:trans_word2,...} of the perturbed trans\n",
    "        original_trans_alignment_dict: {src_word1:trans_word1, src_word2:trans_word2,...} of the original trans\n",
    "    Return: bool, pd.NA in failed alignment case\n",
    "    \"\"\"\n",
    "    # Turn everything to lowercase, and remove any non-alphabet characters\n",
    "    change_type, original_trans_part, changed_trans_part = \\\n",
    "        change[0], lower_remove_non_alphabet(change[1]), lower_remove_non_alphabet(change[2])\n",
    "    perturbed_trans_alignment_dict = dict(\n",
    "        (lower_remove_non_alphabet(k).replace(' ', ''), lower_remove_non_alphabet(v).replace(' ', '')) for k,v in perturbed_trans_alignment_dict.items()\n",
    "    )\n",
    "    original_trans_alignment_dict = dict(\n",
    "        (lower_remove_non_alphabet(k).replace(' ', ''), lower_remove_non_alphabet(v).replace(' ', '')) for k,v in original_trans_alignment_dict.items()\n",
    "    )\n",
    "    original_word = lower_remove_non_alphabet(original_word)\n",
    "    perturbed_word = lower_remove_non_alphabet(perturbed_word)\n",
    "    \n",
    "\n",
    "    perturbed_word_appears_in_new_trans = pd.NA\n",
    "    if perturbed_word in perturbed_trans_alignment_dict.keys():\n",
    "        perturbed_word_trans = perturbed_trans_alignment_dict[perturbed_word]\n",
    "        if perturbed_word_trans in changed_trans_part.split():\n",
    "            perturbed_word_appears_in_new_trans = True\n",
    "        else:\n",
    "            perturbed_word_appears_in_new_trans = False\n",
    "            \n",
    "    # Missed-translation, or name-specific case\n",
    "    if perturbed_word in changed_trans_part.split():\n",
    "        perturbed_word_appears_in_new_trans = True\n",
    "            \n",
    "\n",
    "    original_word_appears_in_old_trans = pd.NA\n",
    "    if original_word in original_trans_alignment_dict.keys():\n",
    "        original_word_trans = original_trans_alignment_dict[original_word]\n",
    "        if original_word_trans in original_trans_part.split():\n",
    "            original_word_appears_in_old_trans = True\n",
    "        else:\n",
    "            original_word_appears_in_old_trans = False\n",
    "        \n",
    "        if perturbed_word in perturbed_trans_alignment_dict.keys():\n",
    "            if original_word == 'fort' and perturbed_word == 'île' and change == ('replace', 'Fort-de-France', 'Île-de-France'):\n",
    "                print('-------------------------')\n",
    "                print(change)\n",
    "                print('-' + original_word_trans + '-')\n",
    "                print('-' + perturbed_word_trans + '-')\n",
    "                print(original_word_appears_in_old_trans)\n",
    "                print(perturbed_word_appears_in_new_trans)\n",
    "                \n",
    "    # Missed-translation, or name-specific case\n",
    "    if original_word in original_trans_part.split():\n",
    "        original_word_appears_in_old_trans = True\n",
    "            \n",
    "    # If perturbed_word_appears_in_new_trans or original_word_appears_in_old_trans is true, then \n",
    "    # is_due_to_perturbation is true\n",
    "    if (not pd.isnull(perturbed_word_appears_in_new_trans)) and \\\n",
    "        (not pd.isnull(original_word_appears_in_old_trans)):\n",
    "        return (perturbed_word_appears_in_new_trans or original_word_appears_in_old_trans)\n",
    "    elif (pd.isnull(perturbed_word_appears_in_new_trans)) and \\\n",
    "        (not pd.isnull(original_word_appears_in_old_trans)):\n",
    "        if original_word_appears_in_old_trans:\n",
    "            return True\n",
    "        else:\n",
    "            return pd.NA\n",
    "    elif (not pd.isnull(perturbed_word_appears_in_new_trans)) and \\\n",
    "        (pd.isnull(original_word_appears_in_old_trans)):\n",
    "        if perturbed_word_appears_in_new_trans:\n",
    "            return True\n",
    "        else:\n",
    "            return pd.NA\n",
    "    else:\n",
    "        return pd.NA\n",
    "    \n",
    "    \n",
    "def filter_changes(group_df):\n",
    "    changes = []\n",
    "    \n",
    "    for index, row in group_df.iterrows():\n",
    "        for change in row['changes']:\n",
    "            # Filter out the changes caused by perturbation\n",
    "            is_due_to_perturbation_out = is_due_to_perturbation(\n",
    "                                            change, \n",
    "                                            row['original_word'], \n",
    "                                            row['perturbed_word'], \n",
    "                                            row['perturbed_trans_alignment'],\n",
    "                                            row['original_trans_alignment']\n",
    "                                        )\n",
    "            if pd.isnull(is_due_to_perturbation_out) or is_due_to_perturbation_out:\n",
    "                continue\n",
    "                \n",
    "            # Filter out the weird <unk>\n",
    "            if change == ('delete', '< unk >', '') or change == ('insert', '', '< unk >'):\n",
    "                continue\n",
    "                \n",
    "            # Filter out the changes that are not content-related\n",
    "            all_pos_tags = [t.pos_ for t in spacy_model(change[1])] + [t.pos_ for t in spacy_model(change[2])]\n",
    "            content_related_tags = 'NOUN', 'VERB', 'ADJ', 'PRON'\n",
    "            overlap = not set(all_pos_tags).isdisjoint(content_related_tags)\n",
    "            if not overlap:\n",
    "                continue\n",
    "                \n",
    "            changes.append(change)\n",
    "            \n",
    "            \n",
    "    return changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def find_max_freq_change(group_df):\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        group_df: the group of results that has the same original_word and perturbed_word\n",
    "    \"\"\"\n",
    "    assert group_df['original_word'].value_counts().shape[0] == 1  # Because this function is for a single group\n",
    "    assert group_df['perturbed_word'].value_counts().shape[0] == 1  # Because this function is for a single group\n",
    "    \n",
    "    # Filter out the changes that are not directly due to perturbation\n",
    "    all_changes = filter_changes(group_df)\n",
    "    \n",
    "    freq_changes = Counter(all_changes)\n",
    "    \n",
    "    if len(freq_changes.most_common()) == 0:\n",
    "        return 0\n",
    "    return freq_changes.most_common(1)[0][1]\n",
    "\n",
    "change_freq = output.groupby(\n",
    "    ['original_word', 'perturbed_word'], as_index=False\n",
    ").apply(find_max_freq_change).rename(columns={None: 'max_change_freq'}).sort_values(\n",
    "    by='max_change_freq', ascending=False)\n",
    "    \n",
    "\n",
    "change_freq = change_freq[change_freq['perturbed_word'].apply(lambda x: x.isalpha())]\n",
    "\n",
    "change_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cca6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = output.groupby(['original_word', 'perturbed_word'])\n",
    "groups_as_list = [(original_perturb, group) for original_perturb, group in groups]\n",
    "re_ordered_groupes = [groups_as_list[i] for i in change_freq.index.values]\n",
    "\n",
    "for original_perturb, group in re_ordered_groupes:\n",
    "    print(\"----------------------\")\n",
    "    print(f\"original SRC word: {original_perturb[0]}\")\n",
    "    print(f\"perturbed SRC word: {original_perturb[1]}\")\n",
    "    all_changes = filter_changes(group)\n",
    "    freq_changes = Counter(all_changes)\n",
    "    print(freq_changes.most_common(2))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c1316b0",
   "metadata": {},
   "source": [
    "original SRC word: excuse\n",
    "perturbed SRC word: Trust\n",
    "[(('insert', '', 'mir'), 11), (('insert', '', 'uns'), 4)]\n",
    "\n",
    "original SRC word: communist\n",
    "perturbed SRC word: Nationalist\n",
    "[(('delete', Chinas, ''), 11),\n",
    "\n",
    "\n",
    "\n",
    "original SRC word: usa\n",
    "perturbed SRC word: is\n",
    "[(('insert', '', 'kalifornischen'), 10),\n",
    "\n",
    "\n",
    "original SRC word: please\n",
    "perturbed SRC word: you\n",
    "[(('delete', 'Sie', ''), 7), \n",
    "\n",
    "\n",
    "original SRC word: restaurant\n",
    "perturbed SRC word: bar\n",
    "[(('replace', 'es', 'sie'), 5), (('replace', 'einem', 'einer'), 2)]\n",
    "\n",
    "original SRC word: hurry\n",
    "perturbed SRC word: shut\n",
    "[(('replace', 'sich', 'den Mund'), 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363d5bb",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "--> starts to make sense, yet have not seen bias (even gender bias)\n",
    "\n",
    "--> A bigger dataset for inference could help?\n",
    "\n",
    "Some correlation is good, some correlation is bad. Is it a good idea to prevent these correlation??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42568741",
   "metadata": {},
   "source": [
    "# Filter per sentence with different replacements\n",
    "\n",
    "\n",
    "**Note**: can use [sequence alignments](https://stackoverflow.com/questions/5055839/word-level-edit-distance-of-a-sentence) to align the sentences on the target side only. ([code](https://gist.github.com/slowkow/06c6dba9180d013dfd82bec217d22eb5))\n",
    "\n",
    "Pros: could be easier than SRC-TGT alignment\n",
    "\n",
    "Cons: in the case where more output different sentence structure yet same meaning. <br>\n",
    "E.g., \"Today I think the cat is nice\" -- \"I think the cat is nice today\"\n",
    "SRC-TGT alignment would probably see these as the same, but edit distance cannot, bc it only has del, insert, substitute operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecafdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis = output.groupby('SRC_masked_index').apply(lambda x: analyse_single_sentence_perturbed_word(x))  #.rename(columns={None: 'influenced_words'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_pickle('tmp_storages/analyse_winoMT.pkl')\n",
    "# output = pd.read_pickle('tmp_storages/analyse_winoMT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some missing info samples from mustSHE\n",
    "# output[output['CATEGORY']=='1F'][['SRC', 'SRC_original_idx']].drop_duplicates() #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 9999999)\n",
    "\n",
    "sentence_idx = 0\n",
    "print(f\"Original SRC sentence: \\n {output[['SRC', 'SRC_original_idx']].drop_duplicates().set_index('SRC_original_idx').loc[sentence_idx]}\")\n",
    "print()\n",
    "\n",
    "pprint.pprint(analyse_single_sentence(output[output['SRC_original_idx'] == sentence_idx], align_type=\"trans-only\", return_tgt_word_index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce851487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_word = 'general'\n",
    "\n",
    "\n",
    "sentence_df = output[output['SRC_original_idx'] == sentence_idx]\n",
    "sentence_single_perturbed_word_df = sentence_df[sentence_df['original_word'] == original_word]\n",
    "\n",
    "\n",
    "pprint.pprint(analyse_single_sentence_perturbed_word(sentence_single_perturbed_word_df, align_type=\"trans-only\"))\n",
    "align_translations(sentence_single_perturbed_word_df, align_type=\"trans-only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dda942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b591961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63c618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b087f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965bd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12471047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ea104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108a525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f0871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225343d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fbab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ce163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test to see if SRC_similarity is higher than Trans_similarity\n",
    "print(output[\"Trans-edit_distance\"].mean() - output[\"SRC-edit_distance\"].mean())\n",
    "stats.ttest_rel(output[\"SRC-edit_distance\"], \n",
    "                output[\"Trans-edit_distance\"], \n",
    "                alternative='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187eb64",
   "metadata": {},
   "source": [
    "Tiny pvalue --> Indeed SRC-edit_distance is significantly lower than Trans-edit_distance\n",
    "\n",
    "\n",
    "(Careful with this tho, bc with number of samples too large then statistical test does not make sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada80a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(output[\"#TransChanges-#SrcChanges\"], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49917779",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"ChangesSpread/SentenceLength\"].describe())\n",
    "output[\"ChangesSpread/SentenceLength\"].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c12040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bfa2ffb",
   "metadata": {},
   "source": [
    "Some changes seems to have the same meaning but different phrasing, e.g., noun index 24, 36, 47\n",
    "\n",
    "Both for en-de and en-vi\n",
    "\n",
    "\n",
    "Kind of bias: en-vi adjective sample 82\n",
    "\n",
    "Should we cherry-pick examples? Or cherry-pick the replacement?\n",
    "\n",
    "\n",
    "Or narrow down scope of perturbation? (e.g., on countries, jobs, gender, ...?)\n",
    "\n",
    "\n",
    "\n",
    "Some cherry-picked examples anyway:\n",
    "\n",
    "- He comes from England --> Ông ấy đến từ Anh\n",
    "- He comes from Vietnam --> Hắn đến từ Việt Nam\n",
    "- He comes from North Korea --> Hắn đến từ Bắc Triều Tiên\n",
    "\n",
    "\n",
    "\n",
    "- He is european --> Hắn là người Châu Âu\n",
    "- He is asian --> Anh ấy là người châu Á.\n",
    "\n",
    "\n",
    "\n",
    "- He has black hair --> Hắn có tóc đen.\n",
    "- He has blonde hair --> Anh ấy có tóc vàng\n",
    "\n",
    "\n",
    "But if we limit this then it would hurt the model overal performance as well? \n",
    "\n",
    "*Jan: some kind of loss to minimize the number of changes, but not completely forbidden the changes*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e35e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ca514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755682c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be18d809",
   "metadata": {},
   "source": [
    "# Translation quality vs #changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "output[\"OriginalTran_Quality\"] = output.apply(\n",
    "    lambda x: sentence_gleu([x['tokenized_REF']], x['tokenized_SRC-Trans']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.plot.scatter(x='OriginalTran_Quality', y=\"#TransChanges-#SrcChanges/SentenceLength\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(output['OriginalTran_Quality'], output[\"#TransChanges-#SrcChanges/SentenceLength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(output[\"OriginalTran_Quality\"], bins='sturges')\n",
    "bin_boundaries = hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f74bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use bins with same number of samples instead of equal-sized bins\n",
    "\n",
    "# results, bin_boundaries = pd.qcut(output[\"OriginalTran_Quality\"], q=5, retbins=True)\n",
    "# bin_boundaries\n",
    "\n",
    "\n",
    "# Remove bins with too few samples\n",
    "cut_point = 99999\n",
    "for i, value in enumerate(hist[0]):\n",
    "    if value < 5:\n",
    "        cut_point = i\n",
    "        break\n",
    "        \n",
    "bin_boundaries = bin_boundaries[:cut_point]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_boundaries\n",
    "\n",
    "X = output['OriginalTran_Quality']\n",
    "Y = output[\"#TransChanges-#SrcChanges/SentenceLength\"]\n",
    "\n",
    "x_plot = [(bin_boundaries[i] + bin_boundaries[i+1])/2 for i in range(0, len(bin_boundaries)-1)]\n",
    "y_plot = [stats.trim_mean(Y[(bin_boundaries[i] < X) & (X < bin_boundaries[i+1])], 0.1) for i in range(0, len(bin_boundaries)-1)]\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('OriginalTrans_Quality')\n",
    "plt.ylabel('Avg_changes')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cb6e1ad",
   "metadata": {},
   "source": [
    "bins = [(bin_boundaries[i], bin_boundaries[i+1]) for i in range(0, len(bin_boundaries)-1)]\n",
    "\n",
    "X = output['OriginalTran_Quality']\n",
    "Y = output[\"#TransChanges-#SrcChanges\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.boxplot([Y[(bin_i[0] < X) & (X < bin_i[1])] for bin_i in bins])\n",
    "# ax.set_xticklabels(bins)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_xlabel('OriginalTran_Quality')\n",
    "ax.set_ylabel('#changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd6f40",
   "metadata": {},
   "source": [
    "Most of the time downward trend (not as clear for en-de with verb, adverb, pronoun; en-vi adverb, pronoun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602006a",
   "metadata": {},
   "source": [
    "**Note**: the plot has outliers removed in both X and Y dimensions, by removing too small bins (X) and trimmed-mean (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f10e72",
   "metadata": {},
   "source": [
    "# #changes vs translation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(output[\"#TransChanges-#SrcChanges\"], bins=20)\n",
    "bin_boundaries = hist[1]\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3529f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use bins with same number of samples instead of equal-sized bins\n",
    "# results, bin_boundaries = pd.qcut(output[\"#TransChanges-#SrcChanges\"], q=5, retbins=True)\n",
    "# bin_boundaries\n",
    "\n",
    "\n",
    "# Remove bins with too few samples\n",
    "cut_point = 99999\n",
    "for i, value in enumerate(hist[0]):\n",
    "    if value < 10:\n",
    "        cut_point = i\n",
    "        break\n",
    "        \n",
    "bin_boundaries = bin_boundaries[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb682d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_boundaries\n",
    "\n",
    "X = output['#TransChanges-#SrcChanges']\n",
    "Y = output[\"OriginalTran_Quality\"]\n",
    "\n",
    "x_plot = [(bin_boundaries[i] + bin_boundaries[i+1])/2 for i in range(0, len(bin_boundaries)-1)]\n",
    "\n",
    "y_plot = [stats.trim_mean(Y[(bin_boundaries[i] <= X) & (X <= bin_boundaries[i+1])], 0.1) for i in range(0, len(bin_boundaries)-1)]\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('Avg_changes')\n",
    "plt.ylabel('OriginalTran_Quality')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edfd7c1d",
   "metadata": {},
   "source": [
    "bins = [(bin_boundaries[i], bin_boundaries[i+1]) for i in range(0, len(bin_boundaries)-1)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.boxplot([Y[(bin_i[0] < X) & (X < bin_i[1])] for bin_i in bins])\n",
    "# ax.set_xticklabels(bins)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_xlabel('#changes')\n",
    "ax.set_ylabel('OriginalTran_Quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c740f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30078a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55ac724c",
   "metadata": {},
   "source": [
    "# SentenceLength vs #changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['SRC-length'] = output.apply(\n",
    "    lambda x: len(x['tokenized_SRC']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a88887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.plot.scatter(x='SRC-length', y=\"#TransChanges-#SrcChanges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f0011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65022c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(output['SRC-length'], output[\"#TransChanges-#SrcChanges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(output[\"SRC-length\"], bins=20)\n",
    "bin_boundaries = hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192befe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bins with too few samples\n",
    "cut_point = 99999\n",
    "for i, value in enumerate(hist[0]):\n",
    "    if value < 10:\n",
    "        cut_point = i\n",
    "        break\n",
    "        \n",
    "bin_boundaries = bin_boundaries[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ad438",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = output['SRC-length']\n",
    "Y = output[\"#TransChanges-#SrcChanges\"]\n",
    "\n",
    "x_plot = [(bin_boundaries[i] + bin_boundaries[i+1])/2 for i in range(0, len(bin_boundaries)-1)]\n",
    "y_plot = [stats.trim_mean(Y[(bin_boundaries[i] < X) & (X < bin_boundaries[i+1])], 0.1) for i in range(0, len(bin_boundaries)-1)]\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.xlabel('SRC-length')\n",
    "plt.ylabel('Avg_changes')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c18891ee",
   "metadata": {},
   "source": [
    "bins = [(bin_boundaries[i], bin_boundaries[i+1]) for i in range(0, len(bin_boundaries)-1)]\n",
    "\n",
    "X = output['SRC-length']\n",
    "Y = output[\"#TransChanges-#SrcChanges\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.boxplot([Y[(bin_i[0] < X) & (X < bin_i[1])] for bin_i in bins])\n",
    "# ax.set_xticklabels(bins)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_xlabel('SRC-length')\n",
    "ax.set_ylabel('#changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5a948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7635704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bc35529",
   "metadata": {},
   "source": [
    "# Beam_size vs #changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_dict = {}\n",
    "beam_values = [1,2,3,4,5]\n",
    "for beam in beam_values:\n",
    "    beam_dict[beam] = read_output_df(dataset, perturb_type, beam, replacement_strategy)\n",
    "    # Make sure the df all have the same index\n",
    "    if beam > 1:\n",
    "        assert beam_dict[beam].index.equals(beam_dict[beam].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63368394",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(beam_values,\n",
    "              [stats.trim_mean(beam_dict[x]['#TransChanges-#SrcChanges'], 0.1) for x in beam_values])\n",
    "plt.xlabel('beam')\n",
    "plt.ylabel('mean_changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5ff79",
   "metadata": {},
   "source": [
    "The mean might not saying anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76076635",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.boxplot([beam_dict[x]['#TransChanges-#SrcChanges'] for x in beam_values])\n",
    "ax.set_xticklabels(beam_values)\n",
    "ax.set_xlabel('beam')\n",
    "ax.set_ylabel('#changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34085167",
   "metadata": {},
   "source": [
    "# Perturbed word type vs #changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40211a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_type_dict = {}\n",
    "word_type_values = [\"noun\", \"verb\", \"adjective\", \"adverb\", \"pronoun\"]\n",
    "for word_type in word_type_values:\n",
    "    word_type_dict[word_type] = read_output_df(dataset, perturb_type=word_type, beam=beam, replacement_strategy=replacement_strategy)\n",
    "\n",
    "    \n",
    "print('--------------------------------')\n",
    "print('word type    -   trimmed-mean #changes')\n",
    "\n",
    "for word_type in word_type_values:\n",
    "    print(f\"{word_type} - {stats.trim_mean(word_type_dict[word_type]['#TransChanges-#SrcChanges'], 0.1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.boxplot([word_type_dict[x]['#TransChanges-#SrcChanges'] for x in word_type_values])\n",
    "ax.set_xticklabels(word_type_values)\n",
    "ax.set_xlabel('word_type')\n",
    "ax.set_ylabel('#changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b06547",
   "metadata": {},
   "source": [
    "# #Changes per sentence across word types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ceef7",
   "metadata": {},
   "source": [
    "See if the chaos changes are sentence-specific. Excluding perturbing pronouns bc not many samples have pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c13866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sentences that has multiple word types perturbed\n",
    "word_type_values = [\"noun\", \"verb\", \"adjective\", \"adverb\"]\n",
    "index_intersection = word_type_dict[word_type_values[0]].index\n",
    "for i in range(1, len(word_type_values)):\n",
    "    index_intersection = \\\n",
    "        index_intersection.intersection(word_type_dict[word_type_values[i]].index)\n",
    "\n",
    "len(index_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3094f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_per_word_type = pd.DataFrame()\n",
    "for word_type in word_type_values:\n",
    "    changes_per_word_type[word_type] = word_type_dict[word_type][\"#TransChanges-#SrcChanges\"].loc[index_intersection]\n",
    "    \n",
    "# Count the number of samples where the changes in trans always bigger than changes in SRC\n",
    "changes_per_word_type[(changes_per_word_type['noun'] > 0) & (changes_per_word_type['verb'] > 0) & \\\n",
    "                      (changes_per_word_type['adjective'] > 0) & (changes_per_word_type['adverb'] > 0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba41e95",
   "metadata": {},
   "source": [
    "Small portion of rows --> not sentence-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ace820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy import displacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"He is from Vietnam\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(f\"{'Node (from)-->':<15} {'Relation':^10} {'-->Node (to)':>15}\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"{:<15} {:^10} {:>15}\".format(str(token.head.text), str(token.dep_), str(token.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42699574",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(f\"Token: {token.text}\")\n",
    "    print(f\"Ancestors: {list(token.ancestors)}\")\n",
    "    print(f\"Children: {list(token.children)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy import displacy \n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "sentence = \"Er kommt aus Vietnam\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(f\"{'Node (from)-->':<15} {'Relation':^10} {'-->Node (to)':>15}\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"{:<15} {:^10} {:>15}\".format(str(token.head.text), str(token.dep_), str(token.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47c9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347d716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f080ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9975e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
